{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negras']\n",
      "['panteras', 'negras']\n",
      "['Me', 'las', 'los']\n"
     ]
    }
   ],
   "source": [
    "#Ejercicio 1\n",
    "\n",
    "import re\n",
    "\n",
    "texto1 = 'Me gustan las panteras negras y los leones'\n",
    "\n",
    "palabras11 = re.findall(r'\\bne\\w+\\b', texto1)\n",
    "palabras12 = re.findall(r'\\b\\w{3,}ras\\b',texto1)\n",
    "palabras13 = re.findall(r'\\b\\w{2,4}\\b',texto1)\n",
    "\n",
    "print(palabras11)\n",
    "print(palabras12)\n",
    "print(palabras13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Nivel Educativo Num Centros Num Alumnos (2009/10)\n",
      "0                  Preescolar-infantil         305                29 466\n",
      "1                             Primaria         166                40 596\n",
      "2               Secundaria Obligatoria         108                26 958\n",
      "3                         Bachillerato          65                11 797\n",
      "4     Ciclos formativos de grado medio          47                  6348\n",
      "5  Ciclos formativos de grado superior          33                  8149\n"
     ]
    }
   ],
   "source": [
    "#Ejercicio 2\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://es.wikipedia.org/wiki/Valencia\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "tablas = soup.find_all(\"table\")\n",
    "\n",
    "tabla = tablas[11] # pongo este indice porque es donde la he encontrado\n",
    "data = []\n",
    "filas = tabla.find_all(\"tr\")\n",
    "for fila in filas[1:]:\n",
    "    columnas = fila.find_all(\"td\")\n",
    "\n",
    "    try:\n",
    "        nivel_educativo = columnas[0].text.strip()\n",
    "    except:\n",
    "        nivel_educativo = \"\"\n",
    "\n",
    "    try:\n",
    "        num_centros = columnas[1].text.strip()\n",
    "    except:\n",
    "        num_centros = \"\"\n",
    "    \n",
    "    try:\n",
    "        num_alumnos = columnas[2].text.strip()\n",
    "    except:\n",
    "        num_alumnos = \"\"\n",
    "\n",
    "    if nivel_educativo != \"\" and num_centros != \"\" and num_alumnos != \"\":\n",
    "        data.append([nivel_educativo, num_centros, num_alumnos])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Nivel Educativo\", \"Num Centros\", \"Num Alumnos (2009/10)\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holar queria interesar comprar producto número 4241421\n"
     ]
    }
   ],
   "source": [
    "#Ejercicio 3\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    texto = texto.lower()\n",
    "\n",
    "    doc = nlp(texto)\n",
    "    texto_limpio = ' '.join(token.lemma_ for token in doc if not token.is_punct and not token.is_stop)\n",
    "\n",
    "    return texto_limpio\n",
    "\n",
    "texto_original = \"Hola, como estas hoy? Queria saber si te interesa comprar mi producto. Mi número es 4241421\"\n",
    "texto_limpio = limpiar_texto(texto_original)\n",
    "print(texto_limpio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidad: Apple, Tipo semántico: ORG\n",
      "Entidad: EE.UU., Tipo semántico: LOC\n",
      "Entidad: iPhone, Tipo semántico: MISC\n",
      "Entidad: Galaxy Note 9, Tipo semántico: MISC\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 4\n",
    "\n",
    "def identificar_entidades(texto):\n",
    "    doc = nlp(texto)\n",
    "\n",
    "    ents = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ != '']\n",
    "    \n",
    "    return ents\n",
    "\n",
    "texto4 = \"Apple es la marca que más satisfacción genera en EE.UU., pero el iPhone fue superado por el Galaxy Note 9.\"\n",
    "\n",
    "entidades = identificar_entidades(texto4)\n",
    "\n",
    "for entidad, tipo in entidades:\n",
    "    print(f\"Entidad: {entidad}, Tipo semántico: {tipo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra: Palabras, Categoría: PROPN\n",
      "Palabra: ningÃºn, Categoría: ADJ\n",
      "Palabra: significado, Categoría: NOUN\n",
      "Palabra: aportan, Categoría: VERB\n",
      "Palabra: informaciÃ³n, Categoría: NOUN\n",
      "Palabra: Suelen, Categoría: VERB\n",
      "Palabra: palabras, Categoría: NOUN\n",
      "Palabra: comunes, Categoría: ADJ\n",
      "Palabra: ejemplo, Categoría: NOUN\n",
      "Palabra: preposiciones, Categoría: NOUN\n",
      "Palabra: NLP, Categoría: PROPN\n",
      "Palabra: suele, Categoría: VERB\n",
      "Palabra: trabajar, Categoría: VERB\n",
      "Palabra: vocabularios, Categoría: NOUN\n",
      "Palabra: enormes, Categoría: ADJ\n",
      "Palabra: Don, Categoría: PROPN\n",
      "Palabra: Quijote, Categoría: PROPN\n",
      "Palabra: Mancha, Categoría: PROPN\n",
      "Palabra: aparecen, Categoría: VERB\n",
      "Palabra: torno, Categoría: NOUN\n",
      "Palabra: 000, Categoría: NUM\n",
      "Palabra: palabras, Categoría: NOUN\n",
      "Palabra: distintas, Categoría: ADJ\n",
      "Palabra: interesa, Categoría: VERB\n",
      "Palabra: filtrar, Categoría: VERB\n",
      "Palabra: informaciÃ³n, Categoría: NOUN\n",
      "Palabra: aporten, Categoría: VERB\n",
      "Palabra: Debate, Categoría: NOUN\n",
      "Palabra: Â¿quÃ, Categoría: SYM\n",
      "Palabra: significa, Categoría: VERB\n",
      "Palabra: aporten, Categoría: VERB\n",
      "Palabra: informaciÃ³n, Categoría: PROPN\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 5\n",
    "\n",
    "def limpiar_texto(texto):\n",
    "    signos_puntuacion = ['.', ':', ',']\n",
    "\n",
    "    for signo in signos_puntuacion:\n",
    "        texto = texto.replace(signo, signo + ' ')\n",
    "\n",
    "    doc = nlp(texto)\n",
    "    palabras = [(token.text, token.pos_) for token in doc]\n",
    "    palabras = [palabra for palabra in palabras if not (palabra[0].isspace() or palabra[0] in signos_puntuacion or palabra[0] in nlp.Defaults.stop_words)]\n",
    "    palabras = [palabra for palabra in palabras if len(palabra[0]) >= 3]\n",
    "\n",
    "    return palabras\n",
    "\n",
    "with open('Ex5.txt', 'r') as file:\n",
    "    texto5 = file.read()\n",
    "palabras_limpias = limpiar_texto(texto5)\n",
    "\n",
    "for palabra, categoria in palabras_limpias:\n",
    "    print(f\"Palabra: {palabra}, Categoría: {categoria}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens coincidentes Patrón 1:\n",
      "canta\n",
      "cantan\n",
      "Tokens coincidentes Patrón 2:\n",
      "canta\n",
      "cantan\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 6\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "patron_1 = [{\"LEMMA\": \"cantar\"}]\n",
    "patron_2 = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"ADJ\", \"OP\": \"?\"}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATRON_1\", [patron_1])\n",
    "matcher.add(\"PATRON_2\", [patron_2])\n",
    "\n",
    "# Procesar el texto\n",
    "texto6 = \"Soy un texto que pide a gritos que lo procesen. Por eso yo canto, tú cantas, ella canta, nosotros cantamos, cantáis, cantan…\"\n",
    "doc = nlp(texto6)\n",
    "\n",
    "# Encontrar coincidencias\n",
    "coincidencias_1 = matcher(doc)\n",
    "coincidencias_2 = matcher(doc)\n",
    "\n",
    "# Imprimir los tokens de las coincidencias\n",
    "print(\"Tokens coincidentes Patrón 1:\")\n",
    "for match_id, start, end in coincidencias_1:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)\n",
    "\n",
    "print(\"Tokens coincidentes Patrón 2:\")\n",
    "for match_id, start, end in coincidencias_2:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades:\n",
      "David PERSON\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 7\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "words = [\"Em\", \"agrada\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "doc = spacy.tokens.Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "span = spacy.tokens.Span(doc, start=2, end=3, label=\"PERSON\")\n",
    "doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "print(\"Entidades:\")\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz BoW:\n",
      "[[1 2 0 0 1 1 1 0 1 0]\n",
      " [0 1 1 0 1 1 0 1 0 2]\n",
      " [1 0 0 1 0 1 0 0 0 1]]\n",
      "Matriz sparse:\n",
      "[[1 2 0 0 1 1 1 0 1 0]\n",
      " [0 1 1 0 1 1 0 1 0 2]\n",
      " [1 0 0 1 0 1 0 0 0 1]]\n",
      "Matriz TF-IDF:\n",
      "[[0.31526909 0.63053818 0.         0.         0.31526909 0.24483457\n",
      "  0.41454097 0.         0.41454097 0.        ]\n",
      " [0.         0.31526909 0.41454097 0.         0.31526909 0.24483457\n",
      "  0.         0.41454097 0.         0.63053818]\n",
      " [0.4804584  0.         0.         0.63174505 0.         0.37311881\n",
      "  0.         0.         0.         0.4804584 ]]\n"
     ]
    }
   ],
   "source": [
    "# Ejercicio 8\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "def normalizar_texto(texto):\n",
    "    # Normalizar el texto convirtiendo a minúsculas y eliminando palabras de menos de 4 caracteres\n",
    "    words = texto.lower().split()\n",
    "    words = [word for word in words if len(word) > 4]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Definir el texto\n",
    "textos = [\"Juana le gusta el color azul, pero su color favorito es el rojo.\",\n",
    "        \"Julio odia el azul pero le gusta el verde. Sin embargo, el verde no es su color favorito.\",\n",
    "        \"A Jose le encantan el rojo y el azul. No le gusta el verde.\"]\n",
    "\n",
    "\n",
    "texto8 = [normalizar_texto(frase) for frase in textos]\n",
    "\n",
    "# Calcular la matriz de frecuencia de palabras\n",
    "vec = CountVectorizer()\n",
    "matriz_BoW = vec.fit_transform(texto8)\n",
    "\n",
    "# Convertir la matriz de frecuencia de palabras en una matriz densa\n",
    "matriz_sparse = matriz_BoW.toarray()\n",
    "\n",
    "# Convertir la matriz de frecuencia de palabras en una forma vectorial regularizada\n",
    "tfidf = TfidfTransformer()\n",
    "matriz_Tfidf = tfidf.fit_transform(matriz_BoW)\n",
    "\n",
    "# Imprimir la matriz de frecuencia de palabras\n",
    "print(\"Matriz BoW:\")\n",
    "print(matriz_BoW.toarray())\n",
    "\n",
    "# Imprimir la matriz densa\n",
    "print(\"Matriz sparse:\")\n",
    "print(matriz_sparse)\n",
    "\n",
    "# Imprimir la matriz TF-IDF\n",
    "print(\"Matriz TF-IDF:\")\n",
    "print(matriz_Tfidf.toarray())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
