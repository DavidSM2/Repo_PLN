{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "clear-thailand",
   "metadata": {},
   "source": [
    "# Pràctica 6: extracció d'informació\n",
    "En aquesta pràctica analitzarem de manera no supervisada un corpus de textos per a extraure informació.\\\n",
    "Primer extraurem i analitzarem les entitats que formen el corpus, a continuació extraurem les paraules claus de cada document i finalment realitzarem un *topic *modeling* del corpus.\\\n",
    "Usem el conjunt de crítiques de cinema de \"Mundocine\" que estan en format XML dins d'un directori.  \n",
    "Definim una funció per a extraure el text de la crítica de cada arxiu XML del directori mitjançant una funció de tipus **generator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "italic-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from xml.dom.minidom import parseString\n",
    "\n",
    "def parse_folder(path):\n",
    "    \"\"\"generator that reads the contents of XML files in a folder\n",
    "    Returns the <body> of the <review> in each XML file.\n",
    "    XML files encoded as 'latin-1'\"\"\"\n",
    "    for file in sorted([f for f in os.listdir(path) if f.endswith('.xml')],\n",
    "                        key=lambda x: int(re.match(r'\\d+',x).group())):\n",
    "        with open(os.path.join(path, file), encoding='latin-1') as f:\n",
    "            doc=parseString(re.sub(r'(<>)|&|(<-)', '', f.read()))\n",
    "\n",
    "            titulo = doc.documentElement.attributes[\"title\"].value\n",
    "\n",
    "            btxt = \"\"\n",
    "            review_bod = doc.getElementsByTagName(\"body\")\n",
    "            if len(review_bod) > 0:\n",
    "                for node in review_bod[0].childNodes:\n",
    "                    if node.nodeType == node.TEXT_NODE:\n",
    "                        btxt += node.data + \" \"\n",
    "\n",
    "            rtxt = \"\"\n",
    "            review_summ = doc.getElementsByTagName(\"summary\")\n",
    "            if len(review_summ) > 0:\n",
    "                for node in review_summ[0].childNodes:\n",
    "                    if node.nodeType == node.TEXT_NODE:\n",
    "                        rtxt += node.data + \" \"\n",
    "                #separamos después de ciertos signos de puntuación\n",
    "                rtxt = re.sub(r\"([\\.\\?])\", r\"\\1 \", rtxt)\n",
    "                        \n",
    "            rank = int(doc.documentElement.attributes[\"rank\"].value)\n",
    "            \n",
    "            yield titulo, rtxt, btxt, rank\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "mighty-morocco",
   "metadata": {},
   "source": [
    "## Extracció d'entitats\n",
    "Analitzarem les entitats (tipus i quantitat) que apareixen en cada document del *corpus*\n",
    "### Exercici\n",
    "Construeix una funció de tipus *generator* que retorne en cada iteració les entitats del següent document. Aquestes entitats es generaran com un *string* amb les etiquetes de cada entitat en el text separat per comes, p. ej:\n",
    "```\n",
    "'MISC PER MISC MISC MISC MISC ORG PER PER MISC LOC'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "characteristic-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def extraer_ner(texto):\n",
    "    doc = nlp(texto)\n",
    "    ent_labels = [ent.label_ for ent in doc.ents]\n",
    "    return ' '.join(ent_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "individual-somalia",
   "metadata": {},
   "source": [
    "Prova el seu funcionament sobre el text de la primera crítica en el Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "likely-great",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MISC MISC MISC PER PER MISC MISC MISC MISC MISC MISC PER PER PER MISC LOC PER MISC MISC MISC MISC PER MISC'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = parse_folder('criticas/train/')\n",
    "texto = extraer_ner(next(corpus)[2])\n",
    "texto\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "governing-hundred",
   "metadata": {},
   "source": [
    "Combinarem totes dues funcions per a processar tot el corpus.\\\n",
    "Construeix una funció de tipus *generator* que a partir d'un directori amb les crítiques, processe tots els seus arxius i retorne en cada iteració el llistat d'entitats del següent document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pleased-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criticas_ner(folder):\n",
    "    corpus = parse_folder(folder)\n",
    "    for critica in corpus:\n",
    "        yield extraer_ner(critica[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "amber-motel",
   "metadata": {},
   "source": [
    "Provem el seu funcionament amb el primer arxiu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "recreational-saturday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MISC MISC MISC PER PER MISC MISC MISC MISC MISC MISC PER PER PER MISC LOC PER MISC MISC MISC MISC PER MISC'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criticas_gen = criticas_ner(\"criticas/train\")\n",
    "next(criticas_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "liable-color",
   "metadata": {},
   "source": [
    "Una vegada s'ha consumit un arxiu, el generador passa el següent i no es pot tornar a l'inici del iterador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "elder-spell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MISC MISC MISC MISC MISC LOC MISC PER MISC PER LOC MISC LOC PER PER MISC PER MISC PER MISC MISC LOC PER PER PER MISC MISC MISC MISC MISC PER PER MISC PER PER PER LOC'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(criticas_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "remarkable-order",
   "metadata": {},
   "source": [
    "## Anàlisi de les entitats\n",
    "Com a anàlisi molt simple, veurem quantes entitats apareixen en tot el corpus.\\\n",
    "Per a això, comptem el núm. d'entitats de cada document amb el vectorizador BoW de la llibreria `scikit-learn`.\n",
    "### Exercici\n",
    "Aplica el vectorizador BoW per a obtindre la seua matriu d'ocurrència. Suma el total de vegades que ha aparegut cada entitat i mostra'l en un dataframe de Colles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "transsexual-sheep",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entidad</th>\n",
       "      <th>Apariciones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loc</td>\n",
       "      <td>10646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>misc</td>\n",
       "      <td>32718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>org</td>\n",
       "      <td>2812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>per</td>\n",
       "      <td>29328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Entidad  Apariciones\n",
       "0     loc        10646\n",
       "1    misc        32718\n",
       "2     org         2812\n",
       "3     per        29328"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "vect = CountVectorizer()\n",
    "bow = vect.fit_transform(criticas_gen)\n",
    "totales = bow.sum(axis=0)\n",
    "\n",
    "data = {'Entidad' : vect.get_feature_names_out(), 'Apariciones' : totales.tolist()[0] }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "impaired-simulation",
   "metadata": {},
   "source": [
    "## Extracció de paraules clau\n",
    "En aquest exercici extraurem les paraules clau de cada crítica mitjançant la llibreria `textacy`. Després crearem un BoW d'aquests termes per a analitzar amb quins freqüència apareixen.\\\n",
    "Com les paraules claus determinades per la llibreria `textacy` poden ser n-grames, cal unificar els seus constituents amb `_` per a considerar-los termes únics en el vocabulari.  \n",
    "\n",
    "### Exercici\n",
    "Defineix una funció per a unir diversos n-*gramas en un únic terme.\\\n",
    "Defineix una funció que extraga les paraules clau d'un document i les retorne com una llista de *tokens*. Utilitza l'algorisme `textrank` sobre el text normalitzat en minúscules sense lematizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "raising-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.extract import keyterms as kt\n",
    "\n",
    "def unir_ngramas(texto):\n",
    "    \"\"\"Une todos los términos de un n-grama mediante '_'\n",
    "    para que formen un único término en el vocabulario\"\"\"\n",
    "    #No se que coño tengo que hacer aqui??????????????\n",
    "\n",
    "def extraer_keywords(texto, topn=10):\n",
    "    \"\"\"Extrar las palabras clave de un texto mediante la\n",
    "    librería textacy\n",
    "    Devuelve los topn términos clave como lista de strings\"\"\"\n",
    "    # Y esto no se si esta bien ?????????????\n",
    "    keywords = kt.sgrank(nlp(texto), ngrams=(1, 2, 3), normalize=\"lower\", include_pos=[\"NOUN\", \"PROPN\", \"ADJ\"])\n",
    "    return [keyword for keyword in keywords[:topn]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "separate-salad",
   "metadata": {},
   "source": [
    "Provem sobre la crítica descarregada abans..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "protecting-compatibility",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'textacy.extract.keyterms' has no attribute 'ngrams'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m corpus \u001b[39m=\u001b[39m parse_folder(\u001b[39m'\u001b[39m\u001b[39mcriticas/train/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m unir_ngramas(\u001b[39mnext\u001b[39;49m(corpus)[\u001b[39m2\u001b[39;49m])\n",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m, in \u001b[0;36munir_ngramas\u001b[1;34m(texto)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munir_ngramas\u001b[39m(texto):\n\u001b[0;32m      4\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Une todos los términos de un n-grama mediante '_'\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m    para que formen un único término en el vocabulario\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     ngrams \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(kt\u001b[39m.\u001b[39;49mngrams(texto, n\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, filter_stops\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filter_punct\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m      7\u001b[0m     terms \u001b[39m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m     \u001b[39mfor\u001b[39;00m ngram \u001b[39min\u001b[39;00m ngrams:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'textacy.extract.keyterms' has no attribute 'ngrams'"
     ]
    }
   ],
   "source": [
    "corpus = parse_folder('criticas/train/')\n",
    "unir_ngramas(next(corpus)[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "medical-material",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Defineix una funció *generator* que retorne iterativament per a cada crítica dins d'un directori les seues paraules clau.\n",
    "Ha de retornar per cada document un *string* amb les paraules clau separades per espai, per a poder usar-ho amb el vectorizador de `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "iraqi-spank",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3725003253.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    #COMPLETAR\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def criticas_keyword(folder):\n",
    "    #COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "immune-programming",
   "metadata": {},
   "source": [
    "Provem sobre el primer document del corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "natural-signature",
   "metadata": {},
   "source": [
    "### Anàlisi de les paraules clau\n",
    "Implementa un vectorizador BoW per a comptar les paraules clau que apareixen almenys en un 0,5% dels documents i mostra-les ordenades en ordre descendent de freqüència (paraula, núm. d'aparicions en total) dins d'un *DataFrame. Mostra només els 10 termes més utilitzats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "conscious-pizza",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "També ho podem fer amb la llibreria `gensim`, per a així poder comptar el núm. de documents en el qual apareix cada keyword (amb l'atribut `dfs` de l'objecte `Dictionary`).\\\n",
    "Defineix de nou la funció `critiques_keyword` perquè retorne les paraules clau de cada document en un format compatible amb Gensim i calcula el seu diccionari BoW (no fa falta calcular la matriu de tots els documents).\\\n",
    "Després, utilitza el mètode `filter_extremes` per a quedar-te amb els termes que apareixen almenys en un 0,5% dels documents (nota: usa l'atribut `.num_docs` per a calcular el núm. de documents que formen el 0,5% del total)\\\n",
    "Finalment, usa l'atribut `dfs` del diccionari per a mostrar les paraules clau i el núm. de documents en el qual s'usen, com *dataframe. Mostra només els 10 termes més freqüents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "horizontal-monte",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "Finalment, calcularem les temàtiques del corpus de crítiques mitjançant un model de *topic modeling* usant l'algorisme LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "\n",
    "\n",
    "# herramientas de dibujado\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fleet-avatar",
   "metadata": {},
   "source": [
    "Definim una classe *iterable* per a obtindre els documents del Corpus línia a línia des de l'arxiu del conjunt d'exemple i convertir-los en un llistat de tokens. Al contrari que els generadors, les classes *iterables* poden tornar a l'inici de la llista cada vegada, i no s'esgoten quan es consumeixen tots els elements.\n",
    "### Exercici\n",
    "Per a calcular les temàtiques, utilitzarem els lemes de cada terme del corpus, considerant només aquells la funció morfològica dels quals siga nom, nom propi, adjectiu, verb o adverbi. Defineix una funció `lematize_doc` que retorne per a cada document un llistat dels tokens amb aquest processament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_doc(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']):\n",
    "    \"\"\"Función que devuelve la lista de lemas en minúscula de una string,,\n",
    "    excluyendo las palabras cuyo POS_TAG no está en la lista allowed_postags.\n",
    "    Considera sólo lemas de más de 3 caracteres y omite las stop words.\"\"\"\n",
    "    #COMPLETAR\n",
    "\n",
    "def criticas_tokens(folder):\n",
    "    for c in parse_folder(folder):\n",
    "        yield lemmatize_doc(c[2])\n",
    "            \n",
    "#para no tener que cargar todo el corpus en memoria creamos un streamer\n",
    "class BOW_Corpus(object):\n",
    "    \"\"\"\n",
    "    Iterable: en cada iteración devuelve el vector bag-of-words\n",
    "    del siguiente documento en el corpus.\n",
    "    El corpus es el listado de críticas alojadas en el directorio\n",
    "    pasado como argumento al instanciar la clase.\n",
    "    \n",
    "    Procesa un documento cada vez, así\n",
    "    nunca carga el corpus entero en RAM.\n",
    "    \"\"\"\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        #crea el diccionario = mapeo de documentos a sparse vectors\n",
    "        self.diccionario = gensim.corpora.Dictionary(criticas_tokens(self.dirname))\n",
    "        \n",
    "    def __len__(self):\n",
    "        #necesitamos saber la longitud del corpus para visualizar con pyLDAvis\n",
    "        return self.diccionario.num_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        __iter__ es un iterable => BOW_Corpus es un streamed iterable.\n",
    "        \"\"\"\n",
    "        for tokens in criticas_tokens(self.dirname):\n",
    "            # transforma cada doc (lista de tokens) en un vector sparse uno a uno\n",
    "            yield self.diccionario.doc2bow(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lucky-robinson",
   "metadata": {},
   "source": [
    "Crea la matriu BoW sobre el corpus com un element de la classe BOW_Corpus amb el nom `bow_critiques`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "relevant-wrestling",
   "metadata": {},
   "source": [
    "Mostrem el BoW del primer document com a comprovació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in bow_criticas:\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "#términos en el diccionario\n",
    "len(bow_criticas.diccionario.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#longitud del corpus\n",
    "len(bow_criticas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "difficult-repeat",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Crea un model LDA sobre el corpus de crítiques amb 5 temes i mostra'l gràficament usant les funcions de la llibreria `pyLDAvis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0205235b8c5ab439d493b174c798c34c1a4247fd8615a106b3ec5ca70c663250"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
