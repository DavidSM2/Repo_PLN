{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "clear-thailand",
   "metadata": {},
   "source": [
    "# Pràctica 6: extracció d'informació\n",
    "En aquesta pràctica analitzarem de manera no supervisada un corpus de textos per a extraure informació.\\\n",
    "Primer extraurem i analitzarem les entitats que formen el corpus, a continuació extraurem les paraules claus de cada document i finalment realitzarem un *topic *modeling* del corpus.\\\n",
    "Usem el conjunt de crítiques de cinema de \"Mundocine\" que estan en format XML dins d'un directori.  \n",
    "Definim una funció per a extraure el text de la crítica de cada arxiu XML del directori mitjançant una funció de tipus **generator*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from xml.dom.minidom import parseString\n",
    "\n",
    "def parse_folder(path):\n",
    "    \"\"\"generator that reads the contents of XML files in a folder\n",
    "    Returns the <body> of the <review> in each XML file.\n",
    "    XML files encoded as 'latin-1'\"\"\"\n",
    "    for file in sorted([f for f in os.listdir(path) if f.endswith('.xml')],\n",
    "                        key=lambda x: int(re.match(r'\\d+',x).group())):\n",
    "        with open(os.path.join(path, file), encoding='latin-1') as f:\n",
    "            doc=parseString(re.sub(r'(<>)|&|(<-)', '', f.read()))\n",
    "\n",
    "            titulo = doc.documentElement.attributes[\"title\"].value\n",
    "\n",
    "            btxt = \"\"\n",
    "            review_bod = doc.getElementsByTagName(\"body\")\n",
    "            if len(review_bod) > 0:\n",
    "                for node in review_bod[0].childNodes:\n",
    "                    if node.nodeType == node.TEXT_NODE:\n",
    "                        btxt += node.data + \" \"\n",
    "\n",
    "            rtxt = \"\"\n",
    "            review_summ = doc.getElementsByTagName(\"summary\")\n",
    "            if len(review_summ) > 0:\n",
    "                for node in review_summ[0].childNodes:\n",
    "                    if node.nodeType == node.TEXT_NODE:\n",
    "                        rtxt += node.data + \" \"\n",
    "                #separamos después de ciertos signos de puntuación\n",
    "                rtxt = re.sub(r\"([\\.\\?])\", r\"\\1 \", rtxt)\n",
    "                        \n",
    "            rank = int(doc.documentElement.attributes[\"rank\"].value)\n",
    "            \n",
    "            yield titulo, rtxt, btxt, rank\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "mighty-morocco",
   "metadata": {},
   "source": [
    "## Extracció d'entitats\n",
    "Analitzarem les entitats (tipus i quantitat) que apareixen en cada document del *corpus*\n",
    "### Exercici\n",
    "Construeix una funció de tipus *generator* que retorne en cada iteració les entitats del següent document. Aquestes entitats es generaran com un *string* amb les etiquetes de cada entitat en el text separat per comes, p. ej:\n",
    "```\n",
    "'MISC PER MISC MISC MISC MISC ORG PER PER MISC LOC'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-color",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def extraer_ner(texto):\n",
    "    \"\"\"Extrar las entidades propias de un texto mediante el proceso\n",
    "    NER de la librería spaCy\"\"\"\n",
    "    #COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "individual-somalia",
   "metadata": {},
   "source": [
    "Prova el seu funcionament sobre el text de la primera crítica en el Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "governing-hundred",
   "metadata": {},
   "source": [
    "Combinarem totes dues funcions per a processar tot el corpus.\\\n",
    "Construeix una funció de tipus *generator* que a partir d'un directori amb les crítiques, processe tots els seus arxius i retorne en cada iteració el llistat d'entitats del següent document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criticas_ner(folder):\n",
    "    #COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "amber-motel",
   "metadata": {},
   "source": [
    "Provem el seu funcionament amb el primer arxiu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "criticas_gen = criticas_ner(\"criticas/train\")\n",
    "next(criticas_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "liable-color",
   "metadata": {},
   "source": [
    "Una vegada s'ha consumit un arxiu, el generador passa el següent i no es pot tornar a l'inici del iterador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-spell",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(criticas_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "remarkable-order",
   "metadata": {},
   "source": [
    "## Anàlisi de les entitats\n",
    "Com a anàlisi molt simple, veurem quantes entitats apareixen en tot el corpus.\\\n",
    "Per a això, comptem el núm. d'entitats de cada document amb el vectorizador BoW de la llibreria `scikit-learn`.\n",
    "### Exercici\n",
    "Aplica el vectorizador BoW per a obtindre la seua matriu d'ocurrència. Suma el total de vegades que ha aparegut cada entitat i mostra'l en un dataframe de Colles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "impaired-simulation",
   "metadata": {},
   "source": [
    "## Extracció de paraules clau\n",
    "En aquest exercici extraurem les paraules clau de cada crítica mitjançant la llibreria `textacy`. Després crearem un BoW d'aquests termes per a analitzar amb quins freqüència apareixen.\\\n",
    "Com les paraules claus determinades per la llibreria `textacy` poden ser n-grames, cal unificar els seus constituents amb `_` per a considerar-los termes únics en el vocabulari.  \n",
    "\n",
    "### Exercici\n",
    "Defineix una funció per a unir diversos n-*gramas en un únic terme.\\\n",
    "Defineix una funció que extraga les paraules clau d'un document i les retorne com una llista de *tokens*. Utilitza l'algorisme `textrank` sobre el text normalitzat en minúscules sense lematizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textacy.extract import keyterms as kt\n",
    "\n",
    "def unir_ngramas(texto):\n",
    "    \"\"\"Une todos los términos de un n-grama mediante '_'\n",
    "    para que formen un único término en el vocabulario\"\"\"\n",
    "    #COMPLETAR\n",
    "\n",
    "def extraer_keywords(texto, topn=10):\n",
    "    \"\"\"Extrar las palabras clave de un texto mediante la\n",
    "    librería textacy\n",
    "    Devuelve los topn términos clave como lista de strings\"\"\"\n",
    "    #COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "separate-salad",
   "metadata": {},
   "source": [
    "Provem sobre la crítica descarregada abans..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "medical-material",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Defineix una funció *generator* que retorne iterativament per a cada crítica dins d'un directori les seues paraules clau.\n",
    "Ha de retornar per cada document un *string* amb les paraules clau separades per espai, per a poder usar-ho amb el vectorizador de `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criticas_keyword(folder):\n",
    "    #COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "immune-programming",
   "metadata": {},
   "source": [
    "Provem sobre el primer document del corpus..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "natural-signature",
   "metadata": {},
   "source": [
    "### Anàlisi de les paraules clau\n",
    "Implementa un vectorizador BoW per a comptar les paraules clau que apareixen almenys en un 0,5% dels documents i mostra-les ordenades en ordre descendent de freqüència (paraula, núm. d'aparicions en total) dins d'un *DataFrame. Mostra només els 10 termes més utilitzats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "conscious-pizza",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "També ho podem fer amb la llibreria `gensim`, per a així poder comptar el núm. de documents en el qual apareix cada keyword (amb l'atribut `dfs` de l'objecte `Dictionary`).\\\n",
    "Defineix de nou la funció `critiques_keyword` perquè retorne les paraules clau de cada document en un format compatible amb Gensim i calcula el seu diccionari BoW (no fa falta calcular la matriu de tots els documents).\\\n",
    "Després, utilitza el mètode `filter_extremes` per a quedar-te amb els termes que apareixen almenys en un 0,5% dels documents (nota: usa l'atribut `.num_docs` per a calcular el núm. de documents que formen el 0,5% del total)\\\n",
    "Finalment, usa l'atribut `dfs` del diccionari per a mostrar les paraules clau i el núm. de documents en el qual s'usen, com *dataframe. Mostra només els 10 termes més freqüents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "horizontal-monte",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "Finalment, calcularem les temàtiques del corpus de crítiques mitjançant un model de *topic modeling* usant l'algorisme LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "\n",
    "\n",
    "# herramientas de dibujado\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fleet-avatar",
   "metadata": {},
   "source": [
    "Definim una classe *iterable* per a obtindre els documents del Corpus línia a línia des de l'arxiu del conjunt d'exemple i convertir-los en un llistat de tokens. Al contrari que els generadors, les classes *iterables* poden tornar a l'inici de la llista cada vegada, i no s'esgoten quan es consumeixen tots els elements.\n",
    "### Exercici\n",
    "Per a calcular les temàtiques, utilitzarem els lemes de cada terme del corpus, considerant només aquells la funció morfològica dels quals siga nom, nom propi, adjectiu, verb o adverbi. Defineix una funció `lematize_doc` que retorne per a cada document un llistat dels tokens amb aquest processament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_doc(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']):\n",
    "    \"\"\"Función que devuelve la lista de lemas en minúscula de una string,,\n",
    "    excluyendo las palabras cuyo POS_TAG no está en la lista allowed_postags.\n",
    "    Considera sólo lemas de más de 3 caracteres y omite las stop words.\"\"\"\n",
    "    #COMPLETAR\n",
    "\n",
    "def criticas_tokens(folder):\n",
    "    for c in parse_folder(folder):\n",
    "        yield lemmatize_doc(c[2])\n",
    "            \n",
    "#para no tener que cargar todo el corpus en memoria creamos un streamer\n",
    "class BOW_Corpus(object):\n",
    "    \"\"\"\n",
    "    Iterable: en cada iteración devuelve el vector bag-of-words\n",
    "    del siguiente documento en el corpus.\n",
    "    El corpus es el listado de críticas alojadas en el directorio\n",
    "    pasado como argumento al instanciar la clase.\n",
    "    \n",
    "    Procesa un documento cada vez, así\n",
    "    nunca carga el corpus entero en RAM.\n",
    "    \"\"\"\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        #crea el diccionario = mapeo de documentos a sparse vectors\n",
    "        self.diccionario = gensim.corpora.Dictionary(criticas_tokens(self.dirname))\n",
    "        \n",
    "    def __len__(self):\n",
    "        #necesitamos saber la longitud del corpus para visualizar con pyLDAvis\n",
    "        return self.diccionario.num_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        __iter__ es un iterable => BOW_Corpus es un streamed iterable.\n",
    "        \"\"\"\n",
    "        for tokens in criticas_tokens(self.dirname):\n",
    "            # transforma cada doc (lista de tokens) en un vector sparse uno a uno\n",
    "            yield self.diccionario.doc2bow(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lucky-robinson",
   "metadata": {},
   "source": [
    "Crea la matriu BoW sobre el corpus com un element de la classe BOW_Corpus amb el nom `bow_critiques`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "relevant-wrestling",
   "metadata": {},
   "source": [
    "Mostrem el BoW del primer document com a comprovació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in bow_criticas:\n",
    "    print(b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "#términos en el diccionario\n",
    "len(bow_criticas.diccionario.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#longitud del corpus\n",
    "len(bow_criticas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "difficult-repeat",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Crea un model LDA sobre el corpus de crítiques amb 5 temes i mostra'l gràficament usant les funcions de la llibreria `pyLDAvis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
