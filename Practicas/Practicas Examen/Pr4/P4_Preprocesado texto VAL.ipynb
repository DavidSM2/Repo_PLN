{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dress-auditor",
   "metadata": {},
   "source": [
    "# Pràctica 4 PLN: pre-processat de text i extracció de característiques\n",
    "En aquesta pràctica realitzarem la neteja i pre-processat de diferents conjunts de dades de text.\\\n",
    "Després farem una extracció de característiques com a matriu *sparse* i com a vectors de paraules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "prime-fever",
   "metadata": {},
   "source": [
    "### Noms:\n",
    "Introdueix en aquesta cel·la els noms dels dos integrants del grup:\\\n",
    "*Alumne 1* \\\n",
    "*Alumne 2*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "following-weapon",
   "metadata": {},
   "source": [
    "## Part 1: conjunt de textos de \"mundocine\"\n",
    "En aquest primer conjunt de dades tenim una sèrie de crítiques de pel·lícules de cinema, emmagatzemades en format XML (una crítica per arxiu). Hem preparat una funció de tipus `generator` que processa el directori on estan els arxius de les crítiques i retorna per cada arxiu XML una tupla amb 4 valors:\n",
    " - Nom de la pel·lícula (string)\n",
    " - Resum breu de la crítica (string)\n",
    " - Text de la crítica (*string*)\n",
    " - Valoració de la pel·lícula (*int* d'1 a 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from xml.dom.minidom import parseString\n",
    "\n",
    "def parse_folder(path):\n",
    "    \"\"\"generator that reads the contents of XML files in a folder\n",
    "    Returns the <body> of the <review> in each XML file.\n",
    "    XML files encoded as 'latin-1'\"\"\"\n",
    "    for file in sorted([f for f in os.listdir(path) if f.endswith('.xml')],\n",
    "                        key=lambda x: int(re.match(r'\\d+',x).group())):\n",
    "        with open(os.path.join(path, file), encoding='latin-1') as f:\n",
    "            doc=parseString(f.read())\n",
    "\n",
    "            titulo = doc.documentElement.attributes[\"title\"].value\n",
    "\n",
    "            btxt = \"\"\n",
    "            review_bod = doc.getElementsByTagName(\"body\")\n",
    "            if len(review_bod) > 0:\n",
    "                for node in review_bod[0].childNodes:\n",
    "                    if node.nodeType == node.TEXT_NODE:\n",
    "                        btxt += node.data + \" \"\n",
    "\n",
    "            rtxt = \"\"\n",
    "            review_summ = doc.getElementsByTagName(\"summary\")\n",
    "            if len(review_summ) > 0:\n",
    "                for node in review_summ[0].childNodes:\n",
    "                    if node.nodeType == node.TEXT_NODE:\n",
    "                        rtxt += node.data + \" \"\n",
    "                        \n",
    "            rank = int(doc.documentElement.attributes[\"rank\"].value)\n",
    "            \n",
    "            yield titulo, rtxt, btxt, rank\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "automotive-gentleman",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Les crítiques es troben en el directori \"critiques\" (si no tens el directori, descomprimeix l'arxiu \"critiques.zip\" que s'entrega en el material de la pràctica. \\\n",
    "Càrrega la primera crítica del directori usant el mètode `next` sobre la funció `parse_folder` en l'objecte `critica`. Mostra els seus 4 valors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "critica = ____(____())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "excellent-guitar",
   "metadata": {},
   "source": [
    "## Anàlisi exploratòria\n",
    "Abans de processar el text calcularem una sèrie de paràmetres de cada crítica. \\\n",
    "Per a això processem cada crítica per a i guardarem els resultats en un objecte `DataFrame` de Colles.\\\n",
    "Com a característica de cada crítica extraurem:\n",
    "- Títol de la pel·lícula\n",
    "- Longitud (en caràcters) del resum\n",
    "- Longitud (en caràcters) del text de la crítica\n",
    "- Puntuació de la crítica\n",
    "\n",
    "### Exercici\n",
    "Completa el codi següent per a generar el `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#creamos una lista en blanco\n",
    "datos = __\n",
    "\n",
    "#recorremos las críticas y calculamos sus métricas\n",
    "for c in parse_folder(_____):\n",
    "    datos.append({\n",
    "        'título': ___,\n",
    "        'LongResumen': ___(___),\n",
    "        'LongCritica': ___(___),\n",
    "        'puntuación': ___\n",
    "    })\n",
    "\n",
    "resumen = pd.DataFrame(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "resumen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cordless-combat",
   "metadata": {},
   "source": [
    "## Neteja de text\n",
    "Prepararem aquest conjunt de textos per a entrenar un model per a predir la puntuació de cada crítica a partir del text de la crítica.\\\n",
    "Realitzarem el següent processament:\n",
    "- Separar el text en *tokens*\n",
    "- Eliminar els *tokens* de tipus *stop-word*, signes de puntuació o espais\n",
    "- Convertir les entitats de tipus `PER` al token *persona*\n",
    "- Lematizar el text\n",
    "\n",
    "### Exercici\n",
    "Completa el codi següent per a realitzar aquestes funcions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "#definimos función de normalizado\n",
    "def normaliza(texto):\n",
    "    doc = nlp(texto)\n",
    "    tokens = #devuelve los tokens que cumplen las condiciones\n",
    "    palabras = []\n",
    "    for t in tokens:\n",
    "        if t.ent_iob_=='B' and t.ent_type_=='PER':\n",
    "            palabras.append('persona')\n",
    "        elif t.ent_iob_=='I' and t.ent_type_=='PER':\n",
    "            continue\n",
    "        else:\n",
    "            palabras.append(_._____) #si no es PER añadimos el lema\n",
    "    salida = #junta todos los tokens en un string\n",
    "    \n",
    "    return salida"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "injured-softball",
   "metadata": {},
   "source": [
    "Comprova el seu funcionament en la crítica prèviament descarregada (variable `critica`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "normaliza(____)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beneficial-arkansas",
   "metadata": {},
   "source": [
    "### Possibles millores sobre el processament\n",
    "Observem els següents problemes:\n",
    "- Algunes paraules no se separen correctament perquè en el text original estan unides per signes de puntuació.\n",
    "- La llista de *stop-words* de spaCy no conté 'a','e','i'.\n",
    "- Alguns lemes mantenen les majúscules.\\\n",
    "\n",
    "Redefinim la funció de normalització per a corregir això:\n",
    "- Introduïm un espai després de determinats signes de puntuació (\".\", \"?\") perquè la divisió en tokens siga correcta\n",
    "- Filtrem els *tokens* amb una longitud d'1\n",
    "- Passem a minúscules el lema de cada token\n",
    "\n",
    "Completa la funció per a realitzar aquestes correccions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza_bis(texto):\n",
    "    texto = re.sub(r\"___\", r\"___\", texto) #añadimos un espacio después de \".\" y \"?\"\n",
    "    doc = nlp(texto)\n",
    "    tokens = #filtramos los tokens que nos interesan\n",
    "    palabras = []\n",
    "    for t in tokens:\n",
    "        if t.ent_iob_=='B' and t.ent_type_=='PER':\n",
    "            palabras.append('persona')\n",
    "        elif t.ent_iob_=='I' and t.ent_type_=='PER':\n",
    "            continue\n",
    "        else:\n",
    "            palabras.append(___) #añadimos lema en minúsculas\n",
    "    salida = #junta todos los tokens en un string\n",
    "    \n",
    "    return salida"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "younger-running",
   "metadata": {},
   "source": [
    "Comprova el seu funcionament en la crítica prèviament descarregada (variable `critica`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "normaliza_bis(____)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "funny-parcel",
   "metadata": {},
   "source": [
    "### Anàlisi morfològica\n",
    "En una crítica té molta importància els adjectius utilitzats.\\\n",
    "Crea una funció per a filtrar només els adjectius utilitzats en cada crítica (utilitza el lema de cada adjectiu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_adj(texto):\n",
    "    texto = re.sub(___, ___, texto) #separamos . y ?\n",
    "    doc = nlp(texto)\n",
    "    tokens = #obtenemos lema de todos los tokens de tipo ADJ\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae1e902",
   "metadata": {},
   "source": [
    "Comprova el seu funcionament en la crítica prèviament descarregada (variable `critica`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraer_adj(____)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "limited-integrity",
   "metadata": {},
   "source": [
    "### Processament de tot el conjunt de dades\n",
    "Aplicarem aquestes funcions en bloc a tot el conjunt de dades. \\\n",
    "En aquesta ocasió, no farem res amb el text normalitzat sinó que només ho aplicarem per a calcular el núm. de paraules i el núm. d'adjectius de cada crítica.\n",
    "\n",
    "### Exercici\n",
    "Completa el codi següent:\\\n",
    "Nota: tingues en compte que per a comptar paraules has de dividir el *string* en espais i comptar el núm. d'elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos una lista en blanco\n",
    "datos = []\n",
    "\n",
    "#recorremos las críticas y calculamos sus métricas\n",
    "for c in parse_folder(\"criticas\"):\n",
    "    datos.append({\n",
    "        'título': ___,\n",
    "        'LongResumen': ___,\n",
    "        'LongCritica': ___,\n",
    "        'NumPalabras': ___,\n",
    "        'NumAdj': ___,\n",
    "        'puntuación': c[3]\n",
    "    })\n",
    "\n",
    "resumen = pd.DataFrame(datos)\n",
    "resumen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "laden-special",
   "metadata": {},
   "source": [
    "## Part 2: Extracció de característiques *sparse*\n",
    "Anem a calcules les matrius de característiques *bag-of-words* i *tfidf* del conjunt de textos anterior.\\\n",
    "Usarem la llibreria `scikit-learn` per a vectorizar els documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para no tener que cargar todas las críticas en memoria,\n",
    "#creamos un generador que devuelve iterativamente el\n",
    "#texto procesado de cada crítica\n",
    "\n",
    "def generaCritica(criticas):\n",
    "    \"\"\"Función de tipo generator que devuelve el\n",
    "    texto normalizado de cada crítica.\n",
    "    Entrada:\n",
    "    criticas: objeto 'parse_folder' que itera\n",
    "    sobre el directio de las críticas\n",
    "    Salida:\n",
    "    texto normalizado de cada crítica\"\"\"\n",
    "    for c in criticas:\n",
    "        yield normaliza_bis(c[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "young-receptor",
   "metadata": {},
   "source": [
    "Comprova el seu funcionament generant el text normalitzat de la primera crítica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(generaCritica(____(____)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adjusted-administration",
   "metadata": {},
   "source": [
    "Vectorizem tot el conjunt de dades usant les funcions de `scikit-learn`.\\\n",
    "Aquestes funcions admeten un objecte `generator` com a argument d'entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "\n",
    "criticas = #creamos el objeto generador\n",
    "BoW_criticas = #entrena y transforma con el corpus de críticas\n",
    "BoW_criticas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "stock-wings",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Genera diferents variants de matrius de característiques per al conjunt de les crítiques. Prova amb:\n",
    "- Matriu TF-IDF\n",
    "- Matriu BoW amb unigrames i bigrames\n",
    "- Matriu TF-IDF eliminant les paraules menys freqüents i les més freqüents (mínim de 2 i màxim de 5 documents)\n",
    "- Mostra quines són les paraules més freqüents eliminades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matriz TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matriz BoW con unigramas y bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matriz TF-IDF con min_df=1 y max_df=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#palabras más frecuentes a eliminar\n",
    "#aparecen en el atributo 'stop_words_' del vectorizador"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "primary-intention",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "Ara calcularem els *word vectors* de les paraules del nostre conjunt de dades, usant la classe `word2vec` de la llibreria `gensim`.\\\n",
    "Aquesta llibreria accepta com a argument d'entrada un objecte `iterador` que generarà el text pre-processament de la següent crítica en la seqüència.\\\n",
    "Usarem les funcions de pre-processament de la llibreria `gensim`.\\\n",
    "Primer definim un objecte de tipus `iterator` per a recórrer les crítiques. Es diferencia d'un simple `generator` que es pot reiniciar la generació de la seqüència (necessari per al model `word2vec`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "        \n",
    "class PreprocesaCriticas(object):\n",
    "    \"\"\"Pre-procesa el corpus de críticas con la función 'simple_preprocess'\n",
    "    de la librería gensim\n",
    "    Entrada: directorio de críticas\n",
    "    Salida: iterador sobre las críticas (como lista de tokens)\"\"\"\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for c in parse_folder(self.dirname):\n",
    "            yield simple_preprocess(c[2], deacc=True, min_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instanciamos un objeto para nuestro directorio\n",
    "criticas = PreprocesaCriticas(____)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "pending-passion",
   "metadata": {},
   "source": [
    "Per a provar el seu funcionament amb la primera crítica el convertim en iterable i usem el mètode `next`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#completar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "realistic-holiday",
   "metadata": {},
   "source": [
    "Al contrari que l'objecte generat amb la funció `generaCriticas` l'objecte de `PreprocesaCriticas` es reinicia cada vegada que iterem. \\\n",
    "Calculem els vectors de paraules de tot el corpus amb el model `word2vec` que accepta com a argument d'entrada un objecte de tipus `iterator` com el creat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cálculo de los vectores de palabras\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(___, #iterador con los documentos\n",
    "                               size=10,          #tamaño del vector\n",
    "                               window=5,         #nº de términos adyacentes que usamos para el cálculo\n",
    "                               min_count=5,      #nº mínimo de apariciones del término para contarlo\n",
    "                               iter=100\n",
    "                              )\n",
    "\n",
    "#una vez entrenado el modelo nos quedamos con los vectores calculados\n",
    "#si no se van a actualizar los vectores con nuevos documentos\n",
    "model = model.wv\n",
    "len(model.vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "chronic-greensboro",
   "metadata": {},
   "source": [
    "Seleccionem aleatòriament 25 paraules del conjunt calculat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "palabras_sample = np.random.choice(model.index2word, 25, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "superb-terry",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Comprova com funciona el model buscant les paraules més similars semànticament a \"trama\", \"peli\" i \"pelicula\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#palabras similares a 'trama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#palabras similares a 'peli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "#palabras similares a 'pelicula'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "circular-wheat",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Veurem la influència del preprocessament. Per a això alimentarem el model amb les crítiques *preprocesadas amb la funció de normalització sobre `spaCy` (que produeix text lematizat i amb un altre filtrat).\\\n",
    "Per a això cal re-definir l'objecte `iterador` sobre el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparamos con un modelo que use el preprocesado de spaCy\n",
    "        \n",
    "class PreprocesaCriticasSpacy(object):\n",
    "    \"\"\"Pre-procesa el corpus de críticas con la función de normalización\n",
    "    definida con la librería spaCy\n",
    "    Entrada: directorio de críticas\n",
    "    Salida: iterador sobre las críticas (como lista de tokens)\"\"\"\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for c in parse_folder(self.dirname):\n",
    "            yield ________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "pointed-vault",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Instància aquesta nova classe per al directori de les crítiques en l'objecte `critiques_spacy` i comprova el seu funcionament sobre la primera crítica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "criticas_spacy = PreprocesaCriticasSpacy(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prueba a generar la primera crítica"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "furnished-serial",
   "metadata": {},
   "source": [
    "Calcula el model de vectors de paraules amb aquest nou *pre-processat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelSpacy = Word2Vec(_____, #iterador con los documentos\n",
    "                               size=10,          #tamaño del vector\n",
    "                               window=5,         #nº de términos adyacentes que usamos para el cálculo\n",
    "                               min_count=5,      #nº mínimo de apariciones del término para contarlo\n",
    "                               iter=100\n",
    "                              )\n",
    "\n",
    "#una vez entrenado el modelo nos quedamos con los vectores calculados\n",
    "#si no se van a actualizar los vectores con nuevos documentos\n",
    "modelSpacy = modelSpacy.wv\n",
    "len(modelSpacy.vocab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "weighted-bracelet",
   "metadata": {},
   "source": [
    "### Pregunta\n",
    "per què el nou model té un vocabulari amb molts menys termes que el model anterior?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sound-drawing",
   "metadata": {},
   "source": [
    "### Visualització de word vectors\n",
    "Usem una reducció de dimensionalitat t-SNE per a visualitzar un grup de paraules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "palabras_vectors = model[model.index2word]\n",
    "\n",
    "#seleccinamos unos pocos términos para visualizarlos entre el conjunto\n",
    "random_idx = np.random.randint(len(model.index2word), size=5)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=250, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(palabras_vectors)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='steelblue', alpha=0.1)\n",
    "\n",
    "labels = np.array(model.index2word)[random_idx]\n",
    "\n",
    "\n",
    "T_labels = T[random_idx,:]\n",
    "\n",
    "plt.scatter(T_labels[:, 0], T_labels[:, 1], c='lime', edgecolors='darkgreen')\n",
    "for label, x, y in zip(labels, T_labels[:, 0], T_labels[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "advance-success",
   "metadata": {},
   "source": [
    "Ara carregarem els vectors per a les mateixes paraules amb el model pre-entrenat GloVe de `spaCy` per a comparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_vectors = [nlp.vocab[t].vector for t in model.index2word]\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=250, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(palabras_vectors)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='steelblue', alpha=0.1)\n",
    "\n",
    "labels = np.array(model.index2word)[random_idx]\n",
    "\n",
    "\n",
    "T_labels = T[random_idx,:]\n",
    "\n",
    "plt.scatter(T_labels[:, 0], T_labels[:, 1], c='lime', edgecolors='darkgreen')\n",
    "for label, x, y in zip(labels, T_labels[:, 0], T_labels[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "outdoor-flight",
   "metadata": {},
   "source": [
    "## Part 3: Conjunt de Tuits en espanyol\n",
    "Anem a pre-processar un conjunt de tuits en espanyol etiquetatges amb la seua polaritat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Leemos los datos\n",
    "df = pd.read_csv('tweets_all.csv', index_col=None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "incredible-boating",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Defineix una funció de normalització que faça les següents tasques:\n",
    "- Eliminar esments i URL mitjançant un patró RegEx\n",
    "- Separar el text en *tokens* convertint-los a minúscules, eliminant els que siguen signes de puntuació, espais o dígits\n",
    "- Eliminar els stop-words d'una llista pròpia passada com a argument\n",
    "- Eliminar els símbols de puntuació dels tokens (etiquetes, admiracions, etc.)\n",
    "- Eliminar els tokens d'una longitud menor de 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "#lista de stop-words específicos de nuestro corpus (aproximación)\n",
    "stop_words = ['los', 'pero', 'por', 'que', 'una']\n",
    "\n",
    "patron = re.compile('[{}]'.format(re.escape(string.punctuation))) #elimina símbolos de puntuación\n",
    "\n",
    "def clean_text(text, stop_words=stop_words):\n",
    "    \"\"\"Limpiamos las menciones y URL del texto. Luego convertimos en tokens\n",
    "    y eliminamos signos de puntuación.\n",
    "    Dejamos tokens en minúsculas.\n",
    "    Como salida volvemos a convertir los tokens en cadena de texto\"\"\"\n",
    "    text = ___ #elimina menciones y URL\n",
    "    tokens = nlp(text)\n",
    "    tokens = ___ #filtra tokens (puntuaciones, espacios y dígitos)\n",
    "    filtered_tokens = ___ #limpia tokens (signos de puntuación, stop-words y longitud<2)\n",
    "    filtered_text = ___ #juntam,os como string\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "vulnerable-consensus",
   "metadata": {},
   "source": [
    "Aplica la funció a tots els tuits (columna 'content') creant una nova columna 'net' del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['limpio'] = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-glory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
