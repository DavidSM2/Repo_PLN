{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "documentary-preserve",
   "metadata": {},
   "source": [
    "# Llibreria `spaCy`\n",
    "En aquesta pràctica usarem la llibreria de NLP spaCy. En Anaconda es pot instal·lar amb:\\\n",
    "```conda install -c conda-forge spacy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mental-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos librerías necesarias\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "random-voluntary",
   "metadata": {},
   "source": [
    "# Introducció a spaCy\n",
    "\n",
    "En aquest apartat veurem els conceptes més importants de spaCy i com començar a usar-lo.\n",
    "\n",
    "### L'objecte nlp\n",
    "\n",
    "```python\n",
    "# Importa la clase de lenguaje \"Spanish\"\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "# Crea el objeto nlp\n",
    "nlp = Spanish()\n",
    "```\n",
    "\n",
    "- conté el pipeline de processament\n",
    "- inclou les regles específiques del seu llenguatge per a tokenizar, etc.\n",
    "\n",
    "En el centre de spaCy està l'objecte que conté el <abbr title=\"Un pipeline és una sèrie d'accions que s'executen en seqüència. Cada pas depén de l'anterior usant el seu resultat.\">pipeline</abbr> de processament. Normalment anomenem \"nlp\" a aquesta variable.\n",
    "\n",
    "Per exemple, per a crear un objecte `nlp` d'espanyol pots importar la classe de llenguatge `Spanish` de `spacy.lang.es` i crear un <abbr title=\"És un exemplar d'una classe, a vegades referit incorrectament com a instància.\">instance</abbr>. Pots usar l'objecte nlp com una funció per a analitzar el text.\n",
    "\n",
    "Conté tots els components diferents d'un pipeline.\n",
    "\n",
    "També inclou les regles específiques del seu llenguatge usades per a convertir el text en tokens amb paraules i puntuació. spaCy ofereix suport per a diversos llenguatges que estan disponibles en `spacy.lang`.\n",
    "\n",
    "### L'objecte Doc\n",
    "\n",
    "```python\n",
    "# Creado procesando un string de texto con el objeto nlp\n",
    "doc = nlp(\"¡Hola Mundo!\")\n",
    "\n",
    "# Itera sobre los tokens en un Doc\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "```\n",
    "\n",
    "```out\n",
    "¡\n",
    "Hola\n",
    "Mundo\n",
    "!\n",
    "```\n",
    "\n",
    "Quan processes un <abbr title=\"El tipus de dada de Python per a text.\">string</abbr> de text amb l'objecte `nlp`, spaCy crea un objecte `Doc` - de \"document\".\n",
    "El Doc et permet accedir a la informació sobre el text en una forma estructurada i sense perdre informació.\n",
    "\n",
    "El Doc es comporta com una seqüència normal de Python i et permet iterar sobre els seus tokens o obtindre un token amb el seu índex. Més endavant parlarem més d'això.\n",
    "\n",
    "### L'objecte Token\n",
    "\n",
    "<img src=\"P3_materiales/doc.png\" alt=\"Illustration of a Doc object containing four tokens\" width=\"50%\" />\n",
    "\n",
    "```python\n",
    "doc = nlp(\"¡Hola Mundo!\")\n",
    "\n",
    "# Usa el índice del Doc para obtener un solo Token\n",
    "token = doc[2]\n",
    "\n",
    "# Obtén el texto del token a través del atributo .text\n",
    "print(token.text)\n",
    "```\n",
    "\n",
    "```out\n",
    "Mundo\n",
    "```\n",
    "\n",
    "Els objectes `Token` representen als tokens en un document. Per exemple, una paraula o un signe de puntuació.\n",
    "\n",
    "Per a obtindre el token en una posició específica pots usar l'índex del doc.\n",
    "\n",
    "Els objectes `Token` també proveeixen diversos atributs que et permeten accedir a més informació sobre els tokens. Per exemple, l'atribut `.text` retorna el text del token.\n",
    "\n",
    "---\n",
    "\n",
    "### L'objecte Span\n",
    "\n",
    "<img src=\"P3_materiales/doc_span.png\" width=\"50%\" alt=\"Illustration of a Doc object containing four tokens and three of them wrapped in a Span\" />\n",
    "\n",
    "```python\n",
    "doc = nlp(\"¡Hola Mundo!\")\n",
    "\n",
    "# Un slice de un Doc en un objeto Span\n",
    "span = doc[2:4]\n",
    "\n",
    "# Obtén el texto del span a través del atributo .text\n",
    "print(span.text)\n",
    "```\n",
    "\n",
    "```out\n",
    "Mundo!\n",
    "```\n",
    "\n",
    "Un objecte `Span` és un <abbr title=\"Un slice és un subconjunt d'elements dins d'una seqüència de dades com una llista o un objecte Doc.\">slice</abbr> d'un document compost per un o més tokens. És només un <abbr title=\"En espanyol: representació o vista.\">view</abbr> d'un `Doc` i no conté les dades en si.\n",
    "\n",
    "Per a crear un span pots usar la notació de slice de Python. Per exemple, `2:4` crea un slice que comença en el token en la posició 2 fins a - però no incloent! - el token en la posició 4.\n",
    "\n",
    "### Atributs lèxics\n",
    "\n",
    "```python\n",
    "doc = nlp(\"Eso cuesta €5.\")\n",
    "```\n",
    "\n",
    "```python\n",
    "print(\"Index:   \", [token.i for token in doc])\n",
    "print(\"Text:    \", [token.text for token in doc])\n",
    "\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])\n",
    "```\n",
    "\n",
    "```out\n",
    "Index:    [0, 1, 2, 3, 4]\n",
    "Text:     ['Eso', 'cuesta', '€', '5', '.']\n",
    "\n",
    "is_alpha: [True, True, False, False, False]\n",
    "is_punct: [False, False, False, False, True]\n",
    "like_num: [False, False, False, True, False]\n",
    "```\n",
    "\n",
    "Ací pots veure alguns dels atributs disponibles dels tokens:\n",
    "\n",
    "`i` és l'índex del token dins del document pare.\n",
    "\n",
    "`text` retorna el text del token.\n",
    "\n",
    "`is_alpha`, `is_punct` i `like_num` retornen valors booleans que indiquen si un token està compost per caràcters alfabètics, si és puntuació, o si _sembla_ un número. Per exemple, el token \"10\" - un, zero - o la paraula \"diez\" - D, I, E, Z.\n",
    "\n",
    "Aquests atributs també es diuen atributs lèxics: es refereixen a una entrada en el vocabulari i no depenen del context del token."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "incorrect-healing",
   "metadata": {},
   "source": [
    "## Documents, spans i tokens\n",
    "\n",
    "Quan anomenes `nlp` sobre un string, spaCy primer genera tokens del text i\n",
    "crea un objecte de document. En aquest exercici aprendràs més sobre el `Doc`,\n",
    "així com dels seus\n",
    "<abbr title=\"En espanyol: representacions o vistes.\">views</abbr> `Token` i\n",
    "`Span`.\n",
    "\n",
    "### Pas 1\n",
    "\n",
    "- Importa la classe de llenguatge `Spanish` i crea l'objecte `nlp`.\n",
    "- Processa el text i genera un\n",
    " <abbr title=\"En espanyol: exemplar, a vegades referit incorrectament com a instància.\">instance</abbr>\n",
    " d'un objecte `Doc` en la variable `doc`.\n",
    "- Selecciona el primer token de `Doc` i imprimeix en pantalla la seua `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "identical-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me\n"
     ]
    }
   ],
   "source": [
    "# Importa la clase de lenguaje \"Spanish\" y crea el objeto nlp\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(\"Me gustan las panteras negras y los leones.\")\n",
    "\n",
    "# Selecciona el primer token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Imprime en pantalla el texto del token\n",
    "print(first_token.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "insured-eclipse",
   "metadata": {},
   "source": [
    "### Pas 2\n",
    "\n",
    "- Importa la classe de llenguatge `Spanish` i crea l'objecte `nlp`.\n",
    "- Processa el text i genera un\n",
    " <abbr title=\"En espanyol: exemplar, a vegades referit incorrectament com a instància.\">instance</abbr>\n",
    " d'un objecte `Doc` en la variable `doc`.\n",
    "- Crea un slice de `Doc` per als tokens \"panteras negras\" i \"panteras negras y los leones\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convertible-fence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panteras negras\n",
      "panteras negras y los leones\n"
     ]
    }
   ],
   "source": [
    "# Importa la clase de lenguaje \"Spanish\" y crea el objeto nlp\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(\"Me gustan las panteras negras y los leones.\")\n",
    "\n",
    "# Un slice del Doc para \"panteras negras\"\n",
    "panteras_negras = doc[3:5]\n",
    "print(panteras_negras.text)\n",
    "\n",
    "# Un slice del Doc para \"panteras negras y los leones\" (sin el \".\")\n",
    "panteras_negras_y_leones = doc[3:8]\n",
    "print(panteras_negras_y_leones.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-agency",
   "metadata": {},
   "source": [
    "## Atributs lèxics\n",
    "\n",
    "En aquest exemple usaràs els objectes `Doc` i `Token` de spaCy i els atributs\n",
    "lèxics per a trobar percentatges en el text. Estaràs buscant dos tokens\n",
    "subseqüents: un número i un símbol de percentatge.\n",
    "\n",
    "- Usa l'atribut `like_num` del token per a revisar si un token en el `doc`\n",
    " sembla un número.\n",
    "- Presa el token que segueix al token actual en el document. L'índex del\n",
    " següent token en el `doc` és `token.i + 1`.\n",
    "- Revisa si l'atribut `text` del següent token és un símbol de percentatge\n",
    " \"%\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daily-davis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje encontrado:: 60\n",
      "Porcentaje encontrado:: 4\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.es import Spanish\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(\n",
    "    \"En 1990, más del 60 % de las personas en Asia del Este se encontraban \"\n",
    "    \"en extrema pobreza. Ahora, menos del 4 % lo están.\"\n",
    ")\n",
    "\n",
    "# Itera sobre los tokens en el doc\n",
    "for token in doc:\n",
    "    # Revisa si el token parece un número\n",
    "    if token.like_num:\n",
    "        # Obtén el próximo token en el documento\n",
    "        next_token = doc[token.i + 1]\n",
    "        # Revisa si el texto del siguiente token es igual a '%'\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Porcentaje encontrado::\", token.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "elect-budapest",
   "metadata": {},
   "source": [
    "# Models estadístics\n",
    "En aquest apartat veiem els models estadístics de spaCy.\n",
    "\n",
    "### Què són els models estadístics?\n",
    "\n",
    "Algunes de les coses més interessants que pots analitzar són específiques al context: per exemple, si una paraula és un verb o si un span de text és el nom d'una persona.\n",
    "\n",
    "- Li permeten a spaCy predir atributs lingüístics _en context_\n",
    " - Part-of-speech tags\n",
    " - Dependències sintàctiques\n",
    " - Entitats nomenades\n",
    "- Entrenats amb textos d'exemple anotats\n",
    "- Poden ser actualitzats amb més exemples per a afinar les prediccions\n",
    "\n",
    "### Paquets de models\n",
    "\n",
    "spaCy proveeix diversos paquets de models pre-entrenats que pots descarregar usant el comando `spacy download`. Per exemple, el paquet \"és_core_news_sm\" és un model xicotet d'espanyol que proveeix suport per a totes les capacitats centrals i està entrenat usant textos de notícies.\n",
    "\n",
    "El mètode `spacy.load` càrrega el paquet de model pel seu nom i retorna un objecte `nlp`.\n",
    "\n",
    "El paquet proveeix els <abbr title=\"En anglés: en un context de Machine Learning, coneguts com binary weights.\">paràmetres binaris</abbr> que li permeten a spaCy fer prediccions.\n",
    "\n",
    "També inclou el vocabulari i la metadata perquè spaCy sàpia quin classe de llenguatge usar i com configurar el pipeline de processament.\n",
    "\n",
    "### Predient part-of-speech tags\n",
    "\n",
    "En aquest exemple estem usant spaCy per a predir part-of-speech tags, els tipus de paraules en el context.\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Carga el modelo pequeño de español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Ella comió pizza\")\n",
    "\n",
    "# Itera sobre los tokens\n",
    "for token in doc:\n",
    "    # Imprime en pantalla el texto y el part-of-speech tag predicho\n",
    "    print(token.text, token.pos_)\n",
    "```\n",
    "\n",
    "```out\n",
    "Ella PRON\n",
    "comió VERB\n",
    "pizza NOUN\n",
    "```\n",
    "\n",
    "Primer, carreguem el model xicotet d'espanyol i rebem un objecte `nlp`.\n",
    "\n",
    "Després, processem el text \"Ella va menjar pizza\".\n",
    "\n",
    "Per a cada token en el Doc podem imprimir en pantalla el text i el part-of-speech tag predit usant l'atribut `.post_`.\n",
    "\n",
    "En spaCy, els atributs que retornen un string normalment acaben amb un guió baix (`_`). mentre que atributs sense un guió baix retornen un valor ANEU de tipus <abbr title=\"En anglés: integer, un nombre enter sense part decimal.\">sencer</abbr>.\n",
    "\n",
    "Ací el model va predir correctament \"va menjar\" com el verb i \"pizza\" com el substantiu.\n",
    "\n",
    "### Predient dependències sintàctiques\n",
    "\n",
    "```python\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "```\n",
    "\n",
    "```out\n",
    "Ella PRON nsubj comió\n",
    "comió VERB ROOT comió\n",
    "pizza NOUN obj comió\n",
    "```\n",
    "\n",
    "A més dels part-of-speech tags, també podem predir les relacions entre les paraules. Per exemple, si una paraula és el subjecte o l'objecte d'una oració.\n",
    "\n",
    "L'atribut `.dep_` retorna el dependency label predit.\n",
    "\n",
    "L'atribut `.head` retorna el token <abbr title=\"En anglés: head.\">cap</abbr> sintàctic. Una altra manera de pensar-ho és com el token pare al qual aquesta paraula està lligada.\n",
    "\n",
    "### Esquema de dependency label\n",
    "\n",
    "<img src=\"P3_materiales/dep_example_es.png\" alt=\"Visualization of the dependency graph for 'Ella comió la pizza'\" />\n",
    "\n",
    "| Label     | Descripción             | Ejemplo |\n",
    "| --------- | ----------------------- | ------- |\n",
    "| nsubj | sujeto nominal          | Ella    |\n",
    "| obj   | objeto                  | pizza   |\n",
    "\n",
    "spaCy usa un esquema de <abbr title=\"En espanyol es coneixen com a etiquetes, però per a mantindre la diferenciació entre label i tag, les usem en anglés.\">labels</abbr> estàndard per a descriure dependències sintàctiques. Ací veuràs un exemple d'algunes labels comunes:\n",
    "\n",
    "El pronom \"Ella\" és un subjecte nominal unit al verb - en aquest cas, a \"va menjar\".\n",
    "\n",
    "El substantiu \"pizza\" és un objecte unit al verb \"va menjar\". Està sent menjat pel subjecte \"ella\".\n",
    "\n",
    "### Predient entitats nomenades\n",
    "\n",
    "<img src=\"P3_materiales/ner_example_es.png\" alt=\"Visualization of the named entities in 'Apple es la marca que más satisfacción genera en EE.UU.; pero el iPhone, fue superado por el Galaxy Note 9'\" width=\"80%\" />\n",
    "\n",
    "```python\n",
    "# Procesa un texto\n",
    "doc = nlp(\n",
    "    \"Apple es la marca que más satisfacción genera en EE.UU., \"\n",
    "    \"pero el iPhone, fue superado por el Galaxy Note 9\"\n",
    ")\n",
    "\n",
    "# Itera sobre las entidades predichas\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto y el label de la entidad\n",
    "    print(ent.text, ent.label_)\n",
    "```\n",
    "\n",
    "```out\n",
    "Apple ORG\n",
    "EE.UU LOC\n",
    "iPhone MISC\n",
    "Galaxy Note 9 MISC\n",
    "```\n",
    "\n",
    "Les entitats nomenades són \"objectes de la vida real\" que tenen un nom assignat. Per exemple, una persona, una organització o un país.\n",
    "\n",
    "La propietat `doc.ents` et permet accedir a les entitats nomenades predites pel model.\n",
    "\n",
    "Retorna un iterador d'objectes `Span`, així que podem imprimir en pantalla el text i el label de l'entitat usant l'atribut `.label_`.\n",
    "\n",
    "En aquest cas, el model va predir correctament \"Apple\" com una organització, \"els EUA\" com un lloc, \"iPhone\" i \"Galaxy Note 9\" amb la categoria miscelanea.\n",
    "\n",
    "### Truc: el mètode spacy.explain\n",
    "\n",
    "Per a obtindre definicions dels tags i labels més comuns pots usar la funció assistent `spacy.explain`.\n",
    "\n",
    "```python\n",
    "spacy.explain(\"LOC\")\n",
    "```\n",
    "\n",
    "```out\n",
    "'Name of politically or geographically defined location'\n",
    "```\n",
    "\n",
    "```python\n",
    "spacy.explain(\"NNP\")\n",
    "```\n",
    "\n",
    "```out\n",
    "'noun, proper singular'\n",
    "```\n",
    "\n",
    "```python\n",
    "spacy.explain(\"MISC\")\n",
    "```\n",
    "\n",
    "```out\n",
    "'Miscellaneous entities, e.g. events, nationalities, products or works of art'\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "extended-level",
   "metadata": {},
   "source": [
    "## Models de spaCy\n",
    "\n",
    "Per a usar un model de `spaCy` és necessari instal·lar-lo primer. Per a\n",
    "obtindre més detalls sobre els models estadístics de spaCy i com instal·lar-los\n",
    "en la teua màquina revisa [la documentació](https://spacy.io/usage/models).\n",
    "\n",
    "- Usa `spacy.load` per a carregar el model xicotet d'espanyol `\"es_core_news_sm\"`.\n",
    "- Processa el text i imprimeix en pantalla el text del document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "higher-milton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De acuerdo con la revista Fortune, Apple fue la empresa más admirada en el mundo entre 2008 y 2012.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Carga el modelo \"es_core_news_sm\"\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = (\n",
    "    \"De acuerdo con la revista Fortune, Apple fue la empresa \"\n",
    "    \"más admirada en el mundo entre 2008 y 2012.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# Imprime en pantalla el texto del documento\n",
    "print(doc.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lined-split",
   "metadata": {},
   "source": [
    "## Predient anotacions lingüístiques\n",
    "Ara provarem un dels paquets de models pre-entrenats de spaCy i veure\n",
    "les seues prediccions en acció. Per a\n",
    "esbrinar el que cada tag o label significa pots cridar a `spacy.explain` en\n",
    "el\n",
    "<abbr title=\"En espanyol: bucle, un bloc de codi que es repeteix.\">loop</abbr>.\n",
    "Per exemple, `spacy.explain(\"PROPN\")` o `spacy.explain(\"GPE\")`.\n",
    "\n",
    "### Part 1\n",
    "\n",
    "- Processa el text de l'objecte `nlp` i crea un `doc`.\n",
    "- Per a cada token imprimeix en pantalla el seu text, el seu `.post_` (part-of-speech tag)\n",
    " i el seu `.dep_` (dependency label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "first-excitement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De          0         case      \n",
      "acuerdo     1         fixed     \n",
      "con         2         fixed     \n",
      "la          3         det       \n",
      "revista     4         obl       \n",
      "Fortune     5         appos     \n",
      ",           6         punct     \n",
      "Apple       7         nsubj     \n",
      "fue         8         cop       \n",
      "la          9         det       \n",
      "empresa     10        ROOT      \n",
      "más         11        advmod    \n",
      "admirada    12        amod      \n",
      "en          13        case      \n",
      "el          14        det       \n",
      "mundo       15        obl       \n",
      "entre       16        case      \n",
      "2008        17        obl       \n",
      "y           18        cc        \n",
      "2012        19        conj      \n",
      ".           20        punct     \n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = (\n",
    "    \"De acuerdo con la revista Fortune, Apple fue la empresa \"\n",
    "    \"más admirada en el mundo entre 2008 y 2012.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Obtén el texto del token, el part-of-speech tag y el dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.i\n",
    "    token_dep = token.dep_\n",
    "    # Esto es solo por formato\n",
    "    print(\"{:<12}{:<10}{:<10}\".format(token_text, token_pos, token_dep))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "elementary-funds",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "- Processa el text i crea un objecte `doc`.\n",
    "- Itera sobre els `doc.ents` i imprimeix en pantalla el text de l'entitat i l'atribut\n",
    " `label_`.\n",
    "\n",
    "<codeblock id=\"01_08_02\">\n",
    "\n",
    "Per a crear un `doc` diu l'objecte `nlp` en un string de text. Recorda que\n",
    "necessites usar els noms dels atributs dels tokens amb un guió baix\n",
    "(`_`) per a obtindre el valor del string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "better-float",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fortune ORG\n",
      "Apple ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = (\n",
    "    \"De acuerdo con la revista Fortune, Apple fue la empresa \"\n",
    "    \"más admirada en el mundo entre 2008 y 2012.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# Itera sobre las entidades predichas\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto de la entidad y su label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "positive-survey",
   "metadata": {},
   "source": [
    "## Predient entitats nomenades en context\n",
    "\n",
    "Els models són estadístics i no són _sempre_ correctes. La correcció de les seues\n",
    "prediccions depén de les dades d'entrenament i del text que estàs\n",
    "processant. Vegem un exemple.\n",
    "\n",
    "- Processa el text amb l'objecte `doc`.\n",
    "- Itera sobre les entitats i imprimeix en pantalla el text de l'entitat i el\n",
    " label.\n",
    "- Sembla ser que el model no va predir \"adidas zx\". Crea un span per a aqueixos tokens\n",
    " manualment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "several-works",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olímpicos de Tokio 2020 MISC\n",
      "zx ORG\n",
      "Entidad faltante: adidas zx\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = (\n",
    "    \"Los Olímpicos de Tokio 2020 son la inspiración para la nueva \"\n",
    "    \"colección de zapatillas adidas zx.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# Itera sobre las entidades\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto de la entidad y su label\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Obtén el span para \"adidas zx\"\n",
    "adidas_zx = doc[14:16]\n",
    "\n",
    "# Imprime en pantalla el texto del span\n",
    "print(\"Entidad faltante:\", adidas_zx.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lightweight-amber",
   "metadata": {},
   "source": [
    "# Trobant patrons basats en regles\n",
    "\n",
    "En aquest apartat veurem el matcher de spaCy, que et permet escriure\n",
    "regles per a trobar paraules i frases en el text.\n",
    "\n",
    "### Per què no simplement expressions regulars?\n",
    "\n",
    "- Matching en objectes `Doc`, no solament en strings\n",
    "- Matching en tokens i atributs de tokens\n",
    "- Usa les prediccions del model\n",
    "- Exemple: \"aranya\" (verb) vs. \"aranya\" (substantiu)\n",
    "\n",
    "### Match patterns\n",
    "\n",
    "Els match patterns són llestes de diccionaris. Cada diccionari descriu\n",
    "un token. Els keys són els noms dels atributs del token, mapatges als seus\n",
    "valors esperats.\n",
    "\n",
    "En aquest exemple, estem buscant dos tokens amb el text \"iPhone\" i \"X\".\n",
    "\n",
    "```python\n",
    "[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "```\n",
    "\n",
    "També podem usar altres atributs dels tokens per a trobar el que\n",
    "busquem. Ací estem buscant dos tokens que en minúscules són iguals a\n",
    "\"iphone\" i \"x\".\n",
    "\n",
    "```python\n",
    "[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "```\n",
    "\n",
    "També podem escriure patrons que usen els atributs predits pel\n",
    "model. Ací estem buscant un token amb el lemma \"comprar\", més un substantiu. El\n",
    "lemma és la forma bàsica, així que aquest patró trobaria frases com \"comprando\n",
    "leche\" o \"compré flores\".\n",
    "\n",
    "```python\n",
    "[{\"LEMMA\": \"comprar\"}, {\"POS\": \"NOUN\"}]\n",
    "```\n",
    "\n",
    "### Usando el Matcher\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Importa el Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Carga el modelo y crea un objeto nlp\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Inicializa el matcher con el vocabulario compartido\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Añade el patrón al matcher\n",
    "pattern = [{\"TEXT\": \"adidas\"}, {\"TEXT\": \"zx\"}]\n",
    "matcher.add(\"ADIDAS_PATTERN\", [pattern])\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Nuevos diseños de zapatillas en la colección adidas zx\")\n",
    "\n",
    "# Llama al matcher sobre el doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Llama al matcher sobre el doc\n",
    "doc = nlp(\"Nuevos diseños de zapatillas en la colección adidas zx\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matches:\n",
    "    # Obtén el span resultante\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "```\n",
    "\n",
    "```out\n",
    "adidas zx\n",
    "```\n",
    "\n",
    "- `match_id`: valor hash del nombre del patrón\n",
    "- `start`: índice de inicio del span resultante\n",
    "- `end`: índice del final del span resultante\n",
    "\n",
    "Notes: Quan anomenes el matcher sobre un doc, aquest retorna una llista de <abbr title=\"En espanyol: tuplas, un tuple és un tipus de dada que conté una seqüència fixa d'ítems, com una llista, però que no es pot modificar.\">tuples</abbr>.\n",
    "\n",
    "Cada tuple consisteix de tres valors: el ID del resultat, l'índex d'inici i\n",
    "l'índex del final del span resultant.\n",
    "\n",
    "Això significa que podem iterar sobre els resultats i crear un objecte `Span`:\n",
    "un slice del doc que comença en l'índex d'inici i acaba en l'índex del\n",
    "final.\n",
    "\n",
    "### Trobant per atributs lèxics\n",
    "\n",
    "Ací tenim un exemple d'un patró més complex usant atributs\n",
    "lèxics.\n",
    "\n",
    "```python\n",
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"copa\"},\n",
    "    {\"LOWER\": \"mundial\"},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "```\n",
    "\n",
    "```python\n",
    "doc = nlp(\"2014 Copa Mundial FIFA: Alemania ganó!\")\n",
    "```\n",
    "\n",
    "```out\n",
    "2014 Copa Mundial FIFA:\n",
    "```\n",
    "\n",
    "### Trobant per altres atributs del token\n",
    "\n",
    "```python\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"comer\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "```\n",
    "\n",
    "```python\n",
    "doc = nlp(\"Camila prefería comer tacos. Pero ahora está comiendo pasta.\")\n",
    "```\n",
    "\n",
    "```out\n",
    "comer tacos\n",
    "comiendo pasta\n",
    "```\n",
    "\n",
    "### Usant operadors i quantificadors\n",
    "\n",
    "```python\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"comprar\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # opcional: encuentra 0 o 1 veces\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "```\n",
    "\n",
    "```python\n",
    "doc = nlp(\"Me compré un smartphone. Ahora le estoy comprando aplicaciones.\")\n",
    "```\n",
    "\n",
    "```out\n",
    "compré un smartphone\n",
    "comprando aplicaciones\n",
    "```\n",
    "\n",
    "Operadors i quantificadors et permeten definir amb quina freqüència un\n",
    "token ha de ser trobat. Poden ser afegits amb el key \"OP\".\n",
    "\n",
    "| Exemple | Descripció |\n",
    "| ------------- | ------------------------------- |\n",
    "| `{\"OP\": \"!\"}` | Negació: troba 0 vegades |\n",
    "| `{\"OP\": \"?\"}` | Opcional: troba 0 o 1 vegades |\n",
    "| `{\"OP\": \"+\"}` | Troba 1 o més vegades |\n",
    "| `{\"OP\": \"*\"}` | Troba 0 o més vegades |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "victorian-rainbow",
   "metadata": {},
   "source": [
    "## Usant el Matcher\n",
    "Usarem el `Matcher` basat en regles de spaCy. Prenent l'exemple de l'exercici\n",
    "anterior escriu un patró que trobe la frase \"adidas zx\" en\n",
    "el text.\n",
    "\n",
    "- Importa el `Matcher` des de `spacy.matcher`.\n",
    "- Inicialitza-ho amb el `vocab` compartit de l'objecte `nlp`.\n",
    "- Crea un patró que trobe els valors `\"TEXT\"` de dos tokens: `\"adidas\"` i\n",
    " `\"zx\"`.\n",
    "- Usa el mètode `matcher.add` per a afegir el patró al matcher.\n",
    "- Crida al matcher en el `doc` i guarda el resultat en la variable `matches`.\n",
    "- Itera sobre els resultats i obtingues el span resultant des de l'índex `start`\n",
    " fins a l'índex `end`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "blond-emergency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados: ['adidas zx']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Importa el Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\n",
    "    \"Los Olímpicos de Tokio 2020 son la inspiración para la nueva \"\n",
    "    \"colección de zapatillas adidas zx.\"\n",
    ")\n",
    "\n",
    "# Inicializa el matcher con el vocabulario compartido\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Crea un patrón que encuentre dos tokens: \"adidas\" y \"zx\"\n",
    "pattern = [{\"TEXT\": \"adidas\"}, {\"TEXT\": \"zx\"}]\n",
    "\n",
    "# Añade el patrón al matcher\n",
    "matcher.add(\"ADIDAS_ZX_PATTERN\", [pattern])\n",
    "\n",
    "# Usa al matcher sobre el doc\n",
    "matches = matcher(doc)\n",
    "print(\"Resultados:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "challenging-college",
   "metadata": {},
   "source": [
    "## Escrivint patrons\n",
    "\n",
    "En aquest exercici practicaràs escriure patrons més complexos usant diferents\n",
    "atributs dels tokens i operadors.\n",
    "\n",
    "### Part 1\n",
    "\n",
    "- Escriu __un__ patró que únicament trobe esments de les versions\n",
    " _senceres_ de iOS: \"iOS 7\", \"iOS 11\" i \"iOS 10\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gorgeous-machinery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Después de hacer la actualización de iOS no notarás un rediseño \"\n",
    "    \"radical del sistema: no se compara con los cambios estéticos que \"\n",
    "    \"tuvimos con el iOS 7. La mayoría de las funcionalidades del iOS 11 \"\n",
    "    \"siguen iguales en el iOS 10.\"\n",
    ")\n",
    "\n",
    "# Escribe un patrón para las versiones de iOS enteras\n",
    "# (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Añade el patrón al matcher y usa el matcher sobre el documento\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Itera sobre los resultados e imprime el texto del span\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ranking-taylor",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "- Escriu un patró que únicament trobe maneres de \"descargar\" (tokens\n",
    " amb el lemma \"descargar\") seguit per un token que tinga el part-of-speech tag\n",
    " `\"PROPN\"` (<abbr title=\"En castellà: nom propi.\">proper noun</abbr>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial-heading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de resultados encontrados: 3\n",
      "Resultado encontrado: descargué Fortnite\n",
      "Resultado encontrado: descargando Minecraft\n",
      "Resultado encontrado: descargar Winzip\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"descargué Fortnite en mi computadora, pero no puedo abrir el juego. \"\n",
    "    \"Ayuda? Cuando estaba descargando Minecraft, conseguí la versión de Windows \"\n",
    "    \"donde tiene una carpeta '.zip' y usé el programa por defecto para \"\n",
    "    \"descomprimirlo…así que también tengo que descargar Winzip?\"\n",
    ")\n",
    "\n",
    "# Escribe un patrón que encuentre una forma de \"descargar\" más un nombre propio\n",
    "pattern = [{\"LEMMA\": \"descargar\"}, {\"POS\": \"PROPN\"}]\n",
    "\n",
    "# Añade el patrón al matcher y usa el matcher sobre el documento\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total de resultados encontrados:\", len(matches))\n",
    "\n",
    "# Itera sobre los resultados e imprime el texto del span\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Resultado encontrado:\", doc[start:end].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "conservative-representation",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "- Escriu __un__ patró que trobe un substantiu `\"NOUN\"` seguit d'un o\n",
    " dos adjectius `\"ADJ\"`(un adjectiu i un adjectiu opcional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "second-browser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de resultados encontrados: 4\n",
      "Resultado encontrado: gigante tecnológico\n",
      "Resultado encontrado: lecciones virtuales\n",
      "Resultado encontrado: tecnologías avanzadas\n",
      "Resultado encontrado: tecnologías avanzadas gratuitas\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"El gigante tecnológico IBM está ofreciendo lecciones virtuales \"\n",
    "    \"sobre tecnologías avanzadas gratuitas en español.\"\n",
    ")\n",
    "\n",
    "# Escribe un patrón para un sustantivo más uno o dos adjetivos\n",
    "pattern = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"ADJ\", \"OP\": \"?\"}]\n",
    "\n",
    "# Añade el patrón al matcher y usa el matcher sobre el documento\n",
    "matcher.add(\"NOUN_ADJ_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total de resultados encontrados:\", len(matches))\n",
    "\n",
    "# Itera sobre los resultados e imprime el texto del span\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Resultado encontrado:\", doc[start:end].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "pediatric-runner",
   "metadata": {},
   "source": [
    "# Estructures de dades\n",
    "\n",
    "En aquest apartat veurem el vocabulari compartit i la manera en la qual spaCy\n",
    "maneja els strings.\n",
    "\n",
    "### Vocabulari compartit i string store (1)\n",
    "\n",
    "spaCy guarda totes les dades compartides a través de múltiples documents en un vocabulari, el `Vocab`.\n",
    "\n",
    "Aquest inclou paraules, però també els esquemes de labels per a tags i\n",
    "entitats.\n",
    "\n",
    "Per a usar menys memòria, tots els strings són codificats a hash IDs. Si una\n",
    "paraula ocorre múltiples vegades, no hem de guardar-la cada vegada.\n",
    "\n",
    "En canvi, spaCy usa una funció hash per a generar un ANEU i guarda el string una\n",
    "vegada en el `StringStore`. El string store està disponible com `nlp.vocab.strings`\n",
    "\n",
    "És una <abbr title=\"En espanyol: taula de consulta, com un diccionari.\">lookup table</abbr> que\n",
    "funciona en totes dues direccions. Pots buscar un string i obtindre el seu hash, així com\n",
    " pots buscar un hash per a obtindre el seu valor string. Internament spaCy només\n",
    "es comunica en hash IDs.\n",
    "\n",
    "```python\n",
    "cafe_hash = nlp.vocab.strings[\"café\"]\n",
    "cafe_string = nlp.vocab.strings[cafe_hash]\n",
    "```\n",
    "\n",
    "Els hash IDs no es poden revertir. Si una paraula no està en el vocabulari no\n",
    "hi ha manera d'obtindre la seua string. És per això que sempre hem de passar el\n",
    "vocabulari compartit.\n",
    "\n",
    "```python\n",
    "# Arroja un error si no hemos visto el string antes\n",
    "string = nlp.vocab.strings[32833993555699147]\n",
    "```\n",
    "\n",
    "- Per a obtindre el hash d'un string podem buscar-ho en\n",
    "`nlp.vocab.strings`.\n",
    "\n",
    "- Per a obtindre el string que representa a un hash podem buscar amb el hash.\n",
    "\n",
    "```python\n",
    "doc = nlp(\"Ines toma café\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"café\"])\n",
    "print(\"string value:\", nlp.vocab.strings[32833993555699147])\n",
    "```\n",
    "\n",
    "```out\n",
    "hash value: 32833993555699147\n",
    "string value: café\n",
    "```\n",
    "\n",
    "- El `doc` també exposa el seu vocabulari i strings\n",
    "\n",
    "```python\n",
    "doc = nlp(\"Ines toma café\")\n",
    "print(\"hash value:\", doc.vocab.strings[\"café\"])\n",
    "```\n",
    "\n",
    "```out\n",
    "hash value: 32833993555699147\n",
    "```\n",
    "\n",
    "### Lexemes: entrades en el vocabulari\n",
    "\n",
    "- Un objecte `Lexeme` és una entrada en el vocabulari\n",
    "\n",
    "```python\n",
    "doc = nlp(\"Ines toma café\")\n",
    "lexeme = nlp.vocab[\"café\"]\n",
    "\n",
    "# Imprime en pantalla los atributos léxicos\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)\n",
    "```\n",
    "\n",
    "```out\n",
    "café 32833993555699147 True\n",
    "```\n",
    "\n",
    "- Conté la informació sobre una paraula, __independent del context__\n",
    " - Text de la paraula: `lexeme.text` i `lexeme.orth` (el hash)\n",
    " - Atributs lèxics com `lexeme.is_alpha`\n",
    " - __No__ part-of-speech tags, dependencies o entity labels dependents del\n",
    " context\n",
    "\n",
    "### Vocabulari, hashes i lexemes\n",
    "\n",
    "<img src=\"P3_materiales/vocab_stringstore_es.png\" width=\"70%\" alt=\"Illustration of the words 'Ines', 'toma' and 'café' across the Doc, Vocab and StringStore\" />\n",
    "\n",
    "El `Doc` conté paraules en context - en aquest cas, els tokens \"Ines\", \"toma\" i\n",
    "\"café\" amb els seus part-of-speech tags i dependencies.\n",
    "\n",
    "Cada token es refereix a un lexema, que coneix el hash ID de la paraula. Per a\n",
    "obtindre la representació en string de la paraula, spaCy cerca el hash en el\n",
    "string store.\n",
    "\n",
    "## De strings a hashes\n",
    "\n",
    "### Part 1\n",
    "\n",
    "- Cerca el string \"gat\" en `nlp.vocab.strings` per a obtindre el hash.\n",
    "- Cerca el hash per a obtindre el string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mature-insight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9565357104409163886\n",
      "gato\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Yo tengo un gato\")\n",
    "\n",
    "# Busca el hash para la palabra \"gato\"\n",
    "gato_hash = nlp.vocab.strings[\"gato\"]\n",
    "print(gato_hash)\n",
    "\n",
    "# Busca el gato_hash para obtener el string\n",
    "gato_string = nlp.vocab.strings[gato_hash]\n",
    "print(gato_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "correct-patrol",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "- Cerca el label del string \"PER\" en `nlp.vocab.strings` per a obtindre el hash.\n",
    "- Cerca el hash per a obtindre el string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "facial-sense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4317129024397789502\n",
      "PER\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"David Bowie tiene el label PER\")\n",
    "\n",
    "# Busca el hash para el label del string \"PER\"\n",
    "person_hash = nlp.vocab.strings[\"PER\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Busca el person_hash para obtener el string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "honest-valley",
   "metadata": {},
   "source": [
    "# Estructures de dades: Doc, Span i Token\n",
    "\n",
    "Ara que ho saps tot sobre el vocabulari i el string store podem\n",
    "estudiar les estructures de dades més importants: el `Doc` i els seus views el\n",
    "`Token` i el `Span`.\n",
    "\n",
    "### L'objecte Doc\n",
    "\n",
    "El `Doc` és una de les estructures de dades centrals de spaCy. És creat\n",
    "automàticament quan processes un text amb l'objecte `nlp`. Però també\n",
    "pots crear un instance manualment.\n",
    "\n",
    "Després de crear l'objecte `nlp` podem importar la classe `Doc` des de\n",
    "`spacy.tokens`.\n",
    "\n",
    "```python\n",
    "# Importa la clase Doc\n",
    "from spacy.tokens import Doc\n",
    "```\n",
    "\n",
    "Com a exemple, creguem un doc a partir de tres paraules. Els espais són una\n",
    "llista de valors booleans que indiquen si una paraula està seguida per un\n",
    "espai. Cada token inclou aqueixa informació - inclusivament l'últim!\n",
    "\n",
    "```python\n",
    "# Importa la clase Doc\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Las palabras y espacios que usaremos para crear el doc\n",
    "words = [\"¡\", \"Hola\", \"Mundo\", \"!\"]\n",
    "spaces = [False, True, False, False]\n",
    "```\n",
    "\n",
    "La classe `Doc` rep tres arguments: el vocabulari compartit, les paraules i\n",
    "els espais.\n",
    "\n",
    "```python\n",
    "# Crea un doc manualmente\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "```\n",
    "\n",
    "### El objete Span\n",
    "\n",
    "<img src=\"P3_materiales/span_indices.png\" width=\"65%\" alt=\"Illustration of a Span object within a Doc with token indices\" />\n",
    "\n",
    "Un `Span` és un slice d'un Doc que està format per un o més tokens. El\n",
    "`Span` rep almenys tres arguments: el doc al qual es refereix, l'índex d'inici\n",
    "i l'índex del final del span. Recorda que l'índex del final és\n",
    "excloent!\n",
    "\n",
    "Per a crear un `Span` manualment també podem importar la classe des de\n",
    "`spacy.tokens`. Podem crear un instance amb el doc i els índexs d'inici i\n",
    "final, així com un argument opcional de label.\n",
    "\n",
    "Els `doc.ents` són escribibles així que podem afegir entitats manualment\n",
    "sobreescrivint-los amb una llista de spans.\n",
    "\n",
    "```python\n",
    "# Importa las clases Doc y Span\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Las palabras y espacios que usaremos para crear el doc\n",
    "words = [\"¡\", \"Hola\", \"Mundo\", \"!\"]\n",
    "spaces = [False, True, False, False]\n",
    "\n",
    "# Crea un doc manualmente\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Crea un span manualmente\n",
    "span = Span(doc, 1, 3)\n",
    "\n",
    "# Crea un span con un label\n",
    "span_with_label = Span(doc, 1, 3, label=\"GREETING\")\n",
    "\n",
    "# Añade el span a los doc.ents\n",
    "doc.ents = [span_with_label]\n",
    "```\n",
    "### Bones pràctiques\n",
    "\n",
    "- `Doc` i `Span` són molt poderosos i contenen referències i relacions de\n",
    " paraules i frases\n",
    " - __Convertir el resultat a strings a més tardar possible__\n",
    " - __Usar els atributs dels tokens si estan disponibles__ – per exemple,\n",
    " `token.i` per a l'índex del token\n",
    "- No oblides passar el `vocab` compartit\n",
    "\n",
    "## Docs, spans i entitats des de zero\n",
    "\n",
    "En aquest exercici crearàs els objectes `Doc` i `Span` manualment i actualitzaràs\n",
    "les entitats nomenades - igual que ho fa spaCy darrere de cambres. Un objecte\n",
    "`nlp` compartit ja va ser creat.\n",
    "\n",
    "- Importa les classes `Doc` i `Span` des de `spacy.tokens`.\n",
    "- Usa la classe `Doc` directament per a crear un `doc` a partir de paraules i\n",
    " espais.\n",
    "- Crea un `Span` per a \"David Bowie\" des del `doc` i assigna-ho al label `\"PER\"`.\n",
    "- Sobreescriu els `doc.ents` amb una llista d'una entitat, el `Span` \"David\n",
    " Bowie\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "streaming-business",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me gusta David Bowie\n",
      "David Bowie PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.es import Spanish\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Importa las clases Doc y Span\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"Me\", \"gusta\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Crea un doc a partir de las palabras y los espacios\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Crea un span para \"David Bowie\" a partir del doc y asígnalo al label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Añade el span a las entidades del doc\n",
    "doc.ents = [span]\n",
    "\n",
    "# Imprime en pantalla el texto y los labels de las entidades\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "separated-order",
   "metadata": {},
   "source": [
    "### Trobant frases eficientment, 'phrase matching'\n",
    "\n",
    "A vegades és més eficient buscar els strings exactes en comptes d'escriure els\n",
    "patrons descrivint els tokens individuals. Això és especialment cert per a\n",
    "coses que tenen categories finites - com tots els països del món. Ací\n",
    "tenim una llista de països, així que usem-los com a base per al nostre script\n",
    "per a extraure informació. La llista de noms en strings està disponible en la\n",
    "variable `COUNTRIES`.\n",
    "\n",
    "- Importa el `PhraseMatcher` i inicialitza-ho amb el `vocab` compartit com la\n",
    " variable `matcher`.\n",
    "- Afig els patrons de frases i crida al matcher sobre el `doc`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "conscious-residence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Francia, Alemania, Italia, Bélgica, Países Bajos, Luxemburgo]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "with open(\"P3_materiales/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = Spanish()\n",
    "doc = nlp(\n",
    "    \"La Unión Europea fue fundada por seis países de Europa occidental \"\n",
    "    \"(Francia, Alemania, Italia, Bélgica, Países Bajos, y Luxemburgo) y \"\n",
    "    \"se amplió en seis ocasiones.\"\n",
    ")\n",
    "\n",
    "# Importa el PhraseMatcher e inicialízalo\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Crea objetos Doc patrón y añádelos al matcher\n",
    "# Esta es una versión más rápida de: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Llama al matcher sobre el documento de prueba e imprime el \n",
    "# resultado en pantalla\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "biblical-powell",
   "metadata": {},
   "source": [
    "### Extraient països i relacions\n",
    "\n",
    "En l'exercici anterior vas escriure un script usant el `PhraseMatcher` de spaCy\n",
    "per a trobar noms de països en un text. Usem aqueix cercador de països en\n",
    "un text més llarg. Analitza la sintaxi i actualitza les entitats del document\n",
    "amb els països resultants.\n",
    "\n",
    "- Itera sobre els resultats i crea un `Span` amb el label `\"LOC\"` (Noms d'ubicacions\n",
    " definides política o geogràficament).\n",
    "- Sobreescriu les entitats en el `doc.ents` i afig el span resultant.\n",
    "- Obtingues el token cap de l'arrel del span.\n",
    "- Imprimeix en pantalla el text del token cap i el span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alone-edinburgh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brasil --> Brasil\n",
      "Brasil --> Alemania\n",
      "Brasil --> India\n",
      "Brasil --> Japón\n",
      "segundos --> Japón\n",
      "Japón --> Alemania\n",
      "dos --> Brasil\n",
      "Brasil --> India\n",
      "incluir --> Brasil\n",
      "Brasil --> Alemania\n",
      "Brasil --> India\n",
      "Brasil --> Japón\n",
      "habitualmente --> Egipto\n",
      "Egipto --> Nigeria\n",
      "apoyado --> Estados Unidos\n",
      "membresía --> Japón\n",
      "apoyo --> India\n",
      "apoyan --> Reino Unido\n",
      "Reino --> Francia\n",
      "acceso --> Alemania\n",
      "Alemania --> Brasil\n",
      "Alemania --> India\n",
      "Alemania --> Japón\n",
      "apoyado --> China\n",
      "membresía --> Japón\n",
      "puja --> India\n",
      "apoyo --> Francia\n",
      "Francia --> Rusia\n",
      "Francia --> Reino Unido\n",
      "Francia --> Estados Unidos\n",
      "adquirido --> India\n",
      "expresó --> China\n",
      "expresado --> China\n",
      "candidatura --> India\n",
      "revoca --> India\n",
      "[('Brasil', 'LOC'), ('Alemania', 'LOC'), ('India', 'LOC'), ('Japón', 'LOC'), ('Japón', 'LOC'), ('Alemania', 'LOC'), ('Brasil', 'LOC'), ('India', 'LOC'), ('Brasil', 'LOC'), ('Alemania', 'LOC'), ('India', 'LOC'), ('Japón', 'LOC'), ('Egipto', 'LOC'), ('Nigeria', 'LOC'), ('Estados Unidos', 'LOC'), ('Japón', 'LOC'), ('India', 'LOC'), ('Reino Unido', 'LOC'), ('Francia', 'LOC'), ('Alemania', 'LOC'), ('Brasil', 'LOC'), ('India', 'LOC'), ('Japón', 'LOC'), ('China', 'LOC'), ('Japón', 'LOC'), ('India', 'LOC'), ('Francia', 'LOC'), ('Rusia', 'LOC'), ('Reino Unido', 'LOC'), ('Estados Unidos', 'LOC'), ('India', 'LOC'), ('China', 'LOC'), ('China', 'LOC'), ('India', 'LOC'), ('India', 'LOC')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "with open(\"P3_materiales/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "with open(\"P3_materiales/country_text.txt\", encoding=\"utf8\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Crea un doc y restablece las entidades existentes\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Crea un Span con el label para \"LOC\"\n",
    "    span = Span(doc, start, end, label=\"LOC\")\n",
    "\n",
    "    # Sobrescribe el doc.ents y añade el span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Obtén el token cabeza de la raíz del span\n",
    "    span_root_head = span.root.head\n",
    "    # Imprime en pantalla el texto del token cabeza de\n",
    "    # la raíz del span y el texto del span\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Imprime en pantalla las entidades del documento\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"LOC\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "thorough-asian",
   "metadata": {},
   "source": [
    "# Pipelines de processament\n",
    "\n",
    "Explicarem els pipelines de\n",
    "processament: una sèrie de funcions que s'apliquen a un doc per a afegir\n",
    "atributs com part-of-speech tags, dependency labels o entitats nomenades.\n",
    "\n",
    "En aquest apartat veurem els components del pipeline proveït per spaCy\n",
    "i què succeeix darrere de cambres quan anomenes a un objecte `nlp` sobre un string\n",
    "de text.\n",
    "\n",
    "Ja hem descrit això bastants vegades: li passes un string de text a l'objecte\n",
    "`nlp` i reps un objecte `Doc`.\n",
    "\n",
    "Però, què fa l'objecte `nlp` _realment_?\n",
    "\n",
    "Primer, s'aplica el tokenizer per a convertir el string de text a un objecte\n",
    "`Doc`. A continuació, una sèrie de components del pipeline s'apliquen al doc en\n",
    "ordre. En aquest cas, el tagger, després el parser, després el\n",
    "<abbr title=\"És el component que identifica les entitats nomenades d'un text.\">entity\n",
    "recognizer</abbr> . Finalment, el doc processament és retornat perquè pugues\n",
    "treballar amb ell.\n",
    "\n",
    "### Components inclosos en el pipeline\n",
    "\n",
    "| Nom | Descripció | Crea |\n",
    "| ----------- | :---------------------- | :-------------------------------------------------------- |\n",
    "| __tagger__  | Part-of-speech tagger   | `Token.tag`, `Token.pos`                                  |\n",
    "| __parser__  | Dependency parser       | `Token.dep`, `Token.head`, `Doc.sents`, `Doc.noun_chunks` |\n",
    "| __ner__     | Named entity recognizer | `Doc.ents`, `Token.ent_iob`, `Token.ent_type`             |\n",
    "| __textcat__ | Text classifier         | `Doc.cats`                                                |\n",
    "\n",
    "spaCy ve amb els següents components inclosos en el seu pipeline.\n",
    "\n",
    "El part-of-speech tagger afig els atributs `token.tag` i `token.post`.\n",
    "\n",
    "El dependency parser afig els atributs `token.dep` i `token.head` i és\n",
    "responsable de detectar frases i els\n",
    "<abbr title=\"En espanyol: frases nominals.\">base noun phrases</abbr>, també\n",
    "coneguts com \"noun chunks\".\n",
    "\n",
    "El named entity recognizer afig les entitats detectades a la propietat\n",
    "`doc.ents`. També escriu els atributs del tipus d'entitat en els tokens, la qual cosa\n",
    " indica si un token és part d'una entitat o no.\n",
    "\n",
    "Finalment, el <abbr title=\"En espanyol: classificador de text.\">text classifier</abbr> escriu les labels de categoria que apliquen a\n",
    "tot el text i les afig a la propietat `doc.cats`.\n",
    "\n",
    "Pel fet que les categories de text són sempre molt específiques, el text\n",
    "classifier no està inclòs en els models pre-entrenats per defecte. Però ho\n",
    "pots usar per a entrenar el teu propi sistema.\n",
    "\n",
    "Tots els models que pots carregar en spaCy inclouen diversos arxius i\n",
    "un `meta.json`.\n",
    "\n",
    "El meta defineix coses com el llenguatge i el pipeline. Això li deixa saber a spaCy\n",
    "quins són els components als quals els ha de fer un instance.\n",
    "\n",
    "Els components inclosos que fan prediccions també necessiten dades\n",
    "binàries. Les dades s'inclouen en el paquet del model i es carreguen al\n",
    "component quan càrregues el model.\n",
    "\n",
    "### Atributs del pipeline\n",
    "\n",
    "- `nlp.pipe_names`: una llista de noms de components del pipeline\n",
    "\n",
    "```python\n",
    "print(nlp.pipe_names)\n",
    "```\n",
    "\n",
    "```out\n",
    "['tagger', 'parser', 'ner']\n",
    "```\n",
    "\n",
    "- `nlp.pipeline`: una lista de tuples de `(name, component)`\n",
    "\n",
    "```python\n",
    "print(nlp.pipeline)\n",
    "```\n",
    "\n",
    "```out\n",
    "[('tagger', <spacy.pipeline.Tagger>),\n",
    " ('parser', <spacy.pipeline.DependencyParser>),\n",
    " ('ner', <spacy.pipeline.EntityRecognizer>)]\n",
    "```\n",
    "\n",
    "Per a veure els noms dels components del pipeline que estan en l'objecte\n",
    "`nlp` actual pots usar l'atribut `nlp.pipe_names`.\n",
    "\n",
    "Per a una llista de tuples amb els noms i funcions de cada component usa l'atribut\n",
    "`nlp.pipeline`.\n",
    "\n",
    "Les funcions dels components són aquelles funcions que s'apliquen al doc\n",
    "per a processar-ho i afegir atributs - per exemple, part-of-speech tags o\n",
    "entitats nomenades.\n",
    "\n",
    "## Inspeccionant el pipeline\"\n",
    "\n",
    "Inspeccionem el pipeline del model xicotet d'espanyol!\n",
    "\n",
    "- Càrrega el model `es_core_news_sm` i crea l'objecte `nlp`.\n",
    "- Imprimeix en pantalla els noms dels components del pipeline usant\n",
    " `nlp.pipe_names`.\n",
    "- Imprimeix en pantalla el pipeline sencer de tuples `(name, component)` usant\n",
    " `nlp.pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sexual-competition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x0000014C6B72B280>), ('morphologizer', <spacy.pipeline.morphologizer.Morphologizer object at 0x0000014C6B728BE0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x0000014C68ADA7A0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x0000014C69145880>), ('lemmatizer', <spacy.lang.es.lemmatizer.SpanishLemmatizer object at 0x0000014C68AB3D80>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000014C68ADA490>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Carga el modelo es_core_news_sm\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Imprime en pantalla los nombres de los componentes del pipeline \n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Imprime en pantalla el pipeline entero de tuples (name, component)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "everyday-pharmacy",
   "metadata": {},
   "source": [
    "# Components personalitzats del pipeline\n",
    "\n",
    "Ara que saps com funciona el pipeline de spaCy explorem una altra característica molt poderosa: els components personalitzats per al pipeline.\n",
    "\n",
    "Els components personalitzats del pipeline et permeten afegir la teua pròpia funció al pipeline de spaCy que s'executa quan anomenes a l'objecte `nlp` sobre un text - per exemple, per a modificar el doc i afegir-li més dades.\n",
    "\n",
    "Després que el text és convertit en tokens i un objecte `Doc` ha sigut creat, els components del pipeline s'apliquen en ordre. spaCy ofereix suport per a un rang de components inclosos, però també et permet definir els teus.\n",
    "\n",
    "Els components personalitzats s'executen automàticament quan anomenes a l'objecte `nlp` sobre un text.\n",
    "\n",
    "Són especialment útils per a afegir les teues pròpies metadades als documents i als tokens.\n",
    "\n",
    "També pots usar-los per a actualitzar els atributs inclosos, com els spans de les entitats nomenades.\n",
    "\n",
    "### Anatomia d'un component\n",
    "\n",
    "- Funcions que prenen un `doc`, ho modifiquen i ho retornen\n",
    "- Poden ser afegits usant el mètode `nlp.add_pipe`\n",
    "\n",
    "```python\n",
    "def custom_component(doc):\n",
    "    # Haz algo con el doc aquí\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component)\n",
    "```\n",
    "\n",
    "Fonamentalment, un component del pipeline és una funció o un <abbr title=\"Alguna cosa que pot ser anomenat o executat, així com una funció o una classe.\">callable</abbr> que presa a un doc, ho modifica i ho retorna perquè puga ser processat pel pròxim component en el pipeline.\n",
    "\n",
    "Els components poden ser afegits al pipeline usant el mètode `nlp.add_pipe`. Primer cal registrar la funció del component amb un decorador, i després cridar a `nlp.add_pipe`. Aquest presa almenys un argument: el nom de la funció del component.\n",
    "\n",
    "\n",
    "| Argumente | Descripció | Exemple |\n",
    "| -------- | -------------------- | ----------------------------------------- |\n",
    "| `last` | Si és `True`, ho posa d'últim | `nlp.add_pipe('component', last=True)` |\n",
    "| `first` | Si és `True`, ho posa primer | `nlp.add_pipe('component', first=True)` |\n",
    "| `before` | Ho afig abans del component | `nlp.add_pipe('component', before=\"ner\")` |\n",
    "| `after` | Ho afig després del component | `nlp.add_pipe('component', after=\"tagger\")` |\n",
    "\n",
    "Per a especificar _on_ afegir el component en el pipeline, pots usar un dels següents arguments keyword:\n",
    "\n",
    "Si fas que el valor de `last` siga `True`, s'afegirà el component en l'últim lloc del pipeline. Aquest és el comportament per defecte.\n",
    "\n",
    "Si fas que el valor de `first` siga `True`, s'afegirà en el primer lloc del pipeline just després del tokenizer.\n",
    "\n",
    "Els arguments `before` i `after` et permeten definir el nom d'un component existent al qual li pots afegir el nou component abans o després. Per exemple, `before=\"ner\"` afegirà el nou component abans del named entity recognizer.\n",
    "\n",
    "No obstant això, l'altre component al qual se li afegirà un nou component abans o després ha d'existir. Si no, spaCy llançarà un error.\n",
    "\n",
    "### Exemple: un component simple\n",
    "\n",
    "```python\n",
    "from spacy.language import Language\n",
    "\n",
    "# Crea el objeto nlp\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Define un componente personalizado\n",
    "@Language.component('custom_component')\n",
    "def custom_component(doc):\n",
    "    # Imprime la longitud del doc en pantalla\n",
    "    print(\"longitud del Doc:\", len(doc))\n",
    "    # Devuelve el objeto doc\n",
    "    return doc\n",
    "\n",
    "# Añade el componente al primer lugar del pipeline\n",
    "nlp.add_pipe('custom_component', first=True)\n",
    "\n",
    "# Imprime los nombres de los componentes del pipeline\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "```\n",
    "\n",
    "```out\n",
    "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n",
    "```\n",
    "\n",
    "```python\n",
    "# Procesa un texto\n",
    "doc = nlp(\"¡Hola Mundo!\")\n",
    "```\n",
    "\n",
    "```out\n",
    "longitud del Doc: 4\n",
    "```\n",
    "\n",
    "Ací tenim un exemple d'un component simple del pipeline.\n",
    "\n",
    "Comencem amb el model xicotet d'espanyol.\n",
    "\n",
    "Després definim el component - una funció que pren un objecte `Doc` i després el retorna.\n",
    "\n",
    "Fem una cosa simple i imprimim en pantalla la longitud del document que passa pel pipeline.\n",
    "\n",
    "No oblides retornar el doc perquè puga ser processat pel pròxim component en el pipeline! El doc creat pel tokenizer passa per tots els components, així que és important que tots retornen el doc modificat.\n",
    "\n",
    "Ara podem afegir el component al pipeline. Ho afegirem en el primer lloc, just després del tokenizer. Ho fem definint `first=True`.\n",
    "\n",
    "Quan imprimim en pantalla els noms dels components el nou apareix al principi. Això significa que serà aplicat quan processem un doc.\n",
    "\n",
    "Ara, quan processem un text usant l'objecte `nlp`, el component personalitzat serà aplicat al doc i la longitud del document serà impresa en pantalla.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "retired-marking",
   "metadata": {},
   "source": [
    "## Components simples\n",
    "\n",
    "L'exemple mostra un component personalitzat que imprimeix la longitud d'un\n",
    "document. Pots completar-ho?\n",
    "\n",
    "- Completa la funció del component amb la longitud del `doc`.\n",
    "- Afig el `length_component` al pipeline existent com el __primer__\n",
    " component.\n",
    "- Prova el nou pipeline i processa qualsevol text amb l'objecte `nlp` - per exemple\n",
    " , \"Això és una frase.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "statewide-payday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'length_component']\n",
      "Este documento tiene 4 tokens.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define el componente personalizado\n",
    "@Language.component('length_component')\n",
    "def length_component(doc):\n",
    "    # Obtén la longitud del doc\n",
    "    doc_length = len(doc)\n",
    "    print(f\"Este documento tiene {doc_length} tokens.\")\n",
    "    # Devuelve el doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Carga el modelo pequeño de español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Añade el componente en el primer lugar del pipeline e imprime\n",
    "# los nombres de los pipes en pantalla\n",
    "nlp.add_pipe('length_component')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Esto es una frase\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "charitable-leeds",
   "metadata": {},
   "source": [
    "## Components complexos\n",
    "\n",
    "En aquest exercici escriuràs un component personalitzat que use el\n",
    "`PhraseMatcher` per a trobar noms d'animals en el document i afija els\n",
    "spans resultants als `doc.ents`. En la variable `matcher` ja es va crear un\n",
    "`PhraseMatcher` amb els patrons d'animals.\n",
    "\n",
    "- Defineix el component personalitzat i aplica el `matcher` al `doc`.\n",
    "- Crea un `Span` per a cada resultat, assigna-li el label ID per a `\"ANIMAL\"` i\n",
    " sobreescriu els `doc.ents` amb els nous spans.\n",
    "- Afig el nou component al pipeline _després_ del component `\"ner\"`.\n",
    "- Processa el text i imprimeix en pantalla el text de l'entitat i els entity\n",
    " labels de les entitats en `doc.ents`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bronze-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_patterns: [labrador dorado, gato, tortuga, oso de anteojos]\n",
      "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('tortuga', 'ANIMAL'), ('oso de anteojos', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "from spacy.language import Language\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "animals = [\"labrador dorado\", \"gato\", \"tortuga\", \"oso de anteojos\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define el componente personalizado\n",
    "@Language.component('animal_component')\n",
    "def animal_component(doc):\n",
    "    # Aplica el matcher al doc\n",
    "    matches = matcher(doc)\n",
    "    # Crea un Span para cada resultado y asígnales el label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Sobrescribe los doc.ents con los spans resultantes\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Añade el componente al pipeline después del componente \"ner\"\n",
    "nlp.add_pipe('animal_component')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Procesa el texto e imprime en pantalla el texto y el label\n",
    "# de los doc.ents\n",
    "doc = nlp(\"Hoy vimos una tortuga y un oso de anteojos en nuestra caminata\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dental-destiny",
   "metadata": {},
   "source": [
    "# Extensió d'atributs\n",
    "\n",
    "En aquest apartat aprendràs com afegir atributs personalitzats per als objectes `Doc`,\n",
    "`Token` i `Span` per a guardar dades específiques a les teues necessitats.\n",
    "\n",
    "### Afegint atributs personalitzats\n",
    "\n",
    "- Afig metadades personalitzades a documents, tokens i spans\n",
    "- Accessible a través de la propietat `._`\n",
    "\n",
    "```python\n",
    "doc._.title = \"El meu document\"\n",
    "token._.is_color = True\n",
    "span._.has_color = False\n",
    "```\n",
    "\n",
    "- Es registren en els `Doc`, `Token` o `Span` globals usant el mètode `set_extension`\n",
    "\n",
    "```python\n",
    "# Importa las clases globales\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Añade extensiones para el Doc, Token y Span\n",
    "Doc.set_extension(\"title\", default=None)\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "Span.set_extension(\"has_color\", default=False)\n",
    "```\n",
    "\n",
    "Els atributs personalitzats et permeten afegir metadades als docs, tokens i spans. Les dades poden ser afegits una vegada, o calculats dinàmicament.\n",
    "\n",
    "Els atributs personalitzats estan disponibles a través de la propietat `._` (punt i guió baix). Aquesta notació fa que siga clar que van ser agregats per l'usuari i no estan integrats en spaCy com `token.text`.\n",
    "\n",
    "Els atributs han de ser registrats en les classes `Doc`, `Token` i `Span` globals que pots importar des de `spacy.tokens`. Ja vas treballar amb elles en els capítols anteriors. Per a registrar un atribut personalitzat en els `Doc`, `Token` i `Span`, pots usar el mètode `set_extension`.\n",
    "\n",
    "El primer argument és el nom de l'atribut. Els arguments keyword et permeten definir com ha de ser calculat el valor. En aquest cas, té un valor per defecte i pot ser sobreescrit.\n",
    "\n",
    "Hi ha tres tipus d'extensió: extensió d'atributs, extensió de propietats i extensió de mètodes.\n",
    "\n",
    "### Extensió d'atributs\n",
    "\n",
    "- Afegir un valor per defecte que pot ser sobreescrit\n",
    "\n",
    "```python\n",
    "from spacy.tokens import Token\n",
    "\n",
    "# Añade una extensión en el Token con un valor por defecto\n",
    "Token.set_extension(\"is_color\", default=False)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "\n",
    "# Sobrescribe el valor de la extensión de atributo\n",
    "doc[3]._.is_color = True\n",
    "```\n",
    "\n",
    "Les extensions d'atribut afigen un valor per defecte que pot ser sobreescrit.\n",
    "\n",
    "Per exemple, un atribut personalitzat del token, anomenat `is_color`, que té per defecte el valor `False`.\n",
    "\n",
    "En tokens individuals el seu valor pot ser canviat quan se sobreescriu - en aquest cas, `True` per al token \"azul\".\n",
    "\n",
    "### Extensió de propietats\n",
    "\n",
    "- Defineix una funció getter i una funció setter opcional\n",
    "- La funció getter només és anomenada quan _es consulta_ el valor de l'atribut\n",
    "\n",
    "```python\n",
    "from spacy.tokens import Token\n",
    "\n",
    "# Define la función getter\n",
    "def get_is_color(token):\n",
    "    colors = [\"rojo\", \"amarillo\", \"azul\"]\n",
    "    return token.text in colors\n",
    "\n",
    "# Añade una extensión en el Token con getter\n",
    "Token.set_extension(\"is_color\", getter=get_is_color)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "print(doc[3]._.is_color, \"-\", doc[3].text)\n",
    "```\n",
    "\n",
    "```out\n",
    "True - azul\n",
    "```\n",
    "\n",
    "Les extensions de propietats funcionen com les propietats de Python: poden definir una funció <abbr title=\"En espanyol: obtenedor. Una funció que obté i retorna un valor i que Python executa automàticament quan s'accedeix a un atribut especial d'un objecte.\">getter</abbr> i una funció <abbr title=\"En espanyol: establecedor. Una funció que d'alguna forma estableix un valor i que Python executa automàticament quan s'assigna un valor a un atribut especial d'un objecte.\">setter</abbr> opcional.\n",
    "\n",
    "La funció getter només és anomenada quan consultes l'atribut. Això et permet calcular el valor dinàmicament, i inclusivament pot tindre en compte altres atributs personalitzats.\n",
    "\n",
    "Les funcions getter prenen un argument: l'objecte, en aquest cas el token. En aquest exemple, la funció retorna si el text d'un token es troba en la nostra llista de colors.\n",
    "\n",
    "Podem proveir la funció mitjançant l'argument keyword `getter` quan registrem l'extensió.\n",
    "\n",
    "El token \"azul\" ara retorna `True` per a `._.is_color`.\n",
    "\n",
    "- Les extensions de `Span` quasi sempre haurien d'usar un getter\n",
    "\n",
    "```python\n",
    "from spacy.tokens import Span\n",
    "\n",
    "# Define la función getter\n",
    "def get_has_color(span):\n",
    "    colors = [\"rojo\", \"amarillo\", \"azul\"]\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Añade una extensión en el Span con getter\n",
    "Span.set_extension(\"has_color\", getter=get_has_color)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "print(doc[1:4]._.has_color, \"-\", doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, \"-\", doc[0:2].text)\n",
    "```\n",
    "\n",
    "```out\n",
    "True - cielo es azul\n",
    "False - El cielo\n",
    "```\n",
    "\n",
    "Si vols afegir extensions d'atributs en un span, quasi sempre has d'usar una extensió de propietats amb un getter. D'una altra manera, hauries d'actualitzar a mà _cadascun dels spans possibles_ per a afegir tots els valors.\n",
    "\n",
    "En aquest exemple, la funció `get_has_color` pren el span i retorna si el text d'algun dels tokens està en la llista de colors.\n",
    "\n",
    "Després d'haver processat el doc, podem revisar els diferents slices del doc i la propietat personalitzada `._.has_color` ens retornarà un resultat sobre si el span conté un token de color o no.\n",
    "\n",
    "### Extensió de mètodes\n",
    "\n",
    "- Assigna una __funció__ que passa a estar disponible com a mètode d'un objecte\n",
    "- Et permet passar __arguments__ a la funció d'extensió\n",
    "\n",
    "```python\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define un método con argumentos\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Añade una extensión en el Doc con el método\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "print(doc._.has_token(\"azul\"), \"- azul\")\n",
    "print(doc._.has_token(\"nube\"), \"- nube\")\n",
    "```\n",
    "\n",
    "```out\n",
    "True - azul\n",
    "False - nube\n",
    "```\n",
    "\n",
    "L'extensió de mètodes fa que l'extensió de l'atribut siga un mètode que pot ser anomenat.\n",
    "\n",
    "Pots passar-li un o més arguments i calcular els valors de l'atribut de manera dinàmica - per exemple, basats en un cert argument o configuració.\n",
    "\n",
    "En aquest exemple, la funció del mètode revisa si el doc conté un token amb un text donat. El primer argument del mètode és sempre l'objecte en si - en aquest cas, el doc. Es passa automàticament quan es diu al mètode.\n",
    "Tots els altres arguments de la funció seran arguments en l'extensió del mètode. En aquest cas, `token_text`.\n",
    "\n",
    "Ací el mètode personalitzat, `._.has_token`, retorna `True` per a la paraula \"azul\" i `False` per a la paraula \"nube\".\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "removed-production",
   "metadata": {},
   "source": [
    "## Afegint extensions d'atributs (1)\n",
    "\n",
    "Practicarem afegint algunes extensions d'atributs.\n",
    "\n",
    "### Pas 1\n",
    "\n",
    "- Usa `Token.set_extension` per a registrar `\"is_country\"` (per defecte `False`).\n",
    "- Actualitza-ho per a `\"España\"` i imprimeix-ho en pantalla per a tots els tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dressed-credits",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Vivo', False), ('en', False), ('España', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.es import Spanish\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Registra la extensión de atributo del Token, \"is_country\",\n",
    "# con el valor por defecto False \n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "# Procesa el texto y pon True para el atributo \"is_country\"\n",
    "# para el token \"España\"\n",
    "doc = nlp(\"Vivo en España.\")\n",
    "doc[2]._.is_country = True\n",
    "\n",
    "# Imprime en pantalla el texto del token y el atributo \"is_country\"\n",
    "# para todos los tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "quantitative-brooklyn",
   "metadata": {},
   "source": [
    "### Pas 2\n",
    "\n",
    "- Usa `Token.set_extension` per a registrar `\"reversed\"` (funció getter\n",
    " `get_reversed`).\n",
    "- Imprimeix en pantalla el seu valor per cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "statutory-capital",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invertido: sadoT\n",
      "invertido: sal\n",
      "invertido: senoicazilareneg\n",
      "invertido: nos\n",
      "invertido: saslaf\n",
      "invertido: ,\n",
      "invertido: odneyulcni\n",
      "invertido: atse\n",
      "invertido: .\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.es import Spanish\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Define la función getter que toma un token y devuelve su texto al revés\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "\n",
    "# Registra la extensión de propiedad del Token, \"reversed\", con\n",
    "# el getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed)\n",
    "\n",
    "# Procesa el texto e imprime en pantalla el atributo \"reversed\"\n",
    "# para cada token\n",
    "doc = nlp(\"Todas las generalizaciones son falsas, incluyendo esta.\")\n",
    "for token in doc:\n",
    "    print(\"invertido:\", token._.reversed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sticky-burden",
   "metadata": {},
   "source": [
    "## Afegint extensions d'atributs (2)\n",
    "\n",
    "Intentem afegir alguns atributs més complexos usant getters i extensions\n",
    "de mètodes.\n",
    "\n",
    "### Part 1\n",
    "\n",
    "- Completa la funció `get_has_number`.\n",
    "- Usa `Doc.set_extension` per a registrar `\"has_number\"` (getter\n",
    " `get_has_number`) i imprimir el seu valor en pantalla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "particular-range",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.es import Spanish\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Define la función getter\n",
    "def get_has_number(doc):\n",
    "    # Devuelve si alguno de los tokens en el doc devuelve True\n",
    "    # para token.like_num\n",
    "    return any(token.is_digit for token in doc)\n",
    "\n",
    "\n",
    "# Registra la extensión de propiedad del Doc, \"has_number\",\n",
    "# con el getter get_has_number\n",
    "doc.set_extension(\"has_number\", getter=get_has_number)\n",
    "\n",
    "# Procesa el texto y revisa el atributo personalizado \"has_number\"\n",
    "doc = nlp(\"El museo cerró por cinco años en el 2012.\")\n",
    "print(\"has_number:\", doc._.has_number)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "attractive-match",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "- Usa `Span.set_extension` per a registrar `\"to_HTML\"` (mètode `to_HTML`).\n",
    "- Crida-ho sobre `doc[0:2]` amb el tag `\"strong\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "following-stereo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>Hola mundo</strong>\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.es import Spanish\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = Spanish()\n",
    "\n",
    "# Define el método\n",
    "def to_html(span, tag):\n",
    "    # Envuelve el texto del span en un HTML tag y devuélvelo\n",
    "    return f\"<{tag}>{span.text}</{tag}>\"\n",
    "\n",
    "\n",
    "# Registra la extensión de propiedad del Span, \"to_html\",\n",
    "# con el método \"to_html\"\n",
    "Span.set_extension(\"to_html\", method=to_html)\n",
    "\n",
    "# Procesa el texto y llama el método \"to_html\"en el span\n",
    "# con el nombre de tag \"strong\"\n",
    "doc = nlp(\"Hola mundo, esto es una frase.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html(\"strong\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "medieval-exclusion",
   "metadata": {},
   "source": [
    "## Entitats i extensions\n",
    "\n",
    "En aquest exercici combinaràs l'extensió d'atributs personalitzats amb les\n",
    "prediccions del model i crearàs un getter d'atribut que retorna una URL de\n",
    "cerca de Wikipedia si el span és una persona, organització o lloc.\n",
    "\n",
    "- Completa el getter `get_wikipedia_url` perquè només retorne la URL si el\n",
    " label del span està en la llista de labels.\n",
    "- Afig l'extensió del `Span`, `\"wikipedia_url\"`, usant el getter\n",
    " `get_wikipedia_url`.\n",
    "- Itera sobre les entitats en el `doc` i retorna els seus URLs de Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "stable-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Bowie https://es.wikipedia.org/w/index.php?search=David_Bowie\n",
      "Alemania https://es.wikipedia.org/w/index.php?search=Alemania\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Obtén la URL de Wikipedia si el span tiene uno de los siguientes labels\n",
    "    if span.label_ in (\"PER\", \"ORG\", \"LOC\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://es.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "\n",
    "# Añade la extensión del Span, wikipedia_url, usando el getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Antes de finalizar 1976, el interés de David Bowie en la \"\n",
    "    \"floreciente escena musical alemana, le llevó a mudarse a \"\n",
    "    \"Alemania para revitalizar su carrera.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto y la URL de Wikipedia de la entidad\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "vocational-techno",
   "metadata": {},
   "source": [
    "## Components amb extensions\n",
    "\n",
    "L'extensió d'atributs és especialment poderosa si és combinada amb els\n",
    "components personalitzats del pipeline. En aquest exercici escriuràs un\n",
    "component del pipeline que troba noms de països i una extensió d'atribut\n",
    "personalitzada que retorna la ciutat capital del país si està\n",
    "disponible.\n",
    "\n",
    "Un patró de frases amb tots els països està disponible com la variable\n",
    "`matcher`. Un diccionari de països relacionats amb les seues ciutats capitals està\n",
    "disponible com la variable `CAPITALS`.\n",
    "\n",
    "- Completa el `countries_component` i crea un `Span` amb el label `\"LOC\"` per a\n",
    " tots els resultats.\n",
    "- Afig el component al pipeline.\n",
    "- Registra l'extensió de l'atribut del Span, `\"capital\"`, amb el getter\n",
    " `get_capital`.\n",
    "- Processa el text i imprimeix en pantalla el text de l'entitat, el label i la\n",
    " capital de l'entitat per a cada span que conté una entitat en `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "broadband-limit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countries_component']\n",
      "[('República Checa', 'LOC', 'Prague'), ('República Eslovaca', 'LOC', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.lang.es import Spanish\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.language import Language\n",
    "\n",
    "with open(\"P3_materiales/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"P3_materiales/capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = Spanish()\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", None, *list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "@Language.component('countries_component')\n",
    "def countries_component(doc):\n",
    "    # Crea un Span de entidades con el label \"LOC\" para todos los resultados\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"LOC\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Añade el componente al pipeline\n",
    "nlp.add_pipe(\"countries_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# El getter que busca el texto del span en un diccionario de ciudades \n",
    "# capitales de países\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Registra la extensión de atributo del Span, \"capital\", con el \n",
    "# getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Procesa el texto e imprime en pantalla el texto de la entidad,\n",
    "# el label y los atributos \"capital\"\n",
    "doc = nlp(\n",
    "    \"La República Checa podría ayudar a la República Eslovaca \"\n",
    "    \"a proteger su espacio aéreo\"\n",
    ")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
