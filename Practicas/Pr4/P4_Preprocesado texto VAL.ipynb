{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dress-auditor",
   "metadata": {},
   "source": [
    "# Pràctica 4 PLN: pre-processat de text i extracció de característiques\n",
    "En aquesta pràctica realitzarem la neteja i pre-processat de diferents conjunts de dades de text.\\\n",
    "Després farem una extracció de característiques com a matriu *sparse* i com a vectors de paraules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "prime-fever",
   "metadata": {},
   "source": [
    "### Noms:\n",
    "Introdueix en aquesta cel·la els noms dels dos integrants del grup:\\\n",
    "*Alumne 1* \\\n",
    "*Alumne 2*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "following-weapon",
   "metadata": {},
   "source": [
    "## Part 1: conjunt de textos de \"mundocine\"\n",
    "En aquest primer conjunt de dades tenim una sèrie de crítiques de pel·lícules de cinema, emmagatzemades en format XML (una crítica per arxiu). Hem preparat una funció de tipus `generator` que processa el directori on estan els arxius de les crítiques i retorna per cada arxiu XML una tupla amb 4 valors:\n",
    " - Nom de la pel·lícula (string)\n",
    " - Resum breu de la crítica (string)\n",
    " - Text de la crítica (*string*)\n",
    " - Valoració de la pel·lícula (*int* d'1 a 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wireless-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from xml.dom.minidom import parseString\n",
    "\n",
    "def parse_folder(path):\n",
    "    \"\"\"generator that reads the contents of XML files in a folder\n",
    "    Returns the <body> of the <review> in each XML file.\n",
    "    XML files encoded as 'latin-1'\"\"\"\n",
    "    for file in sorted([f for f in os.listdir(path) if f.endswith('.xml')],\n",
    "                        key=lambda x: int(re.match(r'\\d+',x).group())):\n",
    "        with open(os.path.join(path, file), encoding='latin-1') as f:\n",
    "            doc=parseString(f.read())\n",
    "\n",
    "            titulo = doc.documentElement.attributes[\"title\"].value\n",
    "\n",
    "            btxt = \"\"\n",
    "            review_bod = doc.getElementsByTagName(\"body\")\n",
    "            if len(review_bod) > 0:\n",
    "                for node in review_bod[0].childNodes:\n",
    "                    if node.nodeType == node.TEXT_NODE:\n",
    "                        btxt += node.data + \" \"\n",
    "\n",
    "            rtxt = \"\"\n",
    "            review_summ = doc.getElementsByTagName(\"summary\")\n",
    "            if len(review_summ) > 0:\n",
    "                for node in review_summ[0].childNodes:\n",
    "                    if node.nodeType == node.TEXT_NODE:\n",
    "                        rtxt += node.data + \" \"\n",
    "                        \n",
    "            rank = int(doc.documentElement.attributes[\"rank\"].value)\n",
    "            \n",
    "            yield titulo, rtxt, btxt, rank\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "automotive-gentleman",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Les crítiques es troben en el directori \"critiques\" (si no tens el directori, descomprimeix l'arxiu \"critiques.zip\" que s'entrega en el material de la pràctica. \\\n",
    "Càrrega la primera crítica del directori usant el mètode `next` sobre la funció `parse_folder` en l'objecte `critica`. Mostra els seus 4 valors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "median-cambodia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La guerra de los mundos'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critica = next(parse_folder(\"criticas\"))\n",
    "critica[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "excellent-guitar",
   "metadata": {},
   "source": [
    "## Anàlisi exploratòria\n",
    "Abans de processar el text calcularem una sèrie de paràmetres de cada crítica. \\\n",
    "Per a això processem cada crítica per a i guardarem els resultats en un objecte `DataFrame` de Colles.\\\n",
    "Com a característica de cada crítica extraurem:\n",
    "- Títol de la pel·lícula\n",
    "- Longitud (en caràcters) del resum\n",
    "- Longitud (en caràcters) del text de la crítica\n",
    "- Puntuació de la crítica\n",
    "\n",
    "### Exercici\n",
    "Completa el codi següent per a generar el `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fancy-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#creamos una lista en blanco\n",
    "datos = list()\n",
    "\n",
    "#recorremos las críticas y calculamos sus métricas\n",
    "for c in parse_folder(\"criticas\"):\n",
    "    datos.append({\n",
    "        'título': c[0],\n",
    "        'LongResumen': len(c[1]),\n",
    "        'LongCritica': len(c[2]),\n",
    "        'puntuación': c[3]\n",
    "    })\n",
    "\n",
    "resumen = pd.DataFrame(datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "outdoor-jordan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>título</th>\n",
       "      <th>LongResumen</th>\n",
       "      <th>LongCritica</th>\n",
       "      <th>puntuación</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La guerra de los mundos</td>\n",
       "      <td>32</td>\n",
       "      <td>2951</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stars Wars III La venganza de los Sith</td>\n",
       "      <td>30</td>\n",
       "      <td>3954</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Los Increibles</td>\n",
       "      <td>68</td>\n",
       "      <td>2467</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spiderman 2</td>\n",
       "      <td>45</td>\n",
       "      <td>3403</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La casa de cera</td>\n",
       "      <td>42</td>\n",
       "      <td>1696</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>El internado</td>\n",
       "      <td>31</td>\n",
       "      <td>1163</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>La llave del mal</td>\n",
       "      <td>36</td>\n",
       "      <td>1781</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>May</td>\n",
       "      <td>28</td>\n",
       "      <td>3343</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cinderella Man</td>\n",
       "      <td>33</td>\n",
       "      <td>2885</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>La Novia Cadaver</td>\n",
       "      <td>40</td>\n",
       "      <td>736</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Harry poter y el caliz de fuego</td>\n",
       "      <td>47</td>\n",
       "      <td>5127</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Match point</td>\n",
       "      <td>19</td>\n",
       "      <td>2847</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>King Kong</td>\n",
       "      <td>12</td>\n",
       "      <td>2623</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>El jardinero fiel</td>\n",
       "      <td>43</td>\n",
       "      <td>2078</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Doom</td>\n",
       "      <td>25</td>\n",
       "      <td>1790</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Las cronicas de Narnia</td>\n",
       "      <td>55</td>\n",
       "      <td>1451</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Una historia violenta</td>\n",
       "      <td>32</td>\n",
       "      <td>981</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Saw 2</td>\n",
       "      <td>50</td>\n",
       "      <td>3886</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Crash</td>\n",
       "      <td>30</td>\n",
       "      <td>1478</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Jarhead</td>\n",
       "      <td>69</td>\n",
       "      <td>3413</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Memorias de una Geisha</td>\n",
       "      <td>71</td>\n",
       "      <td>2597</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Brokeback mountain</td>\n",
       "      <td>70</td>\n",
       "      <td>5027</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Alone in the dark</td>\n",
       "      <td>57</td>\n",
       "      <td>1519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Munich</td>\n",
       "      <td>45</td>\n",
       "      <td>4669</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Terror en la niebla</td>\n",
       "      <td>37</td>\n",
       "      <td>4637</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Secretos de familia</td>\n",
       "      <td>52</td>\n",
       "      <td>1541</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Underworld evolution</td>\n",
       "      <td>75</td>\n",
       "      <td>1579</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Lutero</td>\n",
       "      <td>69</td>\n",
       "      <td>3083</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Camaron</td>\n",
       "      <td>22</td>\n",
       "      <td>1448</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    título  LongResumen  LongCritica  \\\n",
       "0                  La guerra de los mundos           32         2951   \n",
       "1   Stars Wars III La venganza de los Sith           30         3954   \n",
       "2                           Los Increibles           68         2467   \n",
       "3                              Spiderman 2           45         3403   \n",
       "4                          La casa de cera           42         1696   \n",
       "5                             El internado           31         1163   \n",
       "6                         La llave del mal           36         1781   \n",
       "7                                      May           28         3343   \n",
       "8                           Cinderella Man           33         2885   \n",
       "9                         La Novia Cadaver           40          736   \n",
       "10         Harry poter y el caliz de fuego           47         5127   \n",
       "11                             Match point           19         2847   \n",
       "12                               King Kong           12         2623   \n",
       "13                       El jardinero fiel           43         2078   \n",
       "14                                    Doom           25         1790   \n",
       "15                  Las cronicas de Narnia           55         1451   \n",
       "16                   Una historia violenta           32          981   \n",
       "17                                   Saw 2           50         3886   \n",
       "18                                   Crash           30         1478   \n",
       "19                                 Jarhead           69         3413   \n",
       "20                  Memorias de una Geisha           71         2597   \n",
       "21                      Brokeback mountain           70         5027   \n",
       "22                       Alone in the dark           57         1519   \n",
       "23                                  Munich           45         4669   \n",
       "24                     Terror en la niebla           37         4637   \n",
       "25                     Secretos de familia           52         1541   \n",
       "26                    Underworld evolution           75         1579   \n",
       "27                                  Lutero           69         3083   \n",
       "28                                 Camaron           22         1448   \n",
       "\n",
       "    puntuación  \n",
       "0            1  \n",
       "1            3  \n",
       "2            5  \n",
       "3            2  \n",
       "4            2  \n",
       "5            2  \n",
       "6            3  \n",
       "7            4  \n",
       "8            2  \n",
       "9            3  \n",
       "10           4  \n",
       "11           5  \n",
       "12           5  \n",
       "13           4  \n",
       "14           3  \n",
       "15           3  \n",
       "16           2  \n",
       "17           3  \n",
       "18           3  \n",
       "19           5  \n",
       "20           3  \n",
       "21           2  \n",
       "22           1  \n",
       "23           2  \n",
       "24           1  \n",
       "25           2  \n",
       "26           3  \n",
       "27           2  \n",
       "28           3  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cordless-combat",
   "metadata": {},
   "source": [
    "## Neteja de text\n",
    "Prepararem aquest conjunt de textos per a entrenar un model per a predir la puntuació de cada crítica a partir del text de la crítica.\\\n",
    "Realitzarem el següent processament:\n",
    "- Separar el text en *tokens*\n",
    "- Eliminar els *tokens* de tipus *stop-word*, signes de puntuació o espais\n",
    "- Convertir les entitats de tipus `PER` al token *persona*\n",
    "- Lematizar el text\n",
    "\n",
    "### Exercici\n",
    "Completa el codi següent per a realitzar aquestes funcions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decreased-injection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "#definimos función de normalizado\n",
    "def normaliza(texto):\n",
    "    doc = nlp(texto)\n",
    "    tokens = [t for t in doc if not t.is_punct and not t.is_space and not t.is_stop] #devuelve los tokens que cumplen las condiciones\n",
    "    palabras = []\n",
    "    for t in tokens:\n",
    "        if t.ent_iob_=='B' and t.ent_type_=='PER':\n",
    "            palabras.append('persona')\n",
    "        elif t.ent_iob_=='I' and t.ent_type_=='PER':\n",
    "            continue\n",
    "        else:\n",
    "            palabras.append(t.lemma_) #si no es PER añadimos el lema\n",
    "    salida = ' '.join(palabras) #junta todos los tokens en un string\n",
    "    \n",
    "    return salida"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "injured-softball",
   "metadata": {},
   "source": [
    "Comprova el seu funcionament en la crítica prèviament descarregada (variable `critica`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "wicked-circus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gustar cine masa pelicula ver mundo parecer coñazo insufribl porqué prota tonto culo suerte peli lograr vencer convertir listo chorrada comienzo pelicula esfumar arte magia volver maduro inteligente peli Spielberg huir huir dar tiro cabron meter par actor echar él comer aparte niña viejo metido cuerpo niña ver él hablar version original dar él freaks cine creerar gracia nena puta madre causar pavor cria persona maduro horroroso niño niño ver él rol asustar hijo adolescente Cruise subnormal dar él bofetada ver hueso mano persona reflejo denominar manipulacion militar chico matar bicho ningun arma persona loco pensar venir saco quereis decir fanatismo locura alguien querer luchar medio muerte seguro jodar sobremanera pelicula aparecer mongo abuelo familia salir dios viejo aparecer traje domingo maquillada.¿pero sinsorgado faltar persona volver exmujer mundo tranquilos contar peli pasar peli acabar restar misterio bodrio persona paso convertir director efecto especial contar historias?¿no distei norteamericano guardar compostura gritar momento dificil sensato tranquilizar él discutir chorrada puta peli parar gritar pegar él gracia comienzo peli suelo empezar resquebrajar él gente quedar mirar pasar persona estaria correr puta tiempo escondiéndome ver bicho lanzar rayo machacar to dios esconder seguir hablar mierda queriar conseguir pagasemos entrada cine prima bajarir internet ver calidad peli aceptable pasar él palante mando forward mano morir'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normaliza(critica[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beneficial-arkansas",
   "metadata": {},
   "source": [
    "### Possibles millores sobre el processament\n",
    "Observem els següents problemes:\n",
    "- Algunes paraules no se separen correctament perquè en el text original estan unides per signes de puntuació.\n",
    "- La llista de *stop-words* de spaCy no conté 'a','e','i'.\n",
    "- Alguns lemes mantenen les majúscules.\\\n",
    "\n",
    "Redefinim la funció de normalització per a corregir això:\n",
    "- Introduïm un espai després de determinats signes de puntuació (\".\", \"?\") perquè la divisió en tokens siga correcta\n",
    "- Filtrem els *tokens* amb una longitud d'1\n",
    "- Passem a minúscules el lema de cada token\n",
    "\n",
    "Completa la funció per a realitzar aquestes correccions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "turned-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normaliza_bis(texto):\n",
    "    texto = re.sub(r\"([?.])\", r\"\\1 \", texto) #añadimos un espacio después de \".\" y \"?\"\n",
    "    doc = nlp(texto)\n",
    "    tokens = [t for t in doc if not t.is_punct and not t.is_space and not t.is_stop and len(t) > 1] #filtramos los tokens que nos interesan\n",
    "    palabras = []\n",
    "    for t in tokens:\n",
    "        if t.ent_iob_=='B' and t.ent_type_=='PER':\n",
    "            palabras.append('persona')\n",
    "        elif t.ent_iob_=='I' and t.ent_type_=='PER':\n",
    "            continue\n",
    "        else:\n",
    "            palabras.append(t.lemma_.lower()) #añadimos lema en minúsculas\n",
    "    salida = ' '.join(palabras) #junta todos los tokens en un string\n",
    "    \n",
    "    return salida"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "younger-running",
   "metadata": {},
   "source": [
    "Comprova el seu funcionament en la crítica prèviament descarregada (variable `critica`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "economic-serve",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gustar cine masa pelicula ver mundo parecer coñazo insufribl porqué prota tonto culo suerte peli lograr vencer convertir listo chorrada comienzo pelicula esfumar arte magia volver maduro inteligente peli spielberg huir huir dar tiro cabron meter par actor echar él comer aparte niña viejo metido cuerpo niña ver él hablar version original dar él freaks cine creerar gracia nena puta madre causar pavor cria persona maduro horroroso niño niño ver él rol asustar hijo adolescente cruise subnormal dar él bofetada ver hueso mano persona reflejo denominar manipulacion militar chico matar bicho ningun arma persona loco pensar venir saco quereis decir fanatismo locura alguien querer luchar medio muerte seguro jodar sobremanera pelicula aparecer mongo abuelo familia salir dios viejo aparecer traje domingo maquillada sinsorgado faltar persona volver exmujer mundo tranquilos contar peli pasar peli acabar restar misterio bodrio persona paso convertir director efecto especial contar historia disteis norteamericano guardar compostura gritar momento dificil sensato tranquilizar él discutir chorrada puta peli parar gritar pegar él gracia comienzo peli suelo empezar resquebrajar él gente quedar mirar pasar persona estaria correr puta tiempo escondiéndome ver bicho lanzar rayo machacar to dios esconder seguir hablar mierda queriar conseguir pagasemos entrada cine prima bajarir internet ver calidad peli aceptable pasar él palante mando forward mano morir'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normaliza_bis(critica[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "funny-parcel",
   "metadata": {},
   "source": [
    "### Anàlisi morfològica\n",
    "En una crítica té molta importància els adjectius utilitzats.\\\n",
    "Crea una funció per a filtrar només els adjectius utilitzats en cada crítica (utilitza el lema de cada adjectiu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "special-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_adj(texto):\n",
    "    texto = re.sub(r'([?.])', r'\\1 ', texto) #separamos . y ?\n",
    "    doc = nlp(texto)\n",
    "    tokens = [t.lemma_ for t in doc if t.pos_=='ADJ']\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae1e902",
   "metadata": {},
   "source": [
    "Comprova el seu funcionament en la crítica prèviament descarregada (variable `critica`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "immune-sauce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'insufribl chorrada maduro inteligente solo viejo metido original claro grande igual maduro horroroso adolescente subnormal militar loco seguro viejo sinsorgado solo mejor igual bueno especial dificil sensato chorrada aceptable'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraer_adj(critica[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "limited-integrity",
   "metadata": {},
   "source": [
    "### Processament de tot el conjunt de dades\n",
    "Aplicarem aquestes funcions en bloc a tot el conjunt de dades. \\\n",
    "En aquesta ocasió, no farem res amb el text normalitzat sinó que només ho aplicarem per a calcular el núm. de paraules i el núm. d'adjectius de cada crítica.\n",
    "\n",
    "### Exercici\n",
    "Completa el codi següent:\\\n",
    "Nota: tingues en compte que per a comptar paraules has de dividir el *string* en espais i comptar el núm. d'elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "least-indication",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>título</th>\n",
       "      <th>LongResumen</th>\n",
       "      <th>LongCritica</th>\n",
       "      <th>NumPalabras</th>\n",
       "      <th>NumAdj</th>\n",
       "      <th>puntuación</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La guerra de los mundos</td>\n",
       "      <td>7</td>\n",
       "      <td>569</td>\n",
       "      <td>210</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stars Wars III La venganza de los Sith</td>\n",
       "      <td>9</td>\n",
       "      <td>762</td>\n",
       "      <td>277</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Los Increibles</td>\n",
       "      <td>12</td>\n",
       "      <td>464</td>\n",
       "      <td>166</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spiderman 2</td>\n",
       "      <td>10</td>\n",
       "      <td>653</td>\n",
       "      <td>255</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La casa de cera</td>\n",
       "      <td>8</td>\n",
       "      <td>314</td>\n",
       "      <td>122</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>El internado</td>\n",
       "      <td>7</td>\n",
       "      <td>210</td>\n",
       "      <td>81</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>La llave del mal</td>\n",
       "      <td>9</td>\n",
       "      <td>319</td>\n",
       "      <td>118</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>May</td>\n",
       "      <td>6</td>\n",
       "      <td>565</td>\n",
       "      <td>223</td>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cinderella Man</td>\n",
       "      <td>7</td>\n",
       "      <td>493</td>\n",
       "      <td>189</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>La Novia Cadaver</td>\n",
       "      <td>8</td>\n",
       "      <td>130</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Harry poter y el caliz de fuego</td>\n",
       "      <td>8</td>\n",
       "      <td>877</td>\n",
       "      <td>361</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Match point</td>\n",
       "      <td>4</td>\n",
       "      <td>484</td>\n",
       "      <td>198</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>King Kong</td>\n",
       "      <td>4</td>\n",
       "      <td>480</td>\n",
       "      <td>190</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>El jardinero fiel</td>\n",
       "      <td>8</td>\n",
       "      <td>335</td>\n",
       "      <td>156</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Doom</td>\n",
       "      <td>6</td>\n",
       "      <td>318</td>\n",
       "      <td>112</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Las cronicas de Narnia</td>\n",
       "      <td>11</td>\n",
       "      <td>252</td>\n",
       "      <td>91</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Una historia violenta</td>\n",
       "      <td>5</td>\n",
       "      <td>164</td>\n",
       "      <td>75</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Saw 2</td>\n",
       "      <td>12</td>\n",
       "      <td>646</td>\n",
       "      <td>279</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Crash</td>\n",
       "      <td>7</td>\n",
       "      <td>261</td>\n",
       "      <td>96</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Jarhead</td>\n",
       "      <td>11</td>\n",
       "      <td>579</td>\n",
       "      <td>245</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Memorias de una Geisha</td>\n",
       "      <td>13</td>\n",
       "      <td>454</td>\n",
       "      <td>166</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Brokeback mountain</td>\n",
       "      <td>14</td>\n",
       "      <td>853</td>\n",
       "      <td>341</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Alone in the dark</td>\n",
       "      <td>11</td>\n",
       "      <td>267</td>\n",
       "      <td>107</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Munich</td>\n",
       "      <td>6</td>\n",
       "      <td>747</td>\n",
       "      <td>355</td>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Terror en la niebla</td>\n",
       "      <td>7</td>\n",
       "      <td>827</td>\n",
       "      <td>289</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Secretos de familia</td>\n",
       "      <td>11</td>\n",
       "      <td>272</td>\n",
       "      <td>111</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Underworld evolution</td>\n",
       "      <td>15</td>\n",
       "      <td>278</td>\n",
       "      <td>94</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Lutero</td>\n",
       "      <td>11</td>\n",
       "      <td>519</td>\n",
       "      <td>214</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Camaron</td>\n",
       "      <td>6</td>\n",
       "      <td>264</td>\n",
       "      <td>101</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    título  LongResumen  LongCritica  \\\n",
       "0                  La guerra de los mundos            7          569   \n",
       "1   Stars Wars III La venganza de los Sith            9          762   \n",
       "2                           Los Increibles           12          464   \n",
       "3                              Spiderman 2           10          653   \n",
       "4                          La casa de cera            8          314   \n",
       "5                             El internado            7          210   \n",
       "6                         La llave del mal            9          319   \n",
       "7                                      May            6          565   \n",
       "8                           Cinderella Man            7          493   \n",
       "9                         La Novia Cadaver            8          130   \n",
       "10         Harry poter y el caliz de fuego            8          877   \n",
       "11                             Match point            4          484   \n",
       "12                               King Kong            4          480   \n",
       "13                       El jardinero fiel            8          335   \n",
       "14                                    Doom            6          318   \n",
       "15                  Las cronicas de Narnia           11          252   \n",
       "16                   Una historia violenta            5          164   \n",
       "17                                   Saw 2           12          646   \n",
       "18                                   Crash            7          261   \n",
       "19                                 Jarhead           11          579   \n",
       "20                  Memorias de una Geisha           13          454   \n",
       "21                      Brokeback mountain           14          853   \n",
       "22                       Alone in the dark           11          267   \n",
       "23                                  Munich            6          747   \n",
       "24                     Terror en la niebla            7          827   \n",
       "25                     Secretos de familia           11          272   \n",
       "26                    Underworld evolution           15          278   \n",
       "27                                  Lutero           11          519   \n",
       "28                                 Camaron            6          264   \n",
       "\n",
       "    NumPalabras  NumAdj  puntuación  \n",
       "0           210      29           1  \n",
       "1           277      50           3  \n",
       "2           166      33           5  \n",
       "3           255      37           2  \n",
       "4           122      23           2  \n",
       "5            81      20           2  \n",
       "6           118      23           3  \n",
       "7           223      59           4  \n",
       "8           189      46           2  \n",
       "9            48       6           3  \n",
       "10          361      83           4  \n",
       "11          198      46           5  \n",
       "12          190      38           5  \n",
       "13          156      33           4  \n",
       "14          112      19           3  \n",
       "15           91      23           3  \n",
       "16           75      15           2  \n",
       "17          279      49           3  \n",
       "18           96      18           3  \n",
       "19          245      44           5  \n",
       "20          166      41           3  \n",
       "21          341      80           2  \n",
       "22          107      15           1  \n",
       "23          355      86           2  \n",
       "24          289      71           1  \n",
       "25          111      24           2  \n",
       "26           94      23           3  \n",
       "27          214      45           2  \n",
       "28          101      20           3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creamos una lista en blanco\n",
    "datos = []\n",
    "\n",
    "#recorremos las críticas y calculamos sus métricas\n",
    "for c in parse_folder(\"criticas\"):\n",
    "    datos.append({\n",
    "        'título': c[0],\n",
    "        'LongResumen': len(c[1].split(' ')),\n",
    "        'LongCritica': len(c[2].split(' ')),\n",
    "        'NumPalabras': len(normaliza_bis(c[2]).split(' ')),\n",
    "        'NumAdj': len(extraer_adj(c[2]).split(' ')),\n",
    "        'puntuación': c[3]\n",
    "    })\n",
    "\n",
    "resumen = pd.DataFrame(datos)\n",
    "resumen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "laden-special",
   "metadata": {},
   "source": [
    "## Part 2: Extracció de característiques *sparse*\n",
    "Anem a calcules les matrius de característiques *bag-of-words* i *tfidf* del conjunt de textos anterior.\\\n",
    "Usarem la llibreria `scikit-learn` per a vectorizar els documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "critical-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para no tener que cargar todas las críticas en memoria,\n",
    "#creamos un generador que devuelve iterativamente el\n",
    "#texto procesado de cada crítica\n",
    "\n",
    "def generaCritica(criticas):\n",
    "    \"\"\"Función de tipo generator que devuelve el\n",
    "    texto normalizado de cada crítica.\n",
    "    Entrada:\n",
    "    criticas: objeto 'parse_folder' que itera\n",
    "    sobre el directio de las críticas\n",
    "    Salida:\n",
    "    texto normalizado de cada crítica\"\"\"\n",
    "    for c in criticas:\n",
    "        yield normaliza_bis(c[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "young-receptor",
   "metadata": {},
   "source": [
    "Comprova el seu funcionament generant el text normalitzat de la primera crítica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "arbitrary-walter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gustar cine masa pelicula ver mundo parecer coñazo insufribl porqué prota tonto culo suerte peli lograr vencer convertir listo chorrada comienzo pelicula esfumar arte magia volver maduro inteligente peli spielberg huir huir dar tiro cabron meter par actor echar él comer aparte niña viejo metido cuerpo niña ver él hablar version original dar él freaks cine creerar gracia nena puta madre causar pavor cria persona maduro horroroso niño niño ver él rol asustar hijo adolescente cruise subnormal dar él bofetada ver hueso mano persona reflejo denominar manipulacion militar chico matar bicho ningun arma persona loco pensar venir saco quereis decir fanatismo locura alguien querer luchar medio muerte seguro jodar sobremanera pelicula aparecer mongo abuelo familia salir dios viejo aparecer traje domingo maquillada sinsorgado faltar persona volver exmujer mundo tranquilos contar peli pasar peli acabar restar misterio bodrio persona paso convertir director efecto especial contar historia disteis norteamericano guardar compostura gritar momento dificil sensato tranquilizar él discutir chorrada puta peli parar gritar pegar él gracia comienzo peli suelo empezar resquebrajar él gente quedar mirar pasar persona estaria correr puta tiempo escondiéndome ver bicho lanzar rayo machacar to dios esconder seguir hablar mierda queriar conseguir pagasemos entrada cine prima bajarir internet ver calidad peli aceptable pasar él palante mando forward mano morir'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(generaCritica(parse_folder('criticas')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adjusted-administration",
   "metadata": {},
   "source": [
    "Vectorizem tot el conjunt de dades usant les funcions de `scikit-learn`.\\\n",
    "Aquestes funcions admeten un objecte `generator` com a argument d'entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "collective-publisher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "\n",
    "criticas = generaCritica(parse_folder('criticas'))#creamos el objeto generador\n",
    "\n",
    "BoW_criticas = vect.fit(list(criticas))\n",
    "BoW_criticas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "stock-wings",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Genera diferents variants de matrius de característiques per al conjunt de les crítiques. Prova amb:\n",
    "- Matriu TF-IDF\n",
    "- Matriu BoW amb unigrames i bigrames\n",
    "- Matriu TF-IDF eliminant les paraules menys freqüents i les més freqüents (mínim de 2 i màxim de 5 documents)\n",
    "- Mostra quines són les paraules més freqüents eliminades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "southern-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(norm=None, use_idf=True)\n",
    "criticas = generaCritica(parse_folder('criticas'))\n",
    "tv_matrix = tv.fit_transform(list(criticas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "opposite-pavilion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(ngram_range=(1, 2))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 2))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(ngram_range=(1, 2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#matriz BoW con unigramas y bigramas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "criticas = generaCritica(parse_folder('criticas'))#creamos el objeto generador\n",
    "\n",
    "BoW_criticas = vect.fit(list(criticas))\n",
    "BoW_criticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "composed-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matriz TF-IDF con min_df=1 y max_df=5  SON EL MINIMO DE DOCS EN LOS q aparece y el máximo\n",
    "tv = TfidfVectorizer(norm=None, use_idf=True, min_df=1, max_df=5)\n",
    "criticas = generaCritica(parse_folder('criticas'))\n",
    "tv_matrix = tv.fit_transform(list(criticas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bibliographic-pleasure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acabar',\n",
       " 'actor',\n",
       " 'allá',\n",
       " 'año',\n",
       " 'cara',\n",
       " 'casa',\n",
       " 'cine',\n",
       " 'contar',\n",
       " 'convertir',\n",
       " 'cosa',\n",
       " 'dar',\n",
       " 'decir',\n",
       " 'defraudar',\n",
       " 'dejar',\n",
       " 'dios',\n",
       " 'director',\n",
       " 'duda',\n",
       " 'echar',\n",
       " 'efecto',\n",
       " 'empezar',\n",
       " 'encontrar',\n",
       " 'escena',\n",
       " 'especial',\n",
       " 'esperar',\n",
       " 'film',\n",
       " 'forma',\n",
       " 'guión',\n",
       " 'gustar',\n",
       " 'haber',\n",
       " 'hablar',\n",
       " 'hacer',\n",
       " 'historia',\n",
       " 'hora',\n",
       " 'intentar',\n",
       " 'interesante',\n",
       " 'ir',\n",
       " 'juego',\n",
       " 'llamar',\n",
       " 'llegar',\n",
       " 'lleno',\n",
       " 'malo',\n",
       " 'mano',\n",
       " 'matar',\n",
       " 'mil',\n",
       " 'momento',\n",
       " 'mostrar',\n",
       " 'muerte',\n",
       " 'mujer',\n",
       " 'mundo',\n",
       " 'niña',\n",
       " 'niño',\n",
       " 'obra',\n",
       " 'ocasión',\n",
       " 'original',\n",
       " 'pantalla',\n",
       " 'papel',\n",
       " 'parecer',\n",
       " 'pasar',\n",
       " 'peli',\n",
       " 'película',\n",
       " 'pensar',\n",
       " 'persona',\n",
       " 'personaje',\n",
       " 'poder',\n",
       " 'poner',\n",
       " 'principal',\n",
       " 'protagonista',\n",
       " 'punto',\n",
       " 'quedar',\n",
       " 'querer',\n",
       " 'representar',\n",
       " 'salir',\n",
       " 'seguir',\n",
       " 'terminar',\n",
       " 'terror',\n",
       " 'tiempo',\n",
       " 'tipo',\n",
       " 'venir',\n",
       " 'ver',\n",
       " 'vida',\n",
       " 'volver',\n",
       " 'yo',\n",
       " 'él',\n",
       " 'único'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#palabras más frecuentes a eliminar\n",
    "#aparecen en el atributo 'stop_words_' del vectorizador\n",
    "tv.stop_words_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "primary-intention",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "Ara calcularem els *word vectors* de les paraules del nostre conjunt de dades, usant la classe `word2vec` de la llibreria `gensim`.\\\n",
    "Aquesta llibreria accepta com a argument d'entrada un objecte `iterador` que generarà el text pre-processament de la següent crítica en la seqüència.\\\n",
    "Usarem les funcions de pre-processament de la llibreria `gensim`.\\\n",
    "Primer definim un objecte de tipus `iterator` per a recórrer les crítiques. Es diferencia d'un simple `generator` que es pot reiniciar la generació de la seqüència (necessari per al model `word2vec`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "scenic-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "        \n",
    "class PreprocesaCriticas(object):\n",
    "    \"\"\"Pre-procesa el corpus de críticas con la función 'simple_preprocess'\n",
    "    de la librería gensim\n",
    "    Entrada: directorio de críticas\n",
    "    Salida: iterador sobre las críticas (como lista de tokens)\"\"\"\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for c in parse_folder(self.dirname):\n",
    "            yield simple_preprocess(c[2], deacc=True, min_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "saving-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instanciamos un objeto para nuestro directorio\n",
    "criticas = PreprocesaCriticas('criticas')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "pending-passion",
   "metadata": {},
   "source": [
    "Per a provar el seu funcionament amb la primera crítica el convertim en iterable i usem el mètode `next`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "careful-wagon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cada',\n",
       " 'vez',\n",
       " 'me',\n",
       " 'gusta',\n",
       " 'menos',\n",
       " 'el',\n",
       " 'cine',\n",
       " 'de',\n",
       " 'masas',\n",
       " 'las',\n",
       " 'peliculas',\n",
       " 'que',\n",
       " 'ven',\n",
       " 'todo',\n",
       " 'el',\n",
       " 'mundo',\n",
       " 'me',\n",
       " 'parecen',\n",
       " 'cada',\n",
       " 'vez',\n",
       " 'mas',\n",
       " 'conazo',\n",
       " 'mas',\n",
       " 'insufribles',\n",
       " 'no',\n",
       " 'se',\n",
       " 'porque',\n",
       " 'pero',\n",
       " 'siempre',\n",
       " 'el',\n",
       " 'prota',\n",
       " 'es',\n",
       " 'tonto',\n",
       " 'del',\n",
       " 'culo',\n",
       " 'tiene',\n",
       " 'suerte',\n",
       " 'al',\n",
       " 'final',\n",
       " 'de',\n",
       " 'la',\n",
       " 'peli',\n",
       " 'cuando',\n",
       " 'ha',\n",
       " 'logrado',\n",
       " 'vencer',\n",
       " 'al',\n",
       " 'mal',\n",
       " 'se',\n",
       " 'convierte',\n",
       " 'en',\n",
       " 'listo',\n",
       " 'las',\n",
       " 'chorradas',\n",
       " 'que',\n",
       " 'hacia',\n",
       " 'al',\n",
       " 'comienzo',\n",
       " 'de',\n",
       " 'la',\n",
       " 'pelicula',\n",
       " 'se',\n",
       " 'esfuman',\n",
       " 'como',\n",
       " 'por',\n",
       " 'arte',\n",
       " 'de',\n",
       " 'magia',\n",
       " 'se',\n",
       " 'vuelve',\n",
       " 'maduro',\n",
       " 'inteligente',\n",
       " 'esta',\n",
       " 'peli',\n",
       " 'de',\n",
       " 'spielberg',\n",
       " 'es',\n",
       " 'mas',\n",
       " 'de',\n",
       " 'lo',\n",
       " 'mismo',\n",
       " 'huir',\n",
       " 'huir',\n",
       " 'que',\n",
       " 'no',\n",
       " 'le',\n",
       " 'den',\n",
       " 'ni',\n",
       " 'un',\n",
       " 'solo',\n",
       " 'tiro',\n",
       " 'ademas',\n",
       " 'el',\n",
       " 'cabron',\n",
       " 'ha',\n",
       " 'metido',\n",
       " 'un',\n",
       " 'par',\n",
       " 'de',\n",
       " 'actores',\n",
       " 'que',\n",
       " 'es',\n",
       " 'como',\n",
       " 'para',\n",
       " 'echarles',\n",
       " 'de',\n",
       " 'comer',\n",
       " 'aparte',\n",
       " 'la',\n",
       " 'nina',\n",
       " 'una',\n",
       " 'vieja',\n",
       " 'metida',\n",
       " 'en',\n",
       " 'el',\n",
       " 'cuerpo',\n",
       " 'de',\n",
       " 'una',\n",
       " 'nina',\n",
       " 'porque',\n",
       " 'solo',\n",
       " 'hay',\n",
       " 'que',\n",
       " 'verle',\n",
       " 'hablar',\n",
       " 'en',\n",
       " 'version',\n",
       " 'original',\n",
       " 'claro',\n",
       " 'para',\n",
       " 'darse',\n",
       " 'cuenta',\n",
       " 'que',\n",
       " 'estamos',\n",
       " 'ante',\n",
       " 'uno',\n",
       " 'de',\n",
       " 'los',\n",
       " 'grandes',\n",
       " 'freaks',\n",
       " 'del',\n",
       " 'cine',\n",
       " 'se',\n",
       " 'creeran',\n",
       " 'que',\n",
       " 'hace',\n",
       " 'gracia',\n",
       " 'la',\n",
       " 'nena',\n",
       " 'cuando',\n",
       " 'habla',\n",
       " 'igual',\n",
       " 'que',\n",
       " 'su',\n",
       " 'puta',\n",
       " 'madre',\n",
       " 'pero',\n",
       " 'mi',\n",
       " 'me',\n",
       " 'causa',\n",
       " 'pavor',\n",
       " 'ver',\n",
       " 'una',\n",
       " 'cria',\n",
       " 'que',\n",
       " 'habla',\n",
       " 'como',\n",
       " 'una',\n",
       " 'persona',\n",
       " 'madura',\n",
       " 'es',\n",
       " 'algo',\n",
       " 'horroroso',\n",
       " 'los',\n",
       " 'ninos',\n",
       " 'son',\n",
       " 'ninos',\n",
       " 'verlos',\n",
       " 'fuera',\n",
       " 'de',\n",
       " 'su',\n",
       " 'rol',\n",
       " 'asusta',\n",
       " 'luego',\n",
       " 'esta',\n",
       " 'el',\n",
       " 'hijo',\n",
       " 'adolescente',\n",
       " 'que',\n",
       " 'tiene',\n",
       " 'el',\n",
       " 'cruise',\n",
       " 'otro',\n",
       " 'subnormal',\n",
       " 'que',\n",
       " 'es',\n",
       " 'para',\n",
       " 'darle',\n",
       " 'de',\n",
       " 'bofetadas',\n",
       " 'hasta',\n",
       " 'que',\n",
       " 'se',\n",
       " 'te',\n",
       " 'vea',\n",
       " 'el',\n",
       " 'hueso',\n",
       " 'la',\n",
       " 'mano',\n",
       " 'fiel',\n",
       " 'reflejo',\n",
       " 'de',\n",
       " 'lo',\n",
       " 'que',\n",
       " 'se',\n",
       " 'denomina',\n",
       " 'manipulacion',\n",
       " 'militar',\n",
       " 'el',\n",
       " 'chico',\n",
       " 'quiere',\n",
       " 'matar',\n",
       " 'los',\n",
       " 'bichos',\n",
       " 'sin',\n",
       " 'ningun',\n",
       " 'arma',\n",
       " 'hale',\n",
       " 'lo',\n",
       " 'loco',\n",
       " 'sin',\n",
       " 'pensar',\n",
       " 'venga',\n",
       " 'saco',\n",
       " 'que',\n",
       " 'quereis',\n",
       " 'que',\n",
       " 'os',\n",
       " 'diga',\n",
       " 'pero',\n",
       " 'mi',\n",
       " 'eso',\n",
       " 'me',\n",
       " 'parece',\n",
       " 'fanatismo',\n",
       " 'locura',\n",
       " 'que',\n",
       " 'alguien',\n",
       " 'quiera',\n",
       " 'ir',\n",
       " 'luchar',\n",
       " 'sin',\n",
       " 'medios',\n",
       " 'es',\n",
       " 'ir',\n",
       " 'una',\n",
       " 'muerte',\n",
       " 'segura',\n",
       " 'por',\n",
       " 'eso',\n",
       " 'me',\n",
       " 'jode',\n",
       " 'sobremanera',\n",
       " 'que',\n",
       " 'al',\n",
       " 'final',\n",
       " 'de',\n",
       " 'la',\n",
       " 'pelicula',\n",
       " 'aparezca',\n",
       " 'el',\n",
       " 'mongo',\n",
       " 'este',\n",
       " 'sus',\n",
       " 'abuelos',\n",
       " 'toda',\n",
       " 'la',\n",
       " 'familia',\n",
       " 'se',\n",
       " 'salva',\n",
       " 'todo',\n",
       " 'dios',\n",
       " 'encima',\n",
       " 'la',\n",
       " 'vieja',\n",
       " 'aparece',\n",
       " 'en',\n",
       " 'traje',\n",
       " 'de',\n",
       " 'los',\n",
       " 'domingos',\n",
       " 'toda',\n",
       " 'maquillada',\n",
       " 'pero',\n",
       " 'que',\n",
       " 'sinsorgada',\n",
       " 'es',\n",
       " 'esta',\n",
       " 'solo',\n",
       " 'falta',\n",
       " 'que',\n",
       " 'cruise',\n",
       " 'vuelva',\n",
       " 'con',\n",
       " 'su',\n",
       " 'exmujer',\n",
       " 'que',\n",
       " 'el',\n",
       " 'mundo',\n",
       " 'sea',\n",
       " 'mucho',\n",
       " 'mejor',\n",
       " 'tranquilos',\n",
       " 'os',\n",
       " 'cuento',\n",
       " 'el',\n",
       " 'final',\n",
       " 'de',\n",
       " 'la',\n",
       " 'peli',\n",
       " 'pero',\n",
       " 'no',\n",
       " 'pasa',\n",
       " 'nada',\n",
       " 'todas',\n",
       " 'las',\n",
       " 'pelis',\n",
       " 'acaban',\n",
       " 'de',\n",
       " 'igual',\n",
       " 'manera',\n",
       " 'decir',\n",
       " 'eso',\n",
       " 'no',\n",
       " 'resta',\n",
       " 'misterio',\n",
       " 'al',\n",
       " 'bodrio',\n",
       " 'este',\n",
       " 'yo',\n",
       " 'no',\n",
       " 'se',\n",
       " 'de',\n",
       " 'que',\n",
       " 'va',\n",
       " 'spielberg',\n",
       " 'pero',\n",
       " 'este',\n",
       " 'paso',\n",
       " 'se',\n",
       " 'va',\n",
       " 'convertir',\n",
       " 'tan',\n",
       " 'solo',\n",
       " 'en',\n",
       " 'un',\n",
       " 'director',\n",
       " 'que',\n",
       " 'es',\n",
       " 'bueno',\n",
       " 'haciendo',\n",
       " 'efectos',\n",
       " 'especiales',\n",
       " 'porque',\n",
       " 'lo',\n",
       " 'que',\n",
       " 'es',\n",
       " 'contando',\n",
       " 'historias',\n",
       " 'no',\n",
       " 'os',\n",
       " 'disteis',\n",
       " 'cuenta',\n",
       " 'de',\n",
       " 'que',\n",
       " 'los',\n",
       " 'norteamericanos',\n",
       " 'no',\n",
       " 'saben',\n",
       " 'guardar',\n",
       " 'la',\n",
       " 'compostura',\n",
       " 'que',\n",
       " 'todo',\n",
       " 'lo',\n",
       " 'dicen',\n",
       " 'gritando',\n",
       " 'en',\n",
       " 'momentos',\n",
       " 'dificiles',\n",
       " 'lo',\n",
       " 'mas',\n",
       " 'sensato',\n",
       " 'es',\n",
       " 'tranquilizarse',\n",
       " 'no',\n",
       " 'discutir',\n",
       " 'por',\n",
       " 'chorradas',\n",
       " 'en',\n",
       " 'toda',\n",
       " 'la',\n",
       " 'puta',\n",
       " 'peli',\n",
       " 'no',\n",
       " 'paran',\n",
       " 'de',\n",
       " 'gritar',\n",
       " 'pegarse',\n",
       " 'entre',\n",
       " 'ellos',\n",
       " 'me',\n",
       " 'hace',\n",
       " 'gracia',\n",
       " 'como',\n",
       " 'al',\n",
       " 'comienzo',\n",
       " 'de',\n",
       " 'la',\n",
       " 'peli',\n",
       " 'cuando',\n",
       " 'el',\n",
       " 'suelo',\n",
       " 'empieza',\n",
       " 'resquebrajarse',\n",
       " 'la',\n",
       " 'gente',\n",
       " 'se',\n",
       " 'queda',\n",
       " 'ahi',\n",
       " 'mirar',\n",
       " 'que',\n",
       " 'pasa',\n",
       " 'joder',\n",
       " 'no',\n",
       " 'se',\n",
       " 'ellos',\n",
       " 'pero',\n",
       " 'yo',\n",
       " 'ya',\n",
       " 'estaria',\n",
       " 'corriendo',\n",
       " 'como',\n",
       " 'una',\n",
       " 'puta',\n",
       " 'desde',\n",
       " 'hace',\n",
       " 'tiempo',\n",
       " 'escondiendome',\n",
       " 'si',\n",
       " 'veo',\n",
       " 'que',\n",
       " 'un',\n",
       " 'bicho',\n",
       " 'esta',\n",
       " 'lanzando',\n",
       " 'rayos',\n",
       " 'machacando',\n",
       " 'to',\n",
       " 'dios',\n",
       " 'me',\n",
       " 'escondo',\n",
       " 'en',\n",
       " 'fin',\n",
       " 'que',\n",
       " 'para',\n",
       " 'que',\n",
       " 'seguir',\n",
       " 'hablando',\n",
       " 'de',\n",
       " 'esta',\n",
       " 'mierda',\n",
       " 'si',\n",
       " 'lo',\n",
       " 'que',\n",
       " 'querian',\n",
       " 'ya',\n",
       " 'lo',\n",
       " 'han',\n",
       " 'conseguido',\n",
       " 'que',\n",
       " 'es',\n",
       " 'que',\n",
       " 'pagasemos',\n",
       " 'la',\n",
       " 'entrada',\n",
       " 'por',\n",
       " 'eso',\n",
       " 'creo',\n",
       " 'que',\n",
       " 'va',\n",
       " 'ir',\n",
       " 'al',\n",
       " 'cine',\n",
       " 'su',\n",
       " 'prima',\n",
       " 'partir',\n",
       " 'de',\n",
       " 'ahora',\n",
       " 'me',\n",
       " 'lo',\n",
       " 'bajare',\n",
       " 'todo',\n",
       " 'de',\n",
       " 'internet',\n",
       " 'que',\n",
       " 'por',\n",
       " 'lo',\n",
       " 'que',\n",
       " 'se',\n",
       " 've',\n",
       " 'la',\n",
       " 'calidad',\n",
       " 'de',\n",
       " 'las',\n",
       " 'pelis',\n",
       " 'es',\n",
       " 'aceptable',\n",
       " 'puedo',\n",
       " 'pasarla',\n",
       " 'palante',\n",
       " 'hoy',\n",
       " 'en',\n",
       " 'dia',\n",
       " 'no',\n",
       " 'tener',\n",
       " 'un',\n",
       " 'mando',\n",
       " 'con',\n",
       " 'el',\n",
       " 'forward',\n",
       " 'en',\n",
       " 'la',\n",
       " 'mano',\n",
       " 'es',\n",
       " 'morir']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#completar\n",
    "next(criticas.__iter__())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "realistic-holiday",
   "metadata": {},
   "source": [
    "Al contrari que l'objecte generat amb la funció `generaCriticas` l'objecte de `PreprocesaCriticas` es reinicia cada vegada que iterem. \\\n",
    "Calculem els vectors de paraules de tot el corpus amb el model `word2vec` que accepta com a argument d'entrada un objecte de tipus `iterator` com el creat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "proof-vanilla",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cálculo de los vectores de palabras\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(list(criticas), #iterador con los documentos\n",
    "                               vector_size=10,          #tamaño del vector\n",
    "                               window=5,         #nº de términos adyacentes que usamos para el cálculo\n",
    "                               min_count=5,      #nº mínimo de apariciones del término para contarlo\n",
    "                               epochs=100\n",
    "                              )\n",
    "\n",
    "#una vez entrenado el modelo nos quedamos con los vectores calculados\n",
    "#si no se van a actualizar los vectores con nuevos documentos\n",
    "model = model.wv\n",
    "len(model.key_to_index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "chronic-greensboro",
   "metadata": {},
   "source": [
    "Seleccionem aleatòriament 25 paraules del conjunt calculat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "continent-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "palabras_sample = np.random.choice(model.index_to_key, 25, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "marked-pledge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['siempre', 'dia', 'sentido', 'may', 'realmente', 'jedi', 'ningun',\n",
       "       'violencia', 'dice', 'son', 'grupo', 'ang', 'lo', 'mundo',\n",
       "       'tambien', 'tiempo', 'esa', 'cosas', 'buena', 'carpenter', 'creo',\n",
       "       'fiel', 'empieza', 'principio', 'cada'], dtype='<U13')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras_sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "superb-terry",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Comprova com funciona el model buscant les paraules més similars semànticament a \"trama\", \"peli\" i \"pelicula\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "devoted-toilet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nos', 0.8476152420043945),\n",
       " ('ritmo', 0.8252304792404175),\n",
       " ('quizas', 0.8146380186080933),\n",
       " ('con', 0.7796500325202942),\n",
       " ('nunca', 0.77712482213974),\n",
       " ('hubiera', 0.7687203288078308),\n",
       " ('siempre', 0.7630404829978943),\n",
       " ('muestra', 0.7571978569030762),\n",
       " ('interesantes', 0.7476827502250671),\n",
       " ('muerte', 0.7439104914665222)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('trama')#palabras similares a 'trama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aerial-plumbing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comienzo', 0.9113318920135498),\n",
       " ('tu', 0.8072788119316101),\n",
       " ('mucha', 0.8046942949295044),\n",
       " ('mal', 0.7886249423027039),\n",
       " ('terror', 0.783074676990509),\n",
       " ('pone', 0.7716185450553894),\n",
       " ('entre', 0.7600091695785522),\n",
       " ('dar', 0.7504957914352417),\n",
       " ('esta', 0.7369820475578308),\n",
       " ('puta', 0.7365575432777405)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('peli')#palabras similares a 'peli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "pacific-tower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dios', 0.8604798913002014),\n",
       " ('antes', 0.7922977209091187),\n",
       " ('mala', 0.7679214477539062),\n",
       " ('viene', 0.7592052817344666),\n",
       " ('serie', 0.682867169380188),\n",
       " ('dejar', 0.6762402057647705),\n",
       " ('sobre', 0.6741084456443787),\n",
       " ('vamos', 0.6738705039024353),\n",
       " ('sigue', 0.6731991767883301),\n",
       " ('dar', 0.6532607078552246)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('pelicula')#palabras similares a 'pelicula'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "circular-wheat",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Veurem la influència del preprocessament. Per a això alimentarem el model amb les crítiques *preprocesadas amb la funció de normalització sobre `spaCy` (que produeix text lematizat i amb un altre filtrat).\\\n",
    "Per a això cal re-definir l'objecte `iterador` sobre el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "gentle-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparamos con un modelo que use el preprocesado de spaCy\n",
    "        \n",
    "class PreprocesaCriticasSpacy(object):\n",
    "    \"\"\"Pre-procesa el corpus de críticas con la función de normalización\n",
    "    definida con la librería spaCy\n",
    "    Entrada: directorio de críticas\n",
    "    Salida: iterador sobre las críticas (como lista de tokens)\"\"\"\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for c in parse_folder(self.dirname):\n",
    "            yield normaliza_bis(c[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "pointed-vault",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Instància aquesta nova classe per al directori de les crítiques en l'objecte `critiques_spacy` i comprova el seu funcionament sobre la primera crítica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "thirty-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "criticas_spacy = PreprocesaCriticasSpacy('criticas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "published-carbon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gustar cine masa pelicula ver mundo parecer coñazo insufribl porqué prota tonto culo suerte peli lograr vencer convertir listo chorrada comienzo pelicula esfumar arte magia volver maduro inteligente peli spielberg huir huir dar tiro cabron meter par actor echar él comer aparte niña viejo metido cuerpo niña ver él hablar version original dar él freaks cine creerar gracia nena puta madre causar pavor cria persona maduro horroroso niño niño ver él rol asustar hijo adolescente cruise subnormal dar él bofetada ver hueso mano persona reflejo denominar manipulacion militar chico matar bicho ningun arma persona loco pensar venir saco quereis decir fanatismo locura alguien querer luchar medio muerte seguro jodar sobremanera pelicula aparecer mongo abuelo familia salir dios viejo aparecer traje domingo maquillada sinsorgado faltar persona volver exmujer mundo tranquilos contar peli pasar peli acabar restar misterio bodrio persona paso convertir director efecto especial contar historia disteis norteamericano guardar compostura gritar momento dificil sensato tranquilizar él discutir chorrada puta peli parar gritar pegar él gracia comienzo peli suelo empezar resquebrajar él gente quedar mirar pasar persona estaria correr puta tiempo escondiéndome ver bicho lanzar rayo machacar to dios esconder seguir hablar mierda queriar conseguir pagasemos entrada cine prima bajarir internet ver calidad peli aceptable pasar él palante mando forward mano morir'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(criticas_spacy.__iter__())#prueba a generar la primera crítica"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "furnished-serial",
   "metadata": {},
   "source": [
    "Calcula el model de vectors de paraules amb aquest nou *pre-processat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "significant-zambia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSpacy = Word2Vec(list(criticas_spacy), #iterador con los documentos\n",
    "                               vector_size=10,          #tamaño del vector\n",
    "                               window=5,         #nº de términos adyacentes que usamos para el cálculo\n",
    "                               min_count=5,      #nº mínimo de apariciones del término para contarlo\n",
    "                               epochs=100\n",
    "                              )\n",
    "\n",
    "#una vez entrenado el modelo nos quedamos con los vectores calculados\n",
    "#si no se van a actualizar los vectores con nuevos documentos\n",
    "modelSpacy = modelSpacy.wv\n",
    "len(modelSpacy.index_to_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "weighted-bracelet",
   "metadata": {},
   "source": [
    "### Pregunta\n",
    "per què el nou model té un vocabulari amb molts menys termes que el model anterior?\n",
    "\n",
    "### Respuesta\n",
    "\n",
    "En la función simple_preproces() de gensim, la frase de cada crítica se procesa incluyendo stop_words mientras que en la funcion normaliza_bis() no las incluye."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sound-drawing",
   "metadata": {},
   "source": [
    "### Visualització de word vectors\n",
    "Usem una reducció de dimensionalitat t-SNE per a visualitzar un grup de paraules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "treated-preference",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _imaging: No se puede encontrar el módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmanifold\u001b[39;00m \u001b[39mimport\u001b[39;00m TSNE\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m palabras_vectors \u001b[39m=\u001b[39m model[model\u001b[39m.\u001b[39mindex_to_key]\n\u001b[0;32m      6\u001b[0m \u001b[39m#seleccinamos unos pocos términos para visualizarlos entre el conjunto\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\David_Sanchis\\anaconda3\\envs\\PLN\\lib\\site-packages\\matplotlib\\__init__.py:131\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m parse \u001b[39mas\u001b[39;00m parse_version\n\u001b[0;32m    129\u001b[0m \u001b[39m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    132\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    133\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m \u001b[39mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32mc:\\Users\\David_Sanchis\\anaconda3\\envs\\PLN\\lib\\site-packages\\matplotlib\\rcsetup.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, cbook\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolors\u001b[39;00m \u001b[39mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_fontconfig_pattern\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_enums\u001b[39;00m \u001b[39mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32mc:\\Users\\David_Sanchis\\anaconda3\\envs\\PLN\\lib\\site-packages\\matplotlib\\colors.py:51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumbers\u001b[39;00m \u001b[39mimport\u001b[39;00m Number\n\u001b[0;32m     50\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mPngImagePlugin\u001b[39;00m \u001b[39mimport\u001b[39;00m PngInfo\n\u001b[0;32m     54\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\David_Sanchis\\anaconda3\\envs\\PLN\\lib\\site-packages\\PIL\\Image.py:103\u001b[0m\n\u001b[0;32m     94\u001b[0m MAX_IMAGE_PIXELS \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m4\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[39m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _imaging \u001b[39mas\u001b[39;00m core\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m __version__ \u001b[39m!=\u001b[39m \u001b[39mgetattr\u001b[39m(core, \u001b[39m\"\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    106\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    107\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCore version: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mgetattr\u001b[39m(core,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPillow version: \u001b[39m\u001b[39m{\u001b[39;00m__version__\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _imaging: No se puede encontrar el módulo especificado."
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "palabras_vectors = model[model.index_to_key]\n",
    "\n",
    "#seleccinamos unos pocos términos para visualizarlos entre el conjunto\n",
    "random_idx = np.random.randint(len(model.index_to_key), size=5)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=250, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(palabras_vectors)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='steelblue', alpha=0.1)\n",
    "\n",
    "labels = np.array(model.index_to_key)[random_idx]\n",
    "\n",
    "\n",
    "T_labels = T[random_idx,:]\n",
    "\n",
    "plt.scatter(T_labels[:, 0], T_labels[:, 1], c='lime', edgecolors='darkgreen')\n",
    "for label, x, y in zip(labels, T_labels[:, 0], T_labels[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "advance-success",
   "metadata": {},
   "source": [
    "Ara carregarem els vectors per a les mateixes paraules amb el model pre-entrenat GloVe de `spaCy` per a comparar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "hybrid-antique",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _imaging: No se puede encontrar el módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m palabras_vectors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([nlp\u001b[39m.\u001b[39mvocab[t]\u001b[39m.\u001b[39mvector \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mindex_to_key])\n\u001b[0;32m      6\u001b[0m tsne \u001b[39m=\u001b[39m TSNE(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, n_iter\u001b[39m=\u001b[39m\u001b[39m250\u001b[39m, perplexity\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\David_Sanchis\\anaconda3\\envs\\PLN\\lib\\site-packages\\matplotlib\\__init__.py:131\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m parse \u001b[39mas\u001b[39;00m parse_version\n\u001b[0;32m    129\u001b[0m \u001b[39m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    132\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    133\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m \u001b[39mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32mc:\\Users\\David_Sanchis\\anaconda3\\envs\\PLN\\lib\\site-packages\\matplotlib\\rcsetup.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, cbook\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolors\u001b[39;00m \u001b[39mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_fontconfig_pattern\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_enums\u001b[39;00m \u001b[39mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32mc:\\Users\\David_Sanchis\\anaconda3\\envs\\PLN\\lib\\site-packages\\matplotlib\\colors.py:51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumbers\u001b[39;00m \u001b[39mimport\u001b[39;00m Number\n\u001b[0;32m     50\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mPngImagePlugin\u001b[39;00m \u001b[39mimport\u001b[39;00m PngInfo\n\u001b[0;32m     54\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\David_Sanchis\\anaconda3\\envs\\PLN\\lib\\site-packages\\PIL\\Image.py:103\u001b[0m\n\u001b[0;32m     94\u001b[0m MAX_IMAGE_PIXELS \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m4\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[39m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[39m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[39m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _imaging \u001b[39mas\u001b[39;00m core\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m __version__ \u001b[39m!=\u001b[39m \u001b[39mgetattr\u001b[39m(core, \u001b[39m\"\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    106\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    107\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCore version: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mgetattr\u001b[39m(core,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPillow version: \u001b[39m\u001b[39m{\u001b[39;00m__version__\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _imaging: No se puede encontrar el módulo especificado."
     ]
    }
   ],
   "source": [
    "palabras_vectors = np.array([nlp.vocab[t].vector for t in model.index_to_key])\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0, n_iter=250, perplexity=2)\n",
    "np.set_printoptions(suppress=True)\n",
    "T = tsne.fit_transform(palabras_vectors)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.scatter(T[:, 0], T[:, 1], c='steelblue', alpha=0.1)\n",
    "\n",
    "labels = np.array(model.index_to_key)[random_idx]\n",
    "\n",
    "\n",
    "T_labels = T[random_idx,:]\n",
    "\n",
    "plt.scatter(T_labels[:, 0], T_labels[:, 1], c='lime', edgecolors='darkgreen')\n",
    "for label, x, y in zip(labels, T_labels[:, 0], T_labels[:, 1]):\n",
    "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "outdoor-flight",
   "metadata": {},
   "source": [
    "## Part 3: Conjunt de Tuits en espanyol\n",
    "Anem a pre-processar un conjunt de tuits en espanyol etiquetatges amb la seua polaritat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-studio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Me caes muy bien \\r\\n-Tienes que jugar más partidas al lol con Russel y conmigo\\r\\n-Por qué tan Otako, deja de ser otako\\r\\n-Haber si me muero</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@myendlesshazza a. que puto mal escribo\\r\\n\\r\\nb. me sigo surrando help \\r\\n\\r\\n3. ha quedado raro el \"cómetelo\" ahí JAJAJAJA</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@estherct209 jajajaja la tuya y la d mucha gente seguro!! Pero yo no puedo sin mi melena me muero</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quiero mogollón a @AlbaBenito99 pero sobretodo por lo rápido que contesta a los wasaps</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vale he visto la tia bebiendose su regla y me hs dado muchs grima</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           content  \\\n",
       "0  -Me caes muy bien \\r\\n-Tienes que jugar más partidas al lol con Russel y conmigo\\r\\n-Por qué tan Otako, deja de ser otako\\r\\n-Haber si me muero   \n",
       "1                    @myendlesshazza a. que puto mal escribo\\r\\n\\r\\nb. me sigo surrando help \\r\\n\\r\\n3. ha quedado raro el \"cómetelo\" ahí JAJAJAJA   \n",
       "2                                               @estherct209 jajajaja la tuya y la d mucha gente seguro!! Pero yo no puedo sin mi melena me muero    \n",
       "3                                                          Quiero mogollón a @AlbaBenito99 pero sobretodo por lo rápido que contesta a los wasaps    \n",
       "4                                                                               Vale he visto la tia bebiendose su regla y me hs dado muchs grima    \n",
       "\n",
       "  polarity  \n",
       "0     NONE  \n",
       "1        N  \n",
       "2        N  \n",
       "3        P  \n",
       "4        N  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Leemos los datos\n",
    "df = pd.read_csv('tweets_all.csv', index_col=None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "incredible-boating",
   "metadata": {},
   "source": [
    "### Exercici\n",
    "Defineix una funció de normalització que faça les següents tasques:\n",
    "- Eliminar esments i URL mitjançant un patró RegEx\n",
    "- Separar el text en *tokens* convertint-los a minúscules, eliminant els que siguen signes de puntuació, espais o dígits\n",
    "- Eliminar els stop-words d'una llista pròpia passada com a argument\n",
    "- Eliminar els símbols de puntuació dels tokens (etiquetes, admiracions, etc.)\n",
    "- Eliminar els tokens d'una longitud menor de 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "#lista de stop-words específicos de nuestro corpus (aproximación)\n",
    "stop_words = ['los', 'pero', 'por', 'que', 'una']\n",
    "\n",
    "patron = re.compile('[{}]'.format(re.escape(string.punctuation))) #elimina símbolos de puntuación\n",
    "\n",
    "def clean_text(text, stop_words=stop_words):\n",
    "    \"\"\"Limpiamos las menciones y URL del texto. Luego convertimos en tokens\n",
    "    y eliminamos signos de puntuación.\n",
    "    Dejamos tokens en minúsculas.\n",
    "    Como salida volvemos a convertir los tokens en cadena de texto\"\"\"\n",
    "    text = re.sub(r'@[^ ]+', r'', text) #elimina menciones y URL\n",
    "    tokens = nlp(text)\n",
    "    tokens = [token for token in tokens if not token.is_punct and not token.is_space and not token.is_digit] #filtra tokens (puntuaciones, espacios y dígitos)\n",
    "    filtered_tokens = [token for token in tokens if not token.is_stop and len(token) >= 2 and token.is_alpha] #limpia tokens (signos de puntuación, stop-words y longitud<2)\n",
    "    filtered_text = ' '.join([token.text for token in filtered_tokens]) #juntam,os como string\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "vulnerable-consensus",
   "metadata": {},
   "source": [
    "Aplica la funció a tots els tuits (columna 'content') creant una nova columna 'net' del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['limpio'] = [clean_text(text) for text in df['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-vienna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "      <th>limpio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-Me caes muy bien \\r\\n-Tienes que jugar más partidas al lol con Russel y conmigo\\r\\n-Por qué tan Otako, deja de ser otako\\r\\n-Haber si me muero</td>\n",
       "      <td>NONE</td>\n",
       "      <td>caes jugar partidas lol Russel Otako deja otako muero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@myendlesshazza a. que puto mal escribo\\r\\n\\r\\nb. me sigo surrando help \\r\\n\\r\\n3. ha quedado raro el \"cómetelo\" ahí JAJAJAJA</td>\n",
       "      <td>N</td>\n",
       "      <td>puto escribo sigo surrando help quedado raro cómetelo JAJAJAJA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@estherct209 jajajaja la tuya y la d mucha gente seguro!! Pero yo no puedo sin mi melena me muero</td>\n",
       "      <td>N</td>\n",
       "      <td>jajajaja gente seguro melena muero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quiero mogollón a @AlbaBenito99 pero sobretodo por lo rápido que contesta a los wasaps</td>\n",
       "      <td>P</td>\n",
       "      <td>Quiero mogollón sobretodo rápido contesta wasaps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vale he visto la tia bebiendose su regla y me hs dado muchs grima</td>\n",
       "      <td>N</td>\n",
       "      <td>Vale visto tia bebiendose regla hs muchs grima</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                           content  \\\n",
       "0  -Me caes muy bien \\r\\n-Tienes que jugar más partidas al lol con Russel y conmigo\\r\\n-Por qué tan Otako, deja de ser otako\\r\\n-Haber si me muero   \n",
       "1                    @myendlesshazza a. que puto mal escribo\\r\\n\\r\\nb. me sigo surrando help \\r\\n\\r\\n3. ha quedado raro el \"cómetelo\" ahí JAJAJAJA   \n",
       "2                                               @estherct209 jajajaja la tuya y la d mucha gente seguro!! Pero yo no puedo sin mi melena me muero    \n",
       "3                                                          Quiero mogollón a @AlbaBenito99 pero sobretodo por lo rápido que contesta a los wasaps    \n",
       "4                                                                               Vale he visto la tia bebiendose su regla y me hs dado muchs grima    \n",
       "\n",
       "  polarity                                                          limpio  \n",
       "0     NONE           caes jugar partidas lol Russel Otako deja otako muero  \n",
       "1        N  puto escribo sigo surrando help quedado raro cómetelo JAJAJAJA  \n",
       "2        N                              jajajaja gente seguro melena muero  \n",
       "3        P                Quiero mogollón sobretodo rápido contesta wasaps  \n",
       "4        N                  Vale visto tia bebiendose regla hs muchs grima  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
