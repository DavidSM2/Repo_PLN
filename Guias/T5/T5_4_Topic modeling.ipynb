{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "En este notebook se va a demostrar el uso de distintos modelos de extracción de temáticas (*topic modeling*) en un conjunto de textos de ejemplo sencillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nlp=spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del corpus\n",
    "Creamos un pequeño Corpus de ejemplo formado por 8 frases cortas. Definimos una sencilla función de normalización y aplicamos esta normalización a todo el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    # tokenizamos el texto\n",
    "    tokens = nlp(doc)\n",
    "    # quitamos puntuación/espacios/stop words y cogemos el lema\n",
    "    lemmas = [t.lemma_ for t in tokens if not t.is_punct and not t.is_space and not t.is_stop]\n",
    "    doc = ' '.join(lemmas)\n",
    "    return doc\n",
    "\n",
    "def normalize_corpus(corpus):\n",
    "    \"\"\"Normaliza un corpus de documentos aplicando al función de normalización\n",
    "    normalize_document() a cada documento de la lista pasada como argumento\"\"\"   \n",
    "    return [normalize_document(text) for text in corpus]\n",
    "\n",
    "toy_corpus = [\n",
    "\"The fox jumps over the dog\",\n",
    "\"the fox is very clever and quick\",\n",
    "\"The dog is slow and lazy\",\n",
    "\"The cat is smarter than the fox and the dog\",\n",
    "\"Python is an excellent programming language\",\n",
    "\"Java and Ruby are other programming languages\",\n",
    "\"Python and Java are very popular programming languages\",\n",
    "\"Python programs are smaller than Java programs\"]\n",
    "\n",
    "norm_corpus = normalize_corpus(toy_corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling usando Scikit-learn\n",
    "La librería `scikit-learn` implementa los modelos *Latent Semantic Analysis* (LSA) y *Latent Dirichlet Allocation* (LDA).  \n",
    "Partimos de un modelo TF-IDF para el modelado LSA y de un modelo BoW para el modelado LDA\n",
    "\n",
    "### Modelo LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# usamos características tf-idf para LSA.\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf = tfidf_vectorizer.fit_transform(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función de ayuda para mostrar los resultados (términos asociados a cada tema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    \"\"\"Función auxiliar para mostrar los términos más importantes\n",
    "    de cada topic\"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topic #{topic_idx}: \"\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos los modelos para nuestro corpus (método `fit`) y vemos cuáles son los 3 términos con más peso para cada *topic*. Cada modelo asigna un grado de pertenencia en cada tema a cada término del vocabulario de la matriz tfidf o bow utilizada como entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "# Ajustamos el modelo LSA\n",
    "lsa = TruncatedSVD(n_components=2).fit(tfidf)\n",
    "\n",
    "print(\"\\nTopics en modelo LSA:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print_top_words(lsa, tfidf_feature_names, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `fit` aprende la matriz de `topics` x `términos` para el corpus dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(np.round(lsa.components_, 4), columns=tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.todense().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el porcentaje de pertenencia a cada *topic* de cada una de los documentos asignados por el modelo con el método `transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(lsa.transform(tfidf), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada fila corresponde a un documento del Corpus, y cada columna el grado de pertenencia a ese tema del documento.  \n",
    "El modelo ha separado correctamente el corpus en las dos temáticas principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(lsa.transform(tfidf), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usamos características BoW para LDA.\n",
    "tf_vectorizer = CountVectorizer(min_df=2)\n",
    "tf = tf_vectorizer.fit_transform(norm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos el modelo LDA\n",
    "lda = LatentDirichletAllocation(n_components=2, max_iter=5,\n",
    "                                learning_method='batch',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0).fit(tf)\n",
    "\n",
    "print(\"\\nTopics en modelo LDA:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "print_top_words(lda, tf_feature_names, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El atributo `components_` contiene los parámetros de la distribución de términos en *topics*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lda.components_, columns=tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizando esta matriz muestra la distribución de términos dentro de cada *topic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribucion = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]\n",
    "pd.DataFrame(distribucion, columns=tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como en el caso del LSA, podemos ver el grado de pertenencia de cada documento a cada *topic*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.transform(tf).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(lda.transform(tf), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(lda.transform(tf), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling usando librería Gensim\n",
    "La librería `gensim` implementa los siguientes modelos:  \n",
    "* [Latent Semantic Indexing, LSI (or sometimes LSA)](https://en.wikipedia.org/wiki/Latent_semantic_indexing) transforms documents from either bag-of-words or (preferrably) TfIdf-weighted space into a latent space of a lower dimensionality.  \n",
    "* [Latent Dirichlet Allocation, LDA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA’s topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).  \n",
    "* [Hierarchical Dirichlet Process, HDP](http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf) is a non-parametric bayesian method (note the missing number of requested topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La entrada a los modelos de `gensim`\n",
    " debe ser una lista de tokens y no un texto por cada documento del corpus, por lo que hay que cambiar la función de normalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokenize_document(doc):\n",
    "    # tokenizamos el texto\n",
    "    tokens = nlp(doc)\n",
    "    # quitamos puntuación/espacios y cogemos el lema\n",
    "    lemmas = [t.lemma_.lower() for t in tokens if not t.is_punct and not t.is_space and not t.is_stop]\n",
    "    return lemmas\n",
    "\n",
    "def normalize_tokenize_corpus(corpus):\n",
    "    \"\"\"Normaliza un corpus de documentos aplicando al función de normalización\n",
    "    normalize_tokenize_document() a cada documento de la lista pasada como argumento\"\"\"   \n",
    "    return [normalize_tokenize_document(text) for text in corpus]\n",
    "        \n",
    "norm_tokenized_corpus = normalize_tokenize_corpus(toy_corpus)\n",
    "norm_tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en los modelos de la librería `scikit-learn`, primero generamos matrices de características BoW y TF-IDF como paso previo a aplicar los modelos de topic-modeling.  \n",
    "En `gensim` estas matrices se calculan de manera diferente a `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel, TfidfModel\n",
    "\n",
    "#diccionario de términos únicos del corpus\n",
    "dictionary = Dictionary(norm_tokenized_corpus)\n",
    "#creamos matriz BoW\n",
    "corpus_bow = [dictionary.doc2bow(text)\n",
    "                 for text in norm_tokenized_corpus]\n",
    "#creamos matriz TF-IDF del corpus a partir de BoW\n",
    "tfidf = TfidfModel(corpus_bow)\n",
    "corpus_tfidf = tfidf[corpus_bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada tupla indica la frecuencia del ítem *i* en el documento, donde *i* es el índice de las palabras en el vocabulario del diccionario de Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, k) for i,k in dictionary.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Indexing\n",
    "Los modelos de *topic modeling* de `gensim` asignan un peso de pertenencia de cada término del diccionario bow/tfidf a cada tema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = LsiModel(corpus_tfidf, \n",
    "                      id2word=dictionary,\n",
    "                      num_topics=2)\n",
    "\n",
    "for index, topic in lsi.print_topics(2):\n",
    "    print(f'Topic #{str(index)}\\n{topic}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi.get_topics().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz LSI generada es un objeto específico de Gensim que funciona como un *iterable*. Contiene el grado de pertenencia al *topic* o *topics* más representativos del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_lsi = lsi[corpus_tfidf]\n",
    "topics_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_lsi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in topics_lsi:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objeto *TransformedCorpus* sólo muestra los componentes distintos de cero de la matriz LSI del corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim.matutils.corpus2dense(lsi[corpus_tfidf], len(lsi.projection.s)).T "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(corpus_bow, \n",
    "                      id2word=dictionary,\n",
    "                      iterations=1000,\n",
    "                      num_topics=2)\n",
    "for index, topic in lda.print_topics(2):\n",
    "    print('Topic #{}\\n{}\\n'.format(str(index), topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_lda = lda[corpus_bow]\n",
    "topics_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_lda[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in topics_lda:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.get_topics().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Dirichlet Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no hay que especificar un núm. de topics\n",
    "hdp = HdpModel(corpus_bow, \n",
    "                      id2word=dictionary)\n",
    "for index, topic in hdp.print_topics(2):\n",
    "    print('Topic #{}\\n{}\\n'.format(str(index), topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, topic in hdp.print_topics(8):\n",
    "    print('Topic #{}\\n{}\\n'.format(str(index), topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_hdp = hdp[corpus_bow]\n",
    "topics_hdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in topics_hdp:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación de temática principal\n",
    "Podemos calcular la pertenencia de cada documento a una temática a partir de su modelo calculado:  \n",
    "El modelo LSI sólo devuelve el grado de pertencia de cada documento a los *topics* más relevantes a ese documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "for i, doc in enumerate(corpus_lsi):\n",
    "     print(doc, toy_corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo LDA y HDP calculan la pertenencia a cada tema y devuelven una lista de tuplas por cada tema (nº de tema, grado de pertenencia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lda = lda[corpus_bow]\n",
    "for i, doc in enumerate(corpus_lda):\n",
    "     print(doc, toy_corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el modelo LDA el peso de cada palabra en cada *topic* es una probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.get_topics().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(lda.get_topics(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y las pertenencias de un documento a cada *topic* también son un valor de probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.matutils.corpus2dense(lda[corpus_bow], lda.num_topics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(gensim.matutils.corpus2dense(lda[corpus_bow], lda.num_topics).T, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(gensim.matutils.corpus2dense(lda[corpus_bow], lda.num_topics).T, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el modelo HDP no se especifica un número de temas sino que se definen automáticamente (con importancia decreciente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución\n",
    "corpus_hdp = hdp[corpus_bow]\n",
    "for i, doc in enumerate(corpus_hdp):\n",
    "     print(doc, toy_corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener los términos relevantes para cada tema y su importancia con el método `show_topics` del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsitopics = [[(word,prob) for word, prob in topic] for topicid, topic in lsi.show_topics(formatted=False)]\n",
    "\n",
    "hdptopics = [[(word,prob) for word, prob in topic] for topicid, topic in hdp.show_topics(formatted=False)]\n",
    "\n",
    "ldatopics = [[(word,prob) for word, prob in topic] for topicid, topic in lda.show_topics(formatted=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldatopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsitopics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Coherence\n",
    "La librería `gensim` proporciona una funcionalidad para identificar qué modelo de *topic modeling* se adapta mejor al corpus. La función `CoherenceModel` calcula una puntuación sobre la coherencia del modelo, que podemos usar para compararlos. Esta función utiliza las palabras que definen cada tópico en los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsitopics = [[word for word, prob in topic] for topicid, topic in lsi.show_topics(formatted=False)]\n",
    "\n",
    "hdptopics = [[word for word, prob in topic] for topicid, topic in hdp.show_topics(formatted=False)]\n",
    "\n",
    "ldatopics = [[word for word, prob in topic] for topicid, topic in lda.show_topics(formatted=False)]\n",
    "\n",
    "lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=norm_tokenized_corpus,\n",
    "                               dictionary=dictionary, window_size=10).get_coherence()\n",
    "\n",
    "hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=norm_tokenized_corpus, \n",
    "                               dictionary=dictionary, window_size=10).get_coherence()\n",
    "\n",
    "lda_coherence = CoherenceModel(topics=ldatopics, texts=norm_tokenized_corpus,\n",
    "                               dictionary=dictionary, window_size=10).get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsitopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bar_graph(coherences, indices):\n",
    "    \"\"\"\n",
    "    Función para dibujar una gráfica de barras con:\n",
    "    \n",
    "    coherences: lista de los valores de coherencia\n",
    "    indices: textos para etiquetar las barras.\n",
    "    Ambos parámetros deben tener la misma longitud\n",
    "    \"\"\"\n",
    "    assert len(coherences) == len(indices)\n",
    "    n = len(coherences)\n",
    "    x = np.arange(n)\n",
    "    plt.bar(x, coherences, width=0.2, tick_label=indices, align='center')\n",
    "    plt.xlabel('Modelos')\n",
    "    plt.ylabel('Valor Coherencia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_bar_graph([lsi_coherence, hdp_coherence, lda_coherence],\n",
    "                   ['LSI','HDP', 'LDA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
