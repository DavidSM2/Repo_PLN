{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling (librería Gensim)\n",
    "Vamos a ver cómo realizar un modelado de temática en grandes volúmenes de texto con la librería `gensim`  \n",
    "\n",
    "Utilizaremos el conjunto de datos *Lee* de `Gensim` (es una versión abreviada del conjunto http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF).  \n",
    "\n",
    "Para visualizar gráficamente los tópicos es necesario instalar la librería `pyLDAvis` dentro del entorno de Anaconda con el comando:\n",
    "```python\n",
    "conda install -c conda-forge pyldavis \n",
    "```\n",
    "\n",
    "### Cargamos librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# spacy para lematizar\n",
    "import spacy\n",
    "\n",
    "# herramientas de dibujado\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos un generador para obtener los documentos del Corpus línea a línea desde el archivo del conjunto de ejemplo y convertirlos en un listado de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "stop_words = nlp.Defaults.stop_words #listado de stop-words\n",
    "\n",
    "def lemmatize_corpus(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']):\n",
    "    \"\"\"Función que devuelve el lema de una string,\n",
    "    excluyendo las palabras cuyo POS_TAG no está en la lista\"\"\"\n",
    "    text_out = [t.lemma_.lower() for t in nlp(text)\n",
    "                if t.pos_ in allowed_postags\n",
    "                and len(t.lemma_)>3\n",
    "                and not t.is_stop]\n",
    "    return text_out\n",
    "\n",
    "def normalize_document(text):\n",
    "    \"\"\"Limpia una cadena de texto pasada como argumento\n",
    "    y devuelve un listado de tokens\"\"\"\n",
    "    doc = gensim.utils.simple_preprocess(text, deacc=True, min_len=3)\n",
    "    return [word for word in doc if word not in stop_words]\n",
    "            \n",
    "def build_texts(fname):\n",
    "    \"\"\"\n",
    "    Generador que devuelve el texto tokenizado a partir de un archivo\n",
    "    línea a línea\n",
    "    \"\"\"\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            yield lemmatize_corpus(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 'test', 'test_data'])\n",
    "lee_data_file = data_dir + os.sep + 'lee_background.cor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lee_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto=build_texts(lee_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(texto))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in texto:\n",
    "    print(t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos el diccionario y el corpus para Topic Modeling\n",
    "Las dos entradas para el modelo LDA son un diccionario de `gensim` y un corpus de texto.  \n",
    "Preparamos el diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOW_Corpus(object):\n",
    "    \"\"\"\n",
    "    Iterable: en cada iteración devuelve el vector bag-of-words\n",
    "    del siguiente documento en el corpus.\n",
    "    El corpus es el listado de críticas alojadas en el directorio\n",
    "    pasado como argumento al instanciar la clase.\n",
    "    \n",
    "    Procesa un documento cada vez, así\n",
    "    nunca carga el corpus entero en RAM.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        #creamos bigramas y trigramas\n",
    "        self.bigram = gensim.models.Phrases(build_texts(self.filename), min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "        #optimizamos una vez entreando\n",
    "        self.bigram_mod = gensim.models.phrases.Phraser(self.bigram)\n",
    "\n",
    "        self.trigram = gensim.models.Phrases(self.bigram_mod[build_texts(self.filename)], min_count=5, threshold=50)  \n",
    "        self.trigram_mod = gensim.models.phrases.Phraser(self.trigram)\n",
    "        #crea el diccionario = mapeo de documentos a sparse vectors\n",
    "        self.diccionario = gensim.corpora.Dictionary(self.trigram_mod[map(lambda x: self.bigram_mod[x], build_texts(self.filename))])\n",
    "\n",
    "    def __len__(self):\n",
    "        #necesitamos saber la longitud del corpus para visualizar con pyLDAvis\n",
    "        return self.diccionario.num_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        __iter__ es un iterable => BOW_Corpus es un streamed iterable.\n",
    "        \"\"\"\n",
    "        for tokens in build_texts(self.filename):\n",
    "            # transforma cada doc (lista de tokens) en un vector sparse uno a uno\n",
    "            yield self.diccionario.doc2bow(self.trigram_mod[self.bigram_mod[tokens]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bow = BOW_Corpus(lee_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_bow.diccionario.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bow.diccionario.num_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[k for k in corpus_bow.diccionario.token2id if re.match(r'\\w+_\\w+_\\w+', k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([k for k in corpus_bow.diccionario.token2id if re.match(r'\\w+_\\w+', k)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in corpus_bow:\n",
    "    print(c)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo LDA\n",
    "Es un modelo generativo que considera cada documento como una mezcla de temas donde cada tema tiene una distribución de las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "ldamodel = LdaModel(corpus=corpus_bow, num_topics=10, id2word=corpus_bow.diccionario)\n",
    "pprint(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización de los temas  \n",
    "Podemos visualizarlo gráficamente la distribución de los documentos del Corpus por temas con la librería `pyLDAvis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = gensimvis.prepare(ldamodel, corpus_bow, corpus_bow.diccionario)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
