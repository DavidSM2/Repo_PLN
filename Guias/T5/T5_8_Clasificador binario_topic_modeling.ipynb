{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador binario\n",
    "Vamos a utilizar Spacy y scikit-learn para clasificar con conjunto de tweets en español como positivos/negativos (análisis de sentimientos)\n",
    "\n",
    "## Carga y preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Leemos los datos\n",
    "df = pd.read_csv('tweets_all.csv', index_col=None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 1514 tweets, de los cuales hay 637 positivos y 474 negativos. El resto son neutros o no tienen polaridad clasificada.\n",
    "Vamos a entrenar sólo con los positivos y negativos para utilizar un clasificador binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['polarity']=='P') | (df['polarity']=='N')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quitamos las columnas que no usamos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de texto\n",
    "Hacemos un pequeño pre-procesado del texto antes de extraer las características:  \n",
    "- Quitamos las menciones y las URL del texto porque no aportan valor para el análisis de sentimientos.\n",
    "- Los hashtag sí que pueden aportar valor así que simplemente quitamos el #.\n",
    "- Quitamos los signos de puntuación y palabras menores de 3 caracteres.\n",
    "- Por último quitamos todos los símbolos de puntuación del texto (que forman parte de un token).\n",
    "- Lematizamos el texto y lo guardamos en otra columna para comparar resultados del clasificador. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, spacy\n",
    "nlp=spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de stop-words específicos de nuestro corpus (aproximación)\n",
    "stop_words = ['unos', 'unas', 'algún', 'alguna', 'algunos', 'algunas', 'ese', 'eso', 'así']\n",
    "\n",
    "pattern2 = re.compile('[{}]'.format(re.escape(string.punctuation))) #elimina símbolos de puntuación\n",
    "\n",
    "def clean_text(text, lemas=False):\n",
    "    \"\"\"Limpiamos las menciones y URL del texto. Luego convertimos en tokens\n",
    "    y eliminamos signos de puntuación.\n",
    "    Si lemas=True extraemos el lema, si no dejamos en minúsculas solamente.\n",
    "    Como salida volvemos a convertir los tokens en cadena de texto\"\"\"\n",
    "    text = re.sub(r'@[\\w_]+|https?://[\\w_./]+', '', text) #elimina menciones y URL\n",
    "    tokens = nlp(text)\n",
    "    tokens = [tok.lemma_.lower() if lemas else tok.lower_ for tok in tokens if not tok.is_punct]\n",
    "    filtered_tokens = [pattern2.sub('', tok) for tok in tokens if not (tok in stop_words) and len(tok)>2]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return filtered_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el funcionamiento de estas funciones sobre un tweet de ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original:\\n',df.content[702])\n",
    "print('\\nLimpiado:\\n',clean_text(df.content[702]))\n",
    "print('\\nLematizado:\\n',clean_text(df.content[702], lemas=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos limpieza a todos los tweets del DataFrame y creamos columna nueva con los lemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"limpio\"]=df['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitamos tweets vacíos después de la limpieza\n",
    "df=df[df.limpio!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"lemas\"]=df.content.apply(clean_text, lemas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contamos el nº de palabras por tweet\n",
    "df['words'] = [len(t.split(' ')) for t in df.limpio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador\n",
    "Vamos a usar la librería scikit-learn para aplicar un clasificador binario sobre la polaridad usando una extracción de características Bag-of-Words (BoW)\n",
    "\n",
    "Primero dividimos en conjunto de entrenamiento y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and test sets\n",
    "# Asignamos un 70% a training y un 30% a test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['limpio'], \n",
    "                                                    df['polarity'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Primera entrada de train:\\n', X_train.iloc[0])\n",
    "print('Polaridad:', y_train.iloc[0])\n",
    "print('\\nX_train shape:', X_train.shape)\n",
    "print('\\nX_test shape:', X_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos basados en el topic vector space\n",
    "Aplicamos un modelo LSA a la salida del TF-IDF y lo usamos como entrada a nuestro clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "svd = TruncatedSVD(n_components=750)\n",
    "\n",
    "red_dim = make_pipeline(vect, svd, Normalizer(copy=False))\n",
    "#Entrenamos el modelo con el conjunto de train\n",
    "lsa_train = red_dim.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_test = red_dim.transform(X_test)\n",
    "lsa_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "Probamos con varios modelos de clasificación sobre el dataset reducido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "modelos = [('Logistic Regression', LogisticRegression(solver='liblinear')),\n",
    "           ('Naive Bayes', GaussianNB()),\n",
    "           ('Linear SVM', SGDClassifier(loss='hinge', max_iter=10000, tol=1e-5)),\n",
    "           ('RFB SVM', SVC(gamma='scale', C=2))]\n",
    "\n",
    "for m, clf in modelos:\n",
    "    print('Modelo {} con características LSA 750 dims'.format(m))\n",
    "    #entrenamos sobre train\n",
    "    clf.fit(lsa_train, y_train)\n",
    "    # Predecimos sobre el conjunto de test\n",
    "    prediccion = clf.predict(lsa_test)\n",
    "    print(classification_report(y_test, prediccion))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
